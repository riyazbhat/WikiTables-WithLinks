{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling data code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('processed_table.json') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "import sys\n",
    "import requests\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "import os\n",
    "\n",
    "print('original data has {} entries'.format(len(data)))\n",
    "new_data = []\n",
    "for i, d in enumerate(data):\n",
    "    sys.stdout.write(\"finished {}/{} \\r\".format(i, len(data)))\n",
    "    title = d['title']\n",
    "    title = '_'.join(title.split(' '))    \n",
    "    page = 'https://en.wikipedia.org/wiki/{}'.format(title)\n",
    "        \n",
    "    if len(d['data']) > 5 and len(d['data']) < 40 and len(d['data'][0]) >= 4:\n",
    "        headers = set(d['header'])\n",
    "        if len(headers) == len(d['header']):\n",
    "            cols = len(d['header'])\n",
    "            count = 0\n",
    "            for g in d['data'][0]:\n",
    "                if g[1] is not None:\n",
    "                    count += 1\n",
    "            if count < 0.3 * cols:\n",
    "                continue\n",
    "            \n",
    "            # process if there are enough hyperlinks\n",
    "            title = d['title']\n",
    "            title = '_'.join(title.split(' '))\n",
    "                \n",
    "            if not os.path.exists('htmls/{}.html'.format(title)):\n",
    "                try:\n",
    "                    response = urllib.request.urlopen(page)\n",
    "                    webContent = response.read()\n",
    "                    f = open('htmls/{}.html'.format(title), 'wb')\n",
    "                    f.write(webContent)\n",
    "                    f.close()\n",
    "                except Exception:\n",
    "                    continue\n",
    "\n",
    "            d['page'] = '{}.html'.format(title)\n",
    "            new_data.append(d)\n",
    "\n",
    "print('filtered data has {} entries'.format(len(new_data)))\n",
    "with open('processed_table_with_page.json', 'w') as f:\n",
    "    json.dump(new_data, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "from multiprocessing import Pool\n",
    "import multiprocessing\n",
    "from utils import *\n",
    "\n",
    "def process_link(text):\n",
    "    tmp = []\n",
    "    hrefs = []\n",
    "    for t in text.find_all('a'):\n",
    "        if len(t.get_text().strip()) > 0:\n",
    "            if 'href' in t.attrs and t['href'].startswith('/wiki/'):\n",
    "                tmp.append(t.get_text().strip())\n",
    "                hrefs.append(t['href'])\n",
    "            else:\n",
    "                tmp.append(t.get_text().strip())\n",
    "                hrefs.append('#')\n",
    "    if all([_ == '#' for _ in hrefs]):\n",
    "        return ','.join(tmp).strip(), None\n",
    "    else:\n",
    "        return ','.join(tmp).strip(), ' '.join(hrefs)\n",
    "\n",
    "def remove_ref(text):\n",
    "    for x in text.find_all('sup'):\n",
    "        x.extract()\n",
    "    return text\n",
    "\n",
    "def get_section_title(r):\n",
    "    text = r.previous_sibling\n",
    "    title_hierarchy = []\n",
    "    while text is None or text == '\\n' or text.name not in ['h2', 'h3']:\n",
    "        if text is None:\n",
    "            break\n",
    "        else:\n",
    "            text = text.previous_sibling               \n",
    "    \n",
    "    if text is not None:\n",
    "        title_hierarchy.append(text.find(class_='mw-headline').text)\n",
    "        if text.name in ['h3']:\n",
    "            while text is None or text == '\\n' or text.name not in ['h2']:\n",
    "                if text is None:\n",
    "                    break\n",
    "                else:\n",
    "                    text = text.previous_sibling               \n",
    "\n",
    "            if text is None:\n",
    "                pass\n",
    "            else:\n",
    "                title_hierarchy.append(text.find(class_='mw-headline').text)\n",
    "    \n",
    "    if len(title_hierarchy) == 0:\n",
    "        return ''\n",
    "    else:\n",
    "        tmp = ' -- '.join(title_hierarchy[::-1])\n",
    "        return normalize(tmp)\n",
    "\n",
    "def get_section_text(r):\n",
    "    text = r.previous_sibling\n",
    "    section_text = ''\n",
    "    while text is not None:\n",
    "        if text == '\\n':\n",
    "            text = text.previous_sibling\n",
    "        elif text.name in ['h1', 'h2', 'h3', 'h4']:\n",
    "            break\n",
    "        else:\n",
    "            tmp = text.text\n",
    "            if tmp:\n",
    "                mask = ['note', 'indicate', 'incomplete', 'source', 'reference']\n",
    "                if  any([_ in tmp.lower() for _ in mask]):\n",
    "                    tmp = ''\n",
    "                else:\n",
    "                    tmp = normalize(tmp)\n",
    "                    if section_text:\n",
    "                        section_text = tmp + ' ' + section_text\n",
    "                    else:\n",
    "                        section_text = tmp\n",
    "            text = text.previous_sibling\n",
    "    return section_text\n",
    "\n",
    "def normalize(string):\n",
    "    string = string.strip().replace('\\n', ' ')\n",
    "    return tokenize(string)\n",
    "    \n",
    "def sub_func(f_name):\n",
    "    results = []\n",
    "    with open('htmls/' + f_name, 'r') as f:\n",
    "        soup = BeautifulSoup(f, 'html.parser')\n",
    "        rs = soup.find_all(class_='wikitable sortable')\n",
    "        \n",
    "        for r in rs:\n",
    "            heads = []\n",
    "            rows = []\n",
    "            for i, t_row in enumerate(r.find_all('tr')):\n",
    "                if i == 0:\n",
    "                    for h in t_row.find_all(['th', 'td']):\n",
    "                        h = remove_ref(h)\n",
    "                        if len(h.find_all('a')) > 0:\n",
    "                            heads.append(process_link(h))\n",
    "                        else:\n",
    "                            heads.append((h.get_text().strip(), None))\n",
    "                else:\n",
    "                    row = []\n",
    "                    for h in t_row.find_all(['th', 'td']):\n",
    "                        h = remove_ref(h)\n",
    "                        import pdb\n",
    "                        pdb.set_trace()\n",
    "                        if len(h.find_all('a')) > 0:\n",
    "                            row.append(process_link(h))\n",
    "                        else:\n",
    "                            row.append((h.get_text().strip(), None))\n",
    "                    if all([len(cell[0]) == 0 for cell in row]):\n",
    "                        continue\n",
    "                    else:\n",
    "                        rows.append(row)\n",
    "            \n",
    "            rows = rows[:20]\n",
    "            if any([len(row) != len(heads) for row in rows]) or len(rows) < 8:\n",
    "                continue\n",
    "            else:\n",
    "                section_title = get_section_title(r)\n",
    "                section_text = get_section_text(r)\n",
    "                print(section_title, \"||||\", section_text)\n",
    "                title = soup.title.string\n",
    "                title = re.sub(' - Wikipedia', '', title)\n",
    "                url = 'https://en.wikipedia.org/wiki/{}'.format('_'.join(title.split(' ')))\n",
    "                results.append({'url': url, 'title': title, 'header': heads, 'data': rows, \n",
    "                                'section_title': section_title, 'section_text': section_text})\n",
    "    return results\n",
    "\n",
    "rs = []\n",
    "for f in os.listdir('htmls/'):\n",
    "    tmp = sub_func(f)\n",
    "    rs.append(tmp)\n",
    "\"\"\"\n",
    "results = []\n",
    "for r in rs:\n",
    "    results = results + r\n",
    "\"\"\"\n",
    "#sub_func('Shortest_tennis_match_records.html')\n",
    "#with open('processed_new_table.json', 'w') as f:\n",
    "#    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "string = '11 - 1'\n",
    "re.sub(r'([0-9]{1,2})-([0-9]{1,2})', r'\\1 - \\2', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('processed_new_table.json', 'r') as f:\n",
    "    tables = json.load(f)\n",
    "\n",
    "deletes = []\n",
    "for i, table in enumerate(tables):\n",
    "    # Remove sparse columns\n",
    "    to_remove = []\n",
    "    for j, h in enumerate(table['header']):\n",
    "        #if j == 0:\n",
    "        #    continue\n",
    "        if 'Coordinates' in h[0][0] or 'Image' in h[0][0]:\n",
    "            to_remove.append(j)\n",
    "            continue\n",
    "        \n",
    "        count = 0\n",
    "        total = len(table['data'])\n",
    "        for d in table['data']:\n",
    "            #print(d[j])\n",
    "            if d[j][0][0] != '':\n",
    "                count += 1\n",
    "        \n",
    "        if count / total < 0.5:\n",
    "            to_remove.append(j)\n",
    "    \n",
    "    bias = 0\n",
    "    for r in to_remove:\n",
    "        del tables[i]['header'][r - bias]\n",
    "        for _ in range(len(table['data'])):\n",
    "            del tables[i]['data'][_][r - bias]\n",
    "        bias += 1\n",
    "    \n",
    "    # Remove sparse rows\n",
    "    to_remove = []\n",
    "    for k in range(len(table['data'])):\n",
    "        non_empty = [1 if _[0][0] != '' else 0 for _ in table['data'][k]]\n",
    "        if sum(non_empty) < 0.5 * len(non_empty):\n",
    "            to_remove.append(k)\n",
    "    \n",
    "    bias = 0\n",
    "    for r in to_remove:        \n",
    "        del tables[i]['data'][r - bias]\n",
    "        bias += 1\n",
    "    \n",
    "    if len(table['header']) > 6:\n",
    "        deletes.append(i)\n",
    "    elif len(table['header']) <= 2:\n",
    "        deletes.append(i)\n",
    "    else:\n",
    "        count = 0\n",
    "        total = 0\n",
    "        for row in table['data']:\n",
    "            for cell in row:\n",
    "                if len(cell[0][0]) != '':\n",
    "                    if cell[1] == [None]:\n",
    "                        count += 1                    \n",
    "                    total += 1\n",
    "        if count / total >= 0.7:\n",
    "            deletes.append(i)\n",
    "\n",
    "print('out of {} tables, {} need to be deleted'.format(len(tables), len(deletes)))\n",
    "\n",
    "bias = 0\n",
    "for i in deletes:\n",
    "    del tables[i - bias]\n",
    "    bias += 1\n",
    "\n",
    "with open('processed_new_table_postfiltering.json', 'w') as f:\n",
    "    json.dump(tables, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "import pandas\n",
    "import json\n",
    "from yattag import Doc\n",
    "from yattag import indent\n",
    "import random\n",
    "\n",
    "with open('processed_new_table_postfiltering.json', 'r') as f:\n",
    "    tables = json.load(f)\n",
    "\n",
    "doc, tag, text = Doc().tagtext()\n",
    "\n",
    "cache = ''\n",
    "\n",
    "style = \"\"\"\n",
    "    th {\n",
    "        padding-top: 12px;\n",
    "        padding-bottom: 12px;\n",
    "        text-align: left;\n",
    "        background-color: #c9c9c9;\n",
    "        color: black;\n",
    "    }\n",
    "    td, th {\n",
    "        border: 1px solid #dddddd;\n",
    "        text-align: left;\n",
    "        padding: 8px;\n",
    "    }\n",
    "    td {\n",
    "        padding-top: 12px;\n",
    "        padding-bottom: 12px;\n",
    "        text-align: left;\n",
    "        background-color: #f0f0f0;\n",
    "        color: black;\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "doc.asis('<!DOCTYPE html>')\n",
    "with tag('html'):\n",
    "    with tag('head'):\n",
    "        with tag('style'):\n",
    "            doc.asis(style)\n",
    "\n",
    "        doc.asis('<meta charset=\\\"utf-8\\\">')\n",
    "        doc.asis('<meta name=\\\"viewport\\\" content=\\\"width=device-width, initial-scale=1\\\">')\n",
    "        doc.asis('<title>Demonstration</title>')\n",
    "        doc.asis('<link rel=\"icon\" href=\"\">') #Modifier ici pour le favicon\n",
    "        doc.asis('<script defer src=\"https://use.fontawesome.com/releases/v5.3.1/js/all.js\"></script>')\n",
    "    \n",
    "    \n",
    "    with tag('body'):\n",
    "        random.shuffle(tables)\n",
    "        for table in tables[:200]:\n",
    "            with tag('h3'):\n",
    "                with tag('a', href=table['url']):\n",
    "                    text(table['title'])\n",
    "            \n",
    "            with tag('h4'):\n",
    "                text(table['context'])\n",
    "                \n",
    "            with tag('table', klass='wikitable', style=\"border:1\"):\n",
    "                with tag('tbody'):\n",
    "                    with tag('tr'):\n",
    "                        for cell in table['header']:\n",
    "                            with tag('th'):\n",
    "                                if cell[1] is not None:\n",
    "                                    count = 0\n",
    "                                    for t, s in zip(cell[0], cell[1]):\n",
    "                                        if s is not None:\n",
    "                                            with tag('a', href='http://edward.cs.ucsb.edu:6007/query?name=' + s):\n",
    "                                                text(t)\n",
    "                                            if count < len(cell[1]) - 1:\n",
    "                                                text(', ')\n",
    "                                        else:\n",
    "                                            text(t)\n",
    "                                            \n",
    "                                        count += 1\n",
    "                                else:\n",
    "                                    text(cell[0])\n",
    "\n",
    "                    for row in table['data']:\n",
    "                        with tag('tr'):\n",
    "                            for cell in row:\n",
    "                                with tag('td'):\n",
    "                                    if cell[1] is not None:\n",
    "                                        count = 0\n",
    "                                        for t, s in zip(cell[0], cell[1]):\n",
    "                                            if s is not None:\n",
    "                                                with tag('a', href='http://edward.cs.ucsb.edu:6007/query?name=' + s):\n",
    "                                                    text(t)\n",
    "                                                if count < len(cell[1]) - 1:\n",
    "                                                    text(', ')\n",
    "                                            else:\n",
    "                                                text(t)\n",
    "                                    else:\n",
    "                                        text(cell[0])\n",
    "            doc.stag('br')\n",
    "\n",
    "result = doc.getvalue()\n",
    "\n",
    "with open('index.html', 'w') as f:\n",
    "    f.write(indent(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_text(text):\n",
    "    if 'Section::::' in text:\n",
    "        text = text[:text.find('Section::::')]\n",
    "    try:\n",
    "        intro = text.split('\\n\\n')[1]\n",
    "        d = BeautifulSoup(intro)\n",
    "        intro = d.get_text().strip()\n",
    "        return intro\n",
    "    except Exception:\n",
    "        return 'N/A'\n",
    "\n",
    "dictionary = {}\n",
    "with open('en.json') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        d = json.loads(line.strip())\n",
    "        page = '_'.join(d['title'].split(' '))\n",
    "        dictionary[page] = get_text(d['text'])\n",
    "        sys.stdout.write('finished {}/5989879 \\r'.format(i))\n",
    "        \n",
    "with open('wiki-intro-with-ents-dict.json', 'w') as f:\n",
    "    json.dump(dictionary, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed_new_table_postfiltering.json', 'r') as f:\n",
    "    tables = json.load(f)\n",
    "\n",
    "dictionary = {}\n",
    "missed = []\n",
    "\n",
    "succ, fail = 0, 0\n",
    "for table in tables:\n",
    "    for row in table['data']:\n",
    "        for cell in row:\n",
    "            pages = cell[1]\n",
    "            if pages is not None:\n",
    "                for page in pages.split(' '):\n",
    "                    page = page[6:].split('#')[0]\n",
    "                    if page not in database:\n",
    "                        fail += 1\n",
    "                        print(page)\n",
    "                        #database[page]\n",
    "                    else:\n",
    "                        succ += 1\n",
    "\n",
    "sys.stdout.write('success/fail = {}/{} \\r'.format(succ, fail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import urllib3\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "\n",
    "http = urllib3.PoolManager()\n",
    "urllib3.disable_warnings()\n",
    "\n",
    "def get_summary(page):\n",
    "    if page.startswith('https'):\n",
    "        pass\n",
    "    elif page.startswith('/wiki'):\n",
    "        page = 'https://en.wikipedia.org{}'.format(page)\n",
    "    else:\n",
    "        page = 'https://en.wikipedia.org/wiki/{}'.format(page)\n",
    "    \n",
    "    r = http.request('GET', page)\n",
    "    if r.status == 200:\n",
    "        data = r.data.decode('utf-8')\n",
    "        data = data.replace('</p><p>', ' ')        \n",
    "        soup = BeautifulSoup(data, 'html.parser')\n",
    "\n",
    "        div = soup.body.find(\"div\", {\"class\": \"mw-parser-output\"})\n",
    "\n",
    "        children = div.findChildren(\"p\" , recursive=False)\n",
    "        summary = 'N/A'\n",
    "        for child in children:\n",
    "            if child.get_text().strip() != \"\":\n",
    "                html = str(child)\n",
    "                html = html[html.index('>') + 1:].strip()\n",
    "                if not html.startswith('<'):\n",
    "                    summary = child.get_text().strip()\n",
    "                    break\n",
    "                elif html.startswith('<a>') or html.startswith('<b>') or \\\n",
    "                        html.startswith('<i>') or html.startswith('<a ') or html.startswith('<br>'):\n",
    "                    summary = child.get_text().strip()\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "        return summary\n",
    "    elif r.status == 429:\n",
    "        time.sleep(1)\n",
    "        return get_summary(page)\n",
    "    else:\n",
    "        raise\n",
    "\n",
    "get_summary('/wiki/Soe_Myint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dictionary = {}\n",
    "for f in os.listdir('hyperlinks/'):\n",
    "    if f.endswith('json'):\n",
    "        with open('hyperlinks/' + f, 'r') as fw:\n",
    "            d = json.load(fw)\n",
    "            dictionary.update(d)\n",
    "\n",
    "print('totally {}'.format(len(dictionary)))\n",
    "\n",
    "failed = [k for k, v in dictionary.items() if v == 'N/A']\n",
    "print('failed {} items'.format(len(failed)))\n",
    "\n",
    "with open('wikipedia/round1.json', 'w') as f:\n",
    "    json.dump(dictionary, f, indent=2)\n",
    "    \n",
    "with open('wikipedia/round1_failed.json', 'w') as f:\n",
    "    json.dump(failed, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('wikipedia/round1.json') as f:\n",
    "    dictionary = json.load(f)\n",
    "    \n",
    "with open('wikipedia/round2.json') as f:\n",
    "    dictionary.update(json.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(dictionary))\n",
    "import re\n",
    "import json\n",
    "import urllib.parse\n",
    "\n",
    "#for k, v in dictionary.items():\n",
    "#    dictionary[k] = re.sub(r'\\[[\\d]+\\]', '', v).strip()\n",
    "with open('wikipedia/merged.json') as f:\n",
    "    dictionary = json.load(f)\n",
    "\n",
    "merged_unquote = {}\n",
    "for k, v in dictionary.items():\n",
    "    merged_unquote[urllib.parse.unquote(k)] = v\n",
    "\n",
    "with open('wikipedia/merged_unquote.json', 'w') as f:\n",
    "    json.dump(merged_unquote, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('processed_new_table_postfiltering.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for i, d in enumerate(data):\n",
    "    d['idx'] = i\n",
    "    with open('tables/{}.json'.format(i), 'w') as f:\n",
    "        json.dump(d, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for Generating the request data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "from utils import *\n",
    "import re\n",
    "import copy\n",
    "from shutil import copyfile\n",
    "\n",
    "def recover(string):\n",
    "    string = string[6:]\n",
    "    string = string.replace('_', ' ')\n",
    "    return string\n",
    "    \n",
    "def clean_text(k, string):\n",
    "    if \"Initial visibility\" in string:\n",
    "        return recover(k)\n",
    "    \n",
    "    position = string.find(\"mw-parser-output\")\n",
    "    if position != -1:\n",
    "        left_quote = position - 1\n",
    "        while left_quote >= 0 and string[left_quote] != '(':\n",
    "            left_quote -= 1\n",
    "        right_quote = position + 1\n",
    "        while right_quote < len(string) and string[right_quote] != ')':\n",
    "            right_quote += 1\n",
    "        \n",
    "        string = string[:left_quote] + \" \" + string[right_quote + 1:]\n",
    "        \n",
    "        position = string.find(\"mw-parser-output\")\n",
    "        if position != -1:\n",
    "            #print(string)\n",
    "            right_quote = position + 1\n",
    "            while right_quote < len(string) and string[right_quote] != '\\n':\n",
    "                right_quote += 1\n",
    "            #print(\"----------------\")\n",
    "            string = string[:position] + string[right_quote + 1:]\n",
    "            #print(string)\n",
    "            #print(\"################\")\n",
    "    \n",
    "    string = string.replace(u'\\xa0', u' ')\n",
    "    string = string.replace('\\ufeff', '')\n",
    "    string = string.replace(u'\\u200e', u' ')\n",
    "    string = string.replace('–', '-')\n",
    "    string = string.replace(u'\\u2009', u' ')\n",
    "    string = string.replace(u'\\u2010', u' - ')\n",
    "    string = string.replace(u'\\u2011', u' - ')\n",
    "    string = string.replace(u'\\u2012', u' - ')\n",
    "    string = string.replace(u'\\u2013', u' - ')\n",
    "    string = string.replace(u'\\u2014', u' - ')\n",
    "    string = string.replace(u'\\u2015', u' - ')\n",
    "    string = string.replace(u'\\u2018', u'')\n",
    "    string = string.replace(u'\\u2019', u'')\n",
    "    string = string.replace(u'\\u201c', u'')\n",
    "    string = string.replace(u'\\u201d', u'')    \n",
    "    \n",
    "    string = string.replace(u'\"', u'')\n",
    "    string = re.sub(r'[\\n]+', '\\n', string)\n",
    "    \n",
    "    string = re.sub(r'\\.+', '.', string)\n",
    "    string = re.sub(r' +', ' ', string)\n",
    "    \n",
    "    #string = re.sub(r\"'+\", \"'\", string)\n",
    "    #string = string.replace(\" '\", \" \")\n",
    "    #string = string.replace(\"' \", \" \")\n",
    "    string = filter_firstKsents(string, 12)\n",
    "    \n",
    "    return string\n",
    "\n",
    "with open('wikipedia/merged_unquote.json', 'r') as f:\n",
    "    merged_unquote = json.load(f)\n",
    "\n",
    "for k in merged_unquote:\n",
    "    merged_unquote[k] = clean_text(k, merged_unquote[k])\n",
    "\n",
    "def func(f_id):\n",
    "    if f_id.endswith('.json'):\n",
    "        with open('tables/' + f_id) as f:\n",
    "            table = json.load(f)\n",
    "    \n",
    "    local_dict = {}\n",
    "    for d in table['header']:\n",
    "        for url in d[1]:\n",
    "            if url:\n",
    "                url = urllib.parse.unquote(url)\n",
    "                local_dict[url] = merged_unquote[url]\n",
    "    \n",
    "    for row in table['data']:\n",
    "        for cell in row:\n",
    "            for url in cell[1]:\n",
    "                if url:\n",
    "                    url = urllib.parse.unquote(url)\n",
    "                    local_dict[url] = merged_unquote[url]\n",
    "    #count += 1\n",
    "    #sys.stdout.write(\"finished {} tables \\r\".format(count))\n",
    "    with open('request_wo_filter/{}'.format(f_id), 'w') as f:\n",
    "        json.dump(local_dict, f, indent=2)\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "pool = Pool(64)\n",
    "results_func = pool.map(func, os.listdir('tables/'))\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "copyfile('request/example.json', 'request_wo_filter/examples.json')\n",
    "copyfile('request/example_numeric.json', 'request_wo_filter/example_numeric.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"1999-present\"\n",
    "string = re.sub(r'(built)([0-9]{4}) ', r'\\1 \\2 ', string)\n",
    "string = re.sub(r'\\b([0-9]{4})-', r'\\1 - ', string)\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "def clean_cell_text(string):\n",
    "    string = string.replace('\"', '')\n",
    "    string = string.rstrip('^')\n",
    "    string = re.sub(r'(built)([0-9]{4}) ', r'\\1 \\2 ', string)\n",
    "    string = re.sub(r'\\b([0-9]{4})-', r'\\1 - ', string)    \n",
    "    string = string.replace('\"', '')\n",
    "    string = string.replace(u\"\\u00a0\", u' ')\n",
    "    string = string.replace('\\n', ' ')\n",
    "    string = string.rstrip('^')\n",
    "    string = string.replace('\\u200e', '')\n",
    "    string = string.replace('\\ufeff', '')\n",
    "    string = string.replace('–', '-')\n",
    "    string = string.replace(u'\\u2009', u' ')\n",
    "    string = string.replace(u'\\u2010', u' - ')\n",
    "    string = string.replace(u'\\u2011', u' - ')\n",
    "    string = string.replace(u'\\u2012', u' - ')\n",
    "    string = string.replace(u'\\u2013', u' - ')\n",
    "    string = string.replace(u'\\u2014', u' - ')\n",
    "    string = string.replace(u'\\u2015', u' - ')\n",
    "    string = string.replace(u'\\u2018', u'')\n",
    "    string = string.replace(u'\\u2019', u'')\n",
    "    string = string.replace(u'\\u201c', u'')\n",
    "    string = string.replace(u'\\u201d', u'')\n",
    "    string = re.sub(r' +', ' ', string)\n",
    "    string = string.strip()\n",
    "    return string\n",
    "\"\"\"\n",
    "for fn in os.listdir('tables/'):\n",
    "    with open('tables/{}'.format(fn)) as f:\n",
    "        table = json.load(f)\n",
    "    \n",
    "    for row_idx, row in enumerate(table['data']):\n",
    "        for col_idx, cell in enumerate(row):\n",
    "            for i, ent in enumerate(cell[0]):\n",
    "                if ent:\n",
    "                    table['data'][row_idx][col_idx][0][i] = clean_cell_text(ent)\n",
    "    \n",
    "    for col_idx, header in enumerate(table['header']):\n",
    "        for i, ent in enumerate(header[0]):\n",
    "            if ent:\n",
    "                table['header'][col_idx][0][i] = clean_cell_text(ent)\n",
    "    \n",
    "    with open('tables/{}'.format(fn), 'w') as f:\n",
    "        json.dump(table, f, indent=2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_cell_text(\"2004-2005, 2008-present\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding context information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "for f in glob.glob('tables/*.json'):\n",
    "    with open(f) as fn:\n",
    "        table = json.load(fn)\n",
    "        with open('data/' + f) as fn:\n",
    "            reference_table = json.load(fn)\n",
    "        #del table['context']\n",
    "        table['section_title'] = reference_table['section_title']\n",
    "        table['section_text'] = reference_table['section_text']\n",
    "        table['uid'] = reference_table['uid']\n",
    "        with open(f, 'w') as fn:\n",
    "            json.dump(table, fn, indent=2)\n",
    "\n",
    "for f in glob.glob('tables_tok/*.json'):\n",
    "    with open(f) as fn:\n",
    "        table = json.load(fn)\n",
    "    with open('data/' + f) as fn:\n",
    "        reference_table = json.load(fn)\n",
    "    table['section_title'] = reference_table['section_title']\n",
    "    table['section_text'] = reference_table['section_text']\n",
    "    table['uid'] = reference_table['uid']\n",
    "    with open(f, 'w') as fn:\n",
    "        json.dump(table, fn, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_tables = []\n",
    "for f in glob.glob('tables/*.json'):\n",
    "    with open(f) as fn:\n",
    "        table = json.load(fn)\n",
    "        if table['section_title'] != '':\n",
    "            good_tables.append(table['idx'])\n",
    "print(\"there are {} good tables\".format(len(good_tables)))\n",
    "with open('good_table_context.json', 'w') as f:\n",
    "    json.dump(good_tables, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from transformers import *\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda:5')\n",
    "\n",
    "pretrained_weights = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "model = BertModel.from_pretrained(pretrained_weights)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "with open('Mixed-Reasoning/collected_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for d in data:\n",
    "    table_id = d[0]\n",
    "    with open('request/{}.json'.format(table_id)) as f:\n",
    "        requested_documents = json.load(f)\n",
    "    \n",
    "    idx2key = []\n",
    "    tmp = []\n",
    "    for k, v in requested_documents.items():\n",
    "        tokenized_paragraph = tokenizer.tokenize(v)\n",
    "        if len(tokenized_paragraph) < 512:\n",
    "            tokenized_paragraph = tokenized_paragraph[:512]\n",
    "        \n",
    "        idxs = tokenizer.convert_tokens_to_ids(tokenized_paragraph)\n",
    "        tensor = torch.LongTensor(idxs).unsqueeze(0).to(device)\n",
    "        _, r2 = model(tensor)\n",
    "        tmp.append(r2)\n",
    "        idx2key.append(k)\n",
    "    \n",
    "    requested_repr = torch.cat(tmp, 0)\n",
    "    for q, a in d[1:]:\n",
    "        idxs = tokenizer.encode(q)\n",
    "        print(idxs)\n",
    "        tensor = torch.LongTensor(idxs).unsqueeze(0).to(device)\n",
    "        _, r2 = model(tensor)\n",
    "        r2 = r2.repeat(requested_repr.shape[0], 1)\n",
    "        \n",
    "        similarity = torch.nn.functional.cosine_similarity(r2, requested_repr, dim=1)\n",
    "        print(similarity)\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import json\n",
    "from transformers import *\n",
    "import torch\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import nltk.data\n",
    "from difflib import SequenceMatcher\n",
    "from fuzzywuzzy import fuzz\n",
    "from utils import *\n",
    "import re\n",
    "\n",
    "stopWords = set(stopwords.words('english'))\n",
    "tfidf = TfidfVectorizer(strip_accents=\"unicode\", ngram_range=(2, 3), stop_words=stopWords)\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    \n",
    "def longestSubstringFinder(S,T):\n",
    "    S = S.lower()\n",
    "    T = T.lower()\n",
    "    m = len(S)\n",
    "    n = len(T)\n",
    "    counter = [[0]*(n+1) for x in range(m+1)]\n",
    "    longest = 0\n",
    "    lcs_set = set()\n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            if S[i] == T[j]:\n",
    "                c = counter[i][j] + 1\n",
    "                counter[i+1][j+1] = c\n",
    "                if c > longest:\n",
    "                    lcs_set = set()\n",
    "                    longest = c\n",
    "                    lcs_set.add(S[i-c+1:i+1])\n",
    "                elif c == longest:\n",
    "                    lcs_set.add(S[i-c+1:i+1])\n",
    "    \n",
    "    return longest, lcs_set\n",
    "\n",
    "def longest_match_distance(str1s, str2s):\n",
    "    longest_string = []\n",
    "    for str1 in str1s:\n",
    "        longest_string.append([])\n",
    "        for str2 in str2s:\n",
    "            length, _ = longestSubstringFinder(str1, str2)\n",
    "            longest_string[-1].append(1 - length / len(str1))\n",
    "    return longest_string\n",
    "\n",
    "def searchForAnswer(answer, table, passages, mapping_entity):\n",
    "    results = []\n",
    "    correction = None\n",
    "    for i, row in enumerate(table['data']):\n",
    "        for j, cell in enumerate(row):\n",
    "            success = False\n",
    "            for content, url in zip(cell[0], cell[1]):\n",
    "                if answer.lower() == content.lower():\n",
    "                    results.append((content, (i, j), url, 'table'))\n",
    "                    success = True\n",
    "                    break\n",
    "                elif \" \" + answer.lower() + \" \" in \" \" + content.lower() + \" \":\n",
    "                    correction = content\n",
    "                    results.append((content, (i, j), url, 'table'))\n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            if not success and len(cell[0]) > 1:\n",
    "                content = ' , '.join(cell[0])\n",
    "                if answer == content:\n",
    "                    results.append((content, (i, j), None, 'table'))\n",
    "                elif len(answer) > 3 and \" \" + answer.lower() + \" \" in \" \" + content.lower() + \" \":\n",
    "                    correction = content\n",
    "                    results.append((content, (i, j), None, 'table'))\n",
    "\n",
    "    if len(results) > 0:\n",
    "        return correction, results\n",
    "        \n",
    "    for k, v in passages.items():\n",
    "        if \" \" + answer.lower() + \" \" in \" \" + v.lower() + \" \":\n",
    "            for content, locs in mapping_entity[k].items():\n",
    "                for loc in locs:\n",
    "                    results.append((content, loc, k, 'passage'))\n",
    "    \n",
    "    return None, results\n",
    "\n",
    "def searchForAnswerWithoutSpace(answer, passages, mapping_entity):\n",
    "    correction = None\n",
    "    results = []\n",
    "    for k, v in passages.items():\n",
    "        tmp = (\" \" + v.lower()).find(\" \" + answer.lower())\n",
    "        if tmp != -1:\n",
    "            length = len(answer)\n",
    "            while tmp + length < len(v) and v[tmp + length] != \" \":\n",
    "                length += 1\n",
    "            correction = v[tmp:tmp + length]\n",
    "            for content, locs in mapping_entity[k].items():\n",
    "                for loc in locs:\n",
    "                    results.append((content, loc, k, 'passage'))\n",
    "            break\n",
    "\n",
    "    return correction, results\n",
    "\n",
    "def get_edit_distance_equal_1(answer, table):\n",
    "    results = []\n",
    "    for i, row in enumerate(table['data']):\n",
    "        for j, cell in enumerate(row):\n",
    "            for tmp, url in zip(cell[0], cell[1]):\n",
    "                dist = nltk.edit_distance(answer, tmp)\n",
    "                if dist == 1:\n",
    "                    results.append((tmp, (i, j), url, 'table'))\n",
    "    return results\n",
    "\n",
    "def fixing_answer(string):\n",
    "    if ',' in string:\n",
    "        tmp = string.split(',')[0].strip()\n",
    "        if not tmp.isdigit():\n",
    "            string = [tmp]\n",
    "        else:\n",
    "            return None\n",
    "    elif ' and ' in string:\n",
    "        string = [_.strip() for _ in string.split(' and ')]\n",
    "    elif '(' and ')' in string:\n",
    "        tmp = re.sub(r'([^\\(\\)]+) \\((.+)\\)$', r'\\1###\\2', string)\n",
    "        string = [_.strip() for _ in tmp.split('###')]\n",
    "    elif '-' in string:\n",
    "        if ' - ' in string:\n",
    "            tmp = string.replace(' - ', '-')\n",
    "        else:\n",
    "            tmp = string.replace('-', ' - ')\n",
    "        string = [tmp, string.split('-')[0].strip(), string.split('-')[1].strip()]\n",
    "    elif string.startswith('#'):\n",
    "        string = [string.lstrip('#').strip()]\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "    return string\n",
    "        \n",
    "def func(d):\n",
    "    results = []\n",
    "    table_id = d[0]\n",
    "    #if table_id != 351:\n",
    "    #    return []\n",
    "\n",
    "    with open('request_tok/{}.json'.format(table_id)) as f:\n",
    "        requested_documents = json.load(f)\n",
    "    \n",
    "    with open('tables_tok/{}.json'.format(table_id)) as f:\n",
    "        table = json.load(f)\n",
    "    \n",
    "    threshold = 90\n",
    "\n",
    "    #title_wiki = table['url'][len('https://en.wikipedia.org'):]\n",
    "    #del requested_documents[title_wiki]\n",
    "    # Finding the answer and links to table\n",
    "    qs = []\n",
    "    ans = []\n",
    "    links = []\n",
    "    \n",
    "    # Mapping entity link to cell, entity link to surface word\n",
    "    #mapping_entity_loc = {}\n",
    "    mapping_entity = {}\n",
    "    for row_idx, row in enumerate(table['data']):\n",
    "        for col_idx, cell in enumerate(row):\n",
    "            for i, ent in enumerate(cell[1]):\n",
    "                if ent:\n",
    "                    if ent not in mapping_entity:\n",
    "                        mapping_entity[ent] = {cell[0][i]: [(row_idx, col_idx)]}\n",
    "                    else:\n",
    "                        if cell[0][i] not in mapping_entity[ent]:\n",
    "                            mapping_entity[ent][cell[0][i]] = [(row_idx, col_idx)]\n",
    "                        else:\n",
    "                            mapping_entity[ent][cell[0][i]] = mapping_entity[ent][cell[0][i]] + [(row_idx, col_idx)]\n",
    "    \n",
    "    for col_idx, header in enumerate(table['header']):\n",
    "        for i, ent in enumerate(header[1]):\n",
    "            if ent:\n",
    "                if ent not in mapping_entity:\n",
    "                    mapping_entity[ent] = {header[0][i]: [(-1, col_idx)]}\n",
    "                else:\n",
    "                    if header[0][i] not in mapping_entity[ent]:\n",
    "                        mapping_entity[ent][header[0][i]] = [(-1, col_idx)]\n",
    "                    else:\n",
    "                        mapping_entity[ent][header[0][i]] = mapping_entity[ent][header[0][i]] + [(-1, col_idx)]\n",
    "    \n",
    "    # loop through the qa pairs\n",
    "    for q, a in d[1:]:\n",
    "        correction, tmp = searchForAnswer(a, table, requested_documents, mapping_entity)\n",
    "        if len(tmp) == 0 and len(a) >= 3 and not a.isdigit():\n",
    "            # See if the space becomes a problem\n",
    "            correction, tmp = searchForAnswerWithoutSpace(a, requested_documents, mapping_entity)\n",
    "            if len(tmp) > 0:\n",
    "                print(\"correct span! {} -> {}\".format(a, correction))\n",
    "                pass\n",
    "            else:\n",
    "                # correct the spelling                \n",
    "                tmp = get_edit_distance_equal_1(a, table)\n",
    "                if len(tmp) > 0:\n",
    "                    print(\"correct spelling! {} -> {}\".format(a, tmp[0][0]))\n",
    "                    correction = tmp[0][0]\n",
    "                else:\n",
    "                    # Split the answer\n",
    "                    fixed_as = fixing_answer(a)\n",
    "                    if fixed_as:\n",
    "                        for correction in fixed_as:\n",
    "                            #if correction:\n",
    "                            _, tmp = searchForAnswer(correction, table, requested_documents, mapping_entity)\n",
    "                            if len(tmp) > 0:\n",
    "                                print(\"correct splitting! {} -> {}\".format(a, correction))\n",
    "                                break\n",
    "            \n",
    "            if len(tmp) > 4:\n",
    "                print(\"many uncertainties for {}, decide not to replace it\".format(a))\n",
    "                tmp = []\n",
    "            elif correction and len(tmp) > 0:\n",
    "                a = correction.strip()\n",
    "            else:\n",
    "                pass\n",
    "            \n",
    "        elif correction:\n",
    "            if len(correction) < 2 * len(a) : \n",
    "                print(\"cell correction! {} -> {}\".format(a, correction))\n",
    "                a = correction.strip()\n",
    "        \n",
    "        ans.append((a, tmp))\n",
    "        qs.append(q)\n",
    "        \n",
    "        #if len(tmp) == 0:\n",
    "        #    print(\"FAILED with {} {}\".format(table_id, a))\n",
    "        \n",
    "        # LINKING THE CELL DATA\n",
    "        tmp_link = []\n",
    "        for row_idx, row in enumerate(table['data']):\n",
    "            for col_idx, cell in enumerate(row):\n",
    "                if cell[0] != ['']:\n",
    "                    for ent in cell[0]:\n",
    "                        ratio = fuzz.partial_ratio(' ' + ent + ' ', ' ' + q + ' ')\n",
    "                        if ratio > threshold:\n",
    "                            tmp_link.append((ent, (row_idx, col_idx), None, None, ratio / 100))\n",
    "\n",
    "        links.append(tmp_link)\n",
    "    \n",
    "    keys = []\n",
    "    paras = []\n",
    "    for k, v in requested_documents.items():\n",
    "        for _ in tokenizer.tokenize(v):\n",
    "            keys.append(k)\n",
    "            paras.append(_)\n",
    "    \n",
    "    para_feature = tfidf.fit_transform(paras)    \n",
    "    \n",
    "    q_feature = tfidf.transform(qs)\n",
    "    \n",
    "    dist_match = longest_match_distance(qs, paras)\n",
    "    dist = pairwise_distances(q_feature, para_feature, 'cosine')\n",
    "    \n",
    "    for i in range(len(qs)):\n",
    "        min_dist = {}\n",
    "        tfidf_best_match = ('N/A', 1.)\n",
    "        for k, para, d in zip(keys, paras, dist[i]):\n",
    "            if d < min_dist.get(k, 1):\n",
    "                min_dist[k] = d\n",
    "                if d < tfidf_best_match[-1]:\n",
    "                    tfidf_best_match = (k, para, d)\n",
    "\n",
    "        min_dist = {}\n",
    "        string_best_match = ('N/A', 1.)\n",
    "        \n",
    "        for k, para, d in zip(keys, paras, dist_match[i]):\n",
    "            if d < min_dist.get(k, 1):\n",
    "                min_dist[k] = d\n",
    "                if d < string_best_match[-1]:\n",
    "                    string_best_match = (k, para, d)\n",
    "        \n",
    "        tfidf_nodes = []\n",
    "        if tfidf_best_match[0] != 'N/A':\n",
    "            k = tfidf_best_match[0]\n",
    "            for content, locs in mapping_entity[k].items():\n",
    "                for loc in locs:\n",
    "                    tfidf_nodes.append((content, loc, k, tfidf_best_match[1], tfidf_best_match[2]))\n",
    "        \n",
    "        string_nodes = []\n",
    "        if string_best_match[0] != 'N/A':\n",
    "            k = string_best_match[0]\n",
    "            for content, locs in mapping_entity[k].items():\n",
    "                for loc in locs:\n",
    "                    string_nodes.append((content, loc, k, string_best_match[1], string_best_match[2]))   \n",
    "        \n",
    "        results.append({'table_id': table_id, 'question': qs[i], 'answer-text': ans[i][0], \n",
    "                        'answer-node': ans[i][1], 'tf-idf': tfidf_nodes, \n",
    "                        'string-overlap': string_nodes, 'link': links[i]})\n",
    "    \n",
    "    return results\n",
    "\n",
    "with open('Mixed-Reasoning/collected_data.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "pool = Pool(64)\n",
    "results_func = pool.map(func, data)\n",
    "\n",
    "pool.close()\n",
    "pool.join()\n",
    "\n",
    "results = []\n",
    "for _ in results_func:\n",
    "    results.extend(_)\n",
    "\"\"\"\n",
    "results = []\n",
    "for d in data:\n",
    "    results.extend(func(d))\n",
    "\"\"\"\n",
    "\n",
    "with open('Mixed-Reasoning/processed_step1.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open('Mixed-Reasoning/processed_step1.json', 'r') as f:\n",
    "    processed = json.load(f)\n",
    "\n",
    "easy, medium, hard, no_answer, number, yesorno = 0, 0, 0, 0, 0, 0\n",
    "\n",
    "from_passage, from_cell = 0, 0\n",
    "\n",
    "def hash_string(string):\n",
    "    import hashlib\n",
    "    sha = hashlib.sha256()\n",
    "    sha.update(string.encode())\n",
    "    return sha.hexdigest()[:16]\n",
    "\n",
    "new_processed = []\n",
    "for p in processed:    \n",
    "    question_type = ''\n",
    "    where_from = ''\n",
    "    if p['answer-text'].lower() in ['yes', 'no']:\n",
    "        yesorno += 1\n",
    "        #question_type = 'binary'\n",
    "        continue\n",
    "    else:\n",
    "        number_trigger = ['how many', 'how much', 'how long', 'how far', 'how old', 'difference', 'total']\n",
    "        answer_node = p['answer-node']\n",
    "        if len(answer_node) == 0:\n",
    "            if any([x in p['question'].lower() for x in number_trigger]):\n",
    "                number += 1\n",
    "                question_type = 'numeric'\n",
    "                where_from = 'calculation'\n",
    "            else:\n",
    "                no_answer += 1\n",
    "                continue\n",
    "        else:            \n",
    "            if answer_node[0][-1] == 'passage':\n",
    "                from_passage += 1\n",
    "            else:\n",
    "                from_cell += 1\n",
    "                \n",
    "            where_from = answer_node[0][-1]\n",
    "\n",
    "            matching_cells = []\n",
    "            if p['tf-idf']:\n",
    "                matching_cells.extend([tuple(_[1]) for _ in p['tf-idf']])\n",
    "            if p['string-overlap']:\n",
    "                matching_cells.extend([tuple(_[1]) for _ in p['string-overlap']])\n",
    "            linking_cells = [tuple(_[1]) for _ in p['link']]\n",
    "\n",
    "            evidence_cells = set(matching_cells + linking_cells)\n",
    "            #print(answer_node)\n",
    "            answer_cells = set([tuple(_[1]) for _ in answer_node])\n",
    "            \n",
    "            if len(evidence_cells & answer_cells) > 0:\n",
    "                new_answer_nodes = []\n",
    "                for node in p['answer-node']:\n",
    "                    if tuple(node[1]) in evidence_cells:\n",
    "                        new_answer_nodes.append(node)\n",
    "                p['answer-node'] = new_answer_nodes\n",
    "                \n",
    "                easy += 1\n",
    "                question_type = 'easy'\n",
    "            else:            \n",
    "                answer_row = set([_[0] for _ in answer_cells])\n",
    "                evidence_row = set([_[0] for _ in evidence_cells])\n",
    "                intersect_row = answer_row & evidence_row\n",
    "\n",
    "                if len(intersect_row) > 0:\n",
    "                    new_answer_nodes = []\n",
    "                    for node in p['answer-node']:\n",
    "                        if node[1][0] in intersect_row:\n",
    "                            new_answer_nodes.append(node)\n",
    "                    p['answer-node'] = new_answer_nodes\n",
    "                    \n",
    "                    medium += 1\n",
    "                    question_type = 'medium'\n",
    "                else:\n",
    "                    hard += 1\n",
    "                    question_type = 'hard'\n",
    "    \n",
    "    p['type'] = question_type\n",
    "    p['where'] = where_from\n",
    "    p['question_id'] = hash_string(p['question'])\n",
    "    new_processed.append(p)\n",
    "\n",
    "print(\"easy: {}, medium: {}, hard: {}, no answer: {}, yes/no: {}, number: {}\".format(easy, medium, hard, no_answer, yesorno, number))\n",
    "print(\"from cell: {}, from passage: {}\".format(from_cell, from_passage))\n",
    "\n",
    "new_processed = sorted(new_processed, key=lambda x: x['question_id'])\n",
    "#random.shuffle(new_processed)\n",
    "\n",
    "with open('Mixed-Reasoning/processed_step2.json', 'w') as f:\n",
    "    json.dump(new_processed, f, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done with the pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for fn in os.listdir('tables/'):\n",
    "    if fn.endswith('.json'):\n",
    "        with open('tables/{}'.format(fn)) as f:\n",
    "            table = json.load(f)\n",
    "        \n",
    "        headers = table['header']\n",
    "        if headers[0][0] == ['']:\n",
    "            for i in range(len(table['data'])):\n",
    "                del table['data'][i][0]\n",
    "        \n",
    "            del headers[0]\n",
    "\n",
    "            with open('tables/{}'.format(fn), 'w') as f:\n",
    "                json.dump(table, f, indent=2)\n",
    "                \n",
    "for fn in os.listdir('tables/'):\n",
    "    if fn.endswith('.json'):\n",
    "        with open('tables/{}'.format(fn)) as f:\n",
    "            table = json.load(f)\n",
    "\n",
    "        headers = table['header']\n",
    "\n",
    "        if any([_[0] == ['Rank'] for _ in headers]):\n",
    "            if table['data'][0][0][0] == ['']:\n",
    "                for i in range(len(table['data'])):\n",
    "                    if table['data'][i][0][0] == ['']:\n",
    "                        table['data'][i][0][0] = [str(i + 1)]\n",
    "                \n",
    "                with open('tables/{}'.format(fn), 'w') as f:\n",
    "                    json.dump(table, f, indent=2)\n",
    "        \n",
    "        if any([_[0] == ['Place'] for _ in headers]):\n",
    "            if table['data'][0][0][0] == ['']:\n",
    "                for i in range(len(table['data'])):\n",
    "                    if table['data'][i][0][0] == ['']:\n",
    "                        table['data'][i][0][0] = [str(i + 1)]\n",
    "                \n",
    "                with open('tables/{}'.format(fn), 'w') as f:\n",
    "                    json.dump(table, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def is_num(num):\n",
    "    try:\n",
    "        float(num)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "plausible_tables = []\n",
    "for i in range(7000, 15000):\n",
    "    with open('tables/{}.json'.format(i)) as f:\n",
    "        table = json.load(f)\n",
    "    \n",
    "    for column_idx in range(len(table['header'])):      \n",
    "        if all([is_num(table['data'][row_idx][column_idx][0][0]) for row_idx in range(len(table['data']))]):    \n",
    "            plausible_tables.append(i)\n",
    "            break\n",
    "            \n",
    "print(len(plausible_tables))\n",
    "\n",
    "with open('Mixed-Reasoning/numeric_tables.json', 'w') as f:\n",
    "    json.dump(plausible_tables, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open('Mixed-Reasoning/processed_step2.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "mismatch = 0\n",
    "match = 0\n",
    "for d in data:\n",
    "    if d['where'] == 'table':\n",
    "        if d['answer-text'] != d['answer-node'][0][0]:\n",
    "            print(d['answer-node'][0][0], '#', d['answer-text'])\n",
    "            mismatch += 1\n",
    "        else:\n",
    "            match += 1\n",
    "\n",
    "print(\"final match = {}, partial match = {}\".format(match, mismatch))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "easy_set = []\n",
    "for d in new_processed:\n",
    "    if d['type'] == 'easy':\n",
    "        easy_set.append(d)\n",
    "\n",
    "with open('Mixed-Reasoning/processed_step2_easy_split.json', 'w') as f:\n",
    "    json.dump(easy_set, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "from utils import url2dockey, filter_firstKsents\n",
    "from multiprocessing import Pool\n",
    "\n",
    "with open('Mixed-Reasoning/processed_step2.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "keys = []\n",
    "for k in data:\n",
    "    keys.append(k['question_id'])\n",
    "random.shuffle(keys)\n",
    "\n",
    "tr_size = int(len(keys) * 0.9)\n",
    "\n",
    "train_keys = set(keys[:tr_size])\n",
    "dev_keys = set(keys[tr_size:])\n",
    "\n",
    "with open('Mixed-Reasoning/train_ids.json', 'w') as f:\n",
    "    json.dump(list(train_keys), f)\n",
    "with open('Mixed-Reasoning/dev_ids.json', 'w') as f:\n",
    "    json.dump(list(dev_keys), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the stage1/2/3 training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Mixed-Reasoning/processed_step2.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "with open('Mixed-Reasoning/train_ids.json', 'r') as f:\n",
    "    train_keys = set(json.load(f))\n",
    "with open('Mixed-Reasoning/dev_ids.json', 'r') as f:\n",
    "    dev_keys = set(json.load(f))\n",
    "\n",
    "train_split = []\n",
    "dev_split = []\n",
    "\n",
    "for d in data:\n",
    "    if d['type'] in ['medium', 'easy']:\n",
    "        table_id = d['table_id']\n",
    "        with open('tables_tok/{}.json'.format(table_id), 'r') as f:\n",
    "            table = json.load(f)\n",
    "        headers = [\" , \".join(cell[0]) for cell in table['header']]\n",
    "\n",
    "        answer_nodes = d['answer-node']\n",
    "        answer_rows = set([_[1][0] for _ in answer_nodes])\n",
    "\n",
    "        tmp = []\n",
    "        labels = []\n",
    "        for node in d['tf-idf']:\n",
    "            tmp.append(node + [headers[node[1][1]], 'tf-idf'])        \n",
    "            if node[1][0] in answer_rows:\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "        for node in d['string-overlap']:\n",
    "            tmp.append(node + [headers[node[1][1]], 'string-overlap'])        \n",
    "            if node[1][0] in answer_rows:\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "        for node in d['link']:\n",
    "            tmp.append(node + [headers[node[1][1]], 'link'])   \n",
    "            if node[1][0] in answer_rows:\n",
    "                labels.append(1)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "        \n",
    "        if d['question_id'] in train_keys:\n",
    "            train_split.append({'question': d['question'], 'question_id': d['question_id'], 'table_id': d['table_id'], \n",
    "                              'nodes': tmp, 'labels': labels})\n",
    "        else:\n",
    "            dev_split.append({'question': d['question'], 'question_id': d['question_id'], 'table_id': d['table_id'], \n",
    "                              'nodes': tmp, 'labels': labels})\n",
    "\n",
    "with open('Mixed-Reasoning/stage1_training_data.json', 'w') as f:\n",
    "    json.dump(train_split, f, indent=2)\n",
    "\n",
    "with open('Mixed-Reasoning/stage1_dev_data.json', 'w') as f:\n",
    "    json.dump(dev_split, f, indent=2)\n",
    "    \n",
    "print(\"Done with Stage1 Data Processing\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(d):\n",
    "    train_split, dev_split = [], []\n",
    "    if d['type'] in ['medium', 'easy']:\n",
    "        table_id = d['table_id']\n",
    "        with open('tables_tok/{}.json'.format(table_id), 'r') as f:\n",
    "            table = json.load(f)\n",
    "        \n",
    "        with open('request_tok/{}.json'.format(table_id), 'r') as f:\n",
    "            requested_document = json.load(f)\n",
    "        \n",
    "        headers = [\" , \".join(cell[0]) for cell in table['header']]\n",
    "        \n",
    "        answer_nodes = d['answer-node']\n",
    "        answer_rows = {_[1][0]: _ for _ in answer_nodes}\n",
    "\n",
    "        labels = []\n",
    "        for name, source in zip(['tf-idf', 'string-overlap', 'link'], [d['tf-idf'], d['string-overlap'], d['link']]):\n",
    "            for node in source:\n",
    "                i = node[1][0]\n",
    "                if i in answer_rows and i >= 0:\n",
    "                    tmp = {'question': d['question'], 'question_id': d['question_id'], 'table_id': d['table_id'], 'current': node + [headers[node[1][1]], name]}\n",
    "                    target_nodes = []\n",
    "                    labels = []\n",
    "                    same_row = table['data'][i]\n",
    "                    for j, cell in enumerate(same_row):\n",
    "                        for content, url in zip(cell[0], cell[1]):\n",
    "                            if len(content) > 0:\n",
    "                                if url:\n",
    "                                    doc = requested_document[url]\n",
    "                                    intro = filter_firstKsents(doc, 1)\n",
    "                                    target_nodes.append((content, (i, j), url, headers[j], intro))\n",
    "                                    if url == answer_rows[i][2]:\n",
    "                                        labels.append(1)\n",
    "                                    else:\n",
    "                                        labels.append(0)\n",
    "                                else:\n",
    "                                    target_nodes.append((content, (i, j), None, headers[j], ''))\n",
    "                                    if content == answer_rows[i][0]:\n",
    "                                        labels.append(1)\n",
    "                                    else:\n",
    "                                        labels.append(0)\n",
    "                                \n",
    "                        if len(cell[0]) > 1:\n",
    "                            content = ' , '.join(cell[0])\n",
    "                            if content == answer_rows[i][0]:\n",
    "                                labels.append(1)\n",
    "                            else:\n",
    "                                labels.append(0)\n",
    "                                \n",
    "                            target_nodes.append((content, (i, j), None, headers[j], ''))\n",
    "                        \n",
    "                    tmp['labels'] = labels\n",
    "\n",
    "                    assert sum(labels) > 0, d['question_id']\n",
    "                    \n",
    "                    tmp['target'] = target_nodes\n",
    "                    \n",
    "                    if tmp['question_id'] in train_keys:\n",
    "                        train_split.append(tmp)\n",
    "                    else:\n",
    "                        dev_split.append(tmp)\n",
    "\n",
    "    return train_split, dev_split\n",
    "\n",
    "\n",
    "pool = Pool(64)\n",
    "results = pool.map(func, data)\n",
    "\n",
    "train_split = []\n",
    "dev_split = []\n",
    "for r1, r2 in results:\n",
    "    train_split.extend(r1)\n",
    "    dev_split.extend(r2)\n",
    "\n",
    "with open('Mixed-Reasoning/stage2_training_data.json', 'w') as f:\n",
    "    json.dump(train_split, f, indent=2)\n",
    "\n",
    "with open('Mixed-Reasoning/stage2_dev_data.json', 'w') as f:\n",
    "    json.dump(dev_split, f, indent=2)\n",
    "\n",
    "print(\"Done with Stage2 Data Processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split = []\n",
    "dev_split = []\n",
    "inside, self = 0, 0\n",
    "for d in data:\n",
    "    if d['where'] == 'passage':\n",
    "        table_id = d['table_id']\n",
    "        \n",
    "        with open('request_tok/{}.json'.format(table_id)) as f:\n",
    "            requested_documents = json.load(f)        \n",
    "        \n",
    "        #tmp = mapping.get(str(table_id), [])\n",
    "        \n",
    "        used = set()\n",
    "        for node in d['answer-node']:\n",
    "            if node[2] not in used:\n",
    "                context = requested_documents[node[2]]\n",
    "                context = 'Title : {} . '.format(node[0]) + context\n",
    "                \n",
    "                orig_answer = d['answer-text']\n",
    "\n",
    "                start = context.lower().find(orig_answer.lower())\n",
    "\n",
    "                if start == -1:\n",
    "                    import pdb\n",
    "                    pdb.set_trace()\n",
    "\n",
    "                while context[start].lower() != orig_answer[0].lower():\n",
    "                    start -= 1\n",
    "\n",
    "                answer = context[start:start+len(orig_answer)]\n",
    "                #assert(answer.lower() == orig_answer.lower(), \"{} -> {}\".format(answer, orig_answer))\n",
    "                \n",
    "                if d['question_id'] in train_keys:\n",
    "                    train_split.append({'context': context, 'title': table_id, \n",
    "                                      'question': d['question'], 'question_id': d['question_id'],\n",
    "                                      'answers': [{'answer_start': start, 'text': answer}]})\n",
    "                else:\n",
    "                    dev_split.append({'context': context, 'title': table_id, \n",
    "                                      'question': d['question'], 'question_id': d['question_id'],\n",
    "                                      'answers': [{'answer_start': start, 'text': answer}]})\n",
    "                inside += 1\n",
    "                used.add(node[2])\n",
    "            else:\n",
    "                continue\n",
    "    \n",
    "    if d['where'] == 'table':\n",
    "        table_id = d['table_id']\n",
    "        \n",
    "        with open('request_tok/{}.json'.format(table_id)) as f:\n",
    "            requested_documents = json.load(f)  \n",
    "            \n",
    "        used = set()\n",
    "        for node in d['answer-node']:\n",
    "            if node[2] and node[2] not in used:\n",
    "                context = requested_documents[node[2]]\n",
    "                context = 'Title : {} . '.format(node[0]) + context\n",
    "                \n",
    "                orig_answer = node[0]\n",
    "\n",
    "                start = context.lower().find(orig_answer.lower())\n",
    "\n",
    "                if start == -1:\n",
    "                    import pdb\n",
    "                    pdb.set_trace()\n",
    "\n",
    "                while context[start].lower() != orig_answer[0].lower():\n",
    "                    start -= 1\n",
    "                    \n",
    "                answer = context[start:start+len(orig_answer)]\n",
    "                \n",
    "                if d['question_id'] in train_keys:\n",
    "                    train_split.append({'context': context, 'title': table_id, \n",
    "                                      'question': d['question'], 'question_id': d['question_id'],\n",
    "                                      'answers': [{'answer_start': start, 'text': answer}]})\n",
    "                else:\n",
    "                    dev_split.append({'context': context, 'title': table_id, \n",
    "                                      'question': d['question'], 'question_id': d['question_id'],\n",
    "                                      'answers': [{'answer_start': start, 'text': answer}]})\n",
    "                self += 1\n",
    "                used.add(node[2])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "with open('Mixed-Reasoning/stage3_training_data.json', 'w') as f:\n",
    "    json.dump(train_split, f, indent=2)\n",
    "    \n",
    "with open('Mixed-Reasoning/stage3_dev_data.json', 'w') as f:\n",
    "    json.dump(dev_split, f, indent=2)\n",
    "\n",
    "#print(\"Total amount of training instance = {} and dev instance = {}\".format(len(training_data), len(dev_data)))\n",
    "print(\"Looking inside the passage = {} and self loop = {}\".format(inside, self))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming all files into gzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "\n",
    "fs = ['Mixed-Reasoning/stage1_training_data.json', 'Mixed-Reasoning/stage1_dev_data.json',\n",
    "    'Mixed-Reasoning/stage2_training_data.json', 'Mixed-Reasoning/stage2_dev_data.json', \n",
    "    'Mixed-Reasoning/stage3_training_data.json', 'Mixed-Reasoning/stage3_dev_data.json']\n",
    "\n",
    "for f_n in fs:\n",
    "    compressGZip(f_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics of the table/passages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "column_num = 0\n",
    "row_num = 0\n",
    "cell_length = 0\n",
    "\n",
    "total = 13000\n",
    "total_cell = 0\n",
    "total_url = 0\n",
    "for i in range(0, total):\n",
    "    with open('tables_tok/{}.json'.format(i)) as f:\n",
    "        table = json.load(f)\n",
    "    column_num += len(table['header'])\n",
    "    row_num += len(table['data'])\n",
    "    \n",
    "    for row in table['data']:\n",
    "        for cell in row:\n",
    "            content = ' , '.join(cell[0])\n",
    "            cell_length += len(content.split(' '))\n",
    "            total_cell += 1\n",
    "            \n",
    "            for url in cell[1]:\n",
    "                if url:\n",
    "                    total_url += 1\n",
    "    \n",
    "print('column num = {}; row num = {}'.format(column_num / total, row_num / total))\n",
    "print('cell num = {}; average length/cell = {}; average url/table = {}'.format(\n",
    "    total_cell / total, cell_length / total_cell, total_url / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "with open('wikipedia/merged_unquote.json', 'r') as f:\n",
    "    passages = json.load(f)\n",
    "\n",
    "vs = []\n",
    "for k, v in passages.items():\n",
    "    vs.append(tokenizer.tokenize(v)[:12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_length, word_length = 0, 0\n",
    "for v in vs:\n",
    "    sent_length += len(v)\n",
    "    word_length += len((\" \".join(v)).split(' '))\n",
    "\n",
    "print(\"sentence length = {}, word length = {}\".format(sent_length / len(vs), word_length / len(vs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "for f in glob.glob('tables_tok/*.json'):\n",
    "    filename = os.path.basename(f)\n",
    "    try:\n",
    "        tmp = int(filename.replace('.json', ''))\n",
    "        assert tmp < 17000\n",
    "    except Exception:\n",
    "        f2 = f.replace('tables_tok', 'request_tok')\n",
    "        shutil.move(f, 'tables_tok_name/')\n",
    "        shutil.move(f2, 'request_tok_name/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../OpenDomainWikiTables/released_data/train_dev_test_table_ids.json') as f:\n",
    "    all_table_ids = json.load(f)\n",
    "\n",
    "all_table_ids = all_table_ids['train'] + all_table_ids['dev'] + all_table_ids['test']\n",
    "for table_id in all_table_ids:\n",
    "    shutil.copy('../OpenDomainWikiTables/table_crawling/data/tables_tok/{}.json', 'tables_tok_name/')\n",
    "    shutil.copy('../OpenDomainWikiTables/table_crawling/data/tables_tok/{}.json', 'tables_tok_name/')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
