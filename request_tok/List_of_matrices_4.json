{
  "/wiki/Adjugate_matrix": "In linear algebra , the adjugate , classical adjoint , or adjunct of a square matrix is the transpose of its cofactor matrix . The adjugate has sometimes been called the adjoint , but today the adjoint of a matrix normally refers to its corresponding adjoint operator , which is its conjugate transpose",
  "/wiki/Minor_(linear_algebra)": "In linear algebra , a minor of a matrix A is the determinant of some smaller square matrix , cut down from A by removing one or more of its rows and columns . Minors obtained by removing just one row and one column from square matrices ( first minors ) are required for calculating matrix cofactors , which in turn are useful for computing both the determinant and inverse of square matrices",
  "/wiki/Inverse_matrix": "In linear algebra , an n-by-n square matrix A is called invertible ( also nonsingular or nondegenerate ) if there exists an n-by-n square matrix B such that",
  "/wiki/Laplace_expansion": "In linear algebra , the Laplace expansion , named after Pierre-Simon Laplace , also called cofactor expansion , is an expression for the determinant |B| of an n \u00d7 n matrix B that is a weighted sum of the determinants of n sub-matrices ( or minors ) of B , each of size ( n \u2212 1 ) \u00d7 ( n \u2212 1 ) . The Laplace expansion is of didactic interest for its simplicity and as one of several ways to view and compute the determinant . For large matrices , it quickly becomes inefficient to compute when compared to methods using matrix decomposition . The i , j cofactor of the matrix B is the scalar Cij defined by",
  "/wiki/Alternating_sign_matrix": "In mathematics , an alternating sign matrix is a square matrix of 0s , 1s , and \u22121s such that the sum of each row and column is 1 and the nonzero entries in each row and column alternate in sign . These matrices generalize permutation matrices and arise naturally when using Dodgson condensation to compute a determinant . They are also closely related to the six-vertex model with domain wall boundary conditions from statistical mechanics . They were first defined by William Mills , David Robbins , and Howard Rumsey in the former context",
  "/wiki/Dodgson_condensation": "In mathematics , Dodgson condensation is a method of computing the determinants of square matrices . It is named for its inventor , Charles Lutwidge Dodgson ( better known as Lewis Carroll ) . The method in the case of an n \u00d7 n matrix is to construct an ( n \u2212 1 ) \u00d7 ( n \u2212 1 ) matrix , an ( n \u2212 2 ) \u00d7 ( n \u2212 2 ) , and so on , finishing with a 1 \u00d7 1 matrix , which has one entry , the determinant of the original matrix",
  "/wiki/Augmented_matrix": "In linear algebra , an augmented matrix is a matrix obtained by appending the columns of two given matrices , usually for the purpose of performing the same elementary row operations on each of the given matrices . Given the matrices A and B , where A = [ 1 3 2 2 0 1 5 2 2 ] , B = [ 4 3 1 ] , { \\displaystyle A= { \\begin { bmatrix } 1 & 3 & 2\\\\2 & 0 & 1\\\\5 & 2 & 2\\end { bmatrix } } , \\quad B= { \\begin { bmatrix } 4\\\\3\\\\1\\end { bmatrix } } , } the augmented matrix ( A|B ) is written as ( A | B ) = [ 1 3 2 4 2 0 1 3 5 2 2 1 ] . { \\displaystyle ( A|B ) =\\left [ { \\begin { array } { ccc|c } 1 & 3 & 2 & 4\\\\2 & 0 & 1 & 3\\\\5 & 2 & 2 & 1\\end { array } } \\right ] . } This is useful when solving systems of linear equations . For a given number of unknowns , the number of solutions to a system of linear equations depends only on the rank of the matrix representing the system and the rank of the corresponding augmented matrix . Specifically , according to the Rouch\u00e9-Capelli theorem , any system of linear equations is inconsistent ( has no solutions ) if the rank of the augmented matrix is greater than the rank of the coefficient matrix ; if , on the other hand , the ranks of these two matrices are equal , the system must have at least one solution . The solution is unique if and only if the rank equals the number of variables . Otherwise the general solution has k free parameters where k is the difference between the number of variables and the rank ; hence in such a case there are an infinitude of solutions . An augmented matrix may also be used to find the inverse of a matrix by combining it with the identity matrix",
  "/wiki/B\u00e9zout_matrix": "In mathematics , a B\u00e9zout matrix ( or B\u00e9zoutian or Bezoutiant ) is a special square matrix associated with two polynomials , introduced by James Joseph Sylvester ( 1853 ) and Arthur Cayley ( 1857 ) and named after \u00c9tienne B\u00e9zout . B\u00e9zoutian may also refer to the determinant of this matrix , which is equal to the resultant of the two polynomials . B\u00e9zout matrices are sometimes used to test the stability of a given polynomial",
  "/wiki/Control_theory": "Control theory in control systems engineering is a subfield of mathematics that deals with the control of continuously operating dynamical systems in engineered processes and machines . The objective is to develop a control model for controlling such systems using a control action in an optimum manner without delay or overshoot and ensuring control stability . To do this , a controller with the requisite corrective behaviour is required . This controller monitors the controlled process variable ( PV ) , and compares it with the reference or set point ( SP ) . The difference between actual and desired value of the process variable , called the error signal , or SP-PV error , is applied as feedback to generate a control action to bring the controlled process variable to the same value as the set point . Other aspects which are also studied are controllability and observability . This is the basis for the advanced type of automation that revolutionized manufacturing , aircraft , communications and other industries . This is feedback control , which is usually continuous and involves taking measurements using a sensor and making calculated adjustments to keep the measured variable within a set range by means of a final control element , such as a control valve . Extensive use is usually made of a diagrammatic style known as the block diagram . In it the transfer function , also known as the system function or network function , is a mathematical model of the relation between the input and output based on the differential equations describing the system . Control theory dates from the 19th century , when the theoretical basis for the operation of governors was first described by James Clerk Maxwell . Control theory was further advanced by Edward Routh in 1874 , Charles Sturm and in 1895 , Adolf Hurwitz , who all contributed to the establishment of control stability criteria ; and from 1922 onwards , the development of PID control theory by Nicolas Minorsky",
  "/wiki/Stable_polynomial": "In the context of the characteristic polynomial of a differential equation or difference equation , a polynomial is said to be stable if either :",
  "/wiki/Carleman_matrix": "In mathematics , a Carleman matrix is a matrix used to convert function composition into matrix multiplication . It is often used in iteration theory to find the continuous iteration of functions which can not be iterated by pattern recognition alone . Other uses of Carleman matrices occur in the theory of probability generating functions , and Markov chains",
  "/wiki/Cartan_matrix": "In mathematics , the term Cartan matrix has three meanings . All of these are named after the French mathematician \u00c9lie Cartan . Amusingly , the Cartan matrices in the context of Lie algebras were first investigated by Wilhelm Killing , whereas the Killing form is due to Cartan . [ citation needed ]",
  "/wiki/Associative_algebra": "In mathematics , an associative algebra is an algebraic structure with compatible operations of addition , multiplication ( assumed to be associative ) , and a scalar multiplication by elements in some field . The addition and multiplication operations together give A the structure of a ring ; the addition and scalar multiplication operations together give A the structure of a vector space over K. In this article we will also use the term K-algebra to mean an associative algebra over the field K. A standard first example of a K-algebra is a ring of square matrices over a field K , with the usual matrix multiplication . In this article associative algebras are assumed to have a multiplicative identity , denoted 1 ; they are sometimes called unital associative algebras for clarification . In some areas of mathematics this assumption is not made , and we will call such structures non-unital associative algebras . We will also assume that all rings are unital , and all ring homomorphisms are unital . Many authors consider the more general concept of an associative algebra over a commutative ring R , instead of a field : An R-algebra is an R-module with an associative R-bilinear binary operation , which also contains a multiplicative identity . For examples of this concept , if S is any ring with center C , then S is an associative C-algebra",
  "/wiki/Semisimple_Lie_algebra": "In mathematics , a Lie algebra is semisimple if it is a direct sum of simple Lie algebras , i.e. , non-abelian Lie algebras g { \\displaystyle { \\mathfrak { g } } } whose only ideals are { 0 } and g { \\displaystyle { \\mathfrak { g } } } itself . Throughout the article , unless otherwise stated , a Lie algebra is a finite-dimensional Lie algebra over a field of characteristic 0 . For such a Lie algebra g { \\displaystyle { \\mathfrak { g } } } , if nonzero , the following conditions are equivalent :",
  "/wiki/Circulant_matrix": "In linear algebra , a circulant matrix is a special kind of Toeplitz matrix where each row vector is rotated one element to the right relative to the preceding row vector . In numerical analysis , circulant matrices are important because they are diagonalized by a discrete Fourier transform , and hence linear equations that contain them may be quickly solved using a fast Fourier transform . They can be interpreted analytically as the integral kernel of a convolution operator on the cyclic group C n { \\displaystyle C_ { n } } and hence frequently appear in formal descriptions of spatially invariant linear operations . In cryptography , a circulant matrix is used in the MixColumns step of the Advanced Encryption Standard",
  "/wiki/System_of_linear_equations": "In mathematics , a system of linear equations ( or linear system ) is a collection of one or more linear equations involving the same set of variables . For example ,",
  "/wiki/Discrete_Fourier_transform": "In mathematics , the discrete Fourier transform ( DFT ) converts a finite sequence of equally-spaced samples of a function into a same-length sequence of equally-spaced samples of the discrete-time Fourier transform ( DTFT ) , which is a complex-valued function of frequency . The interval at which the DTFT is sampled is the reciprocal of the duration of the input sequence . An inverse DFT is a Fourier series , using the DTFT samples as coefficients of complex sinusoids at the corresponding DTFT frequencies . It has the same sample-values as the original input sequence . The DFT is therefore said to be a frequency domain representation of the original input sequence . If the original sequence spans all the non-zero values of a function , its DTFT is continuous ( and periodic ) , and the DFT provides discrete samples of one cycle . If the original sequence is one cycle of a periodic function , the DFT provides all the non-zero values of one DTFT cycle . The DFT is the most important discrete transform , used to perform Fourier analysis in many practical applications . In digital signal processing , the function is any quantity or signal that varies over time , such as the pressure of a sound wave , a radio signal , or daily temperature readings , sampled over a finite time interval ( often defined by a window function ) . In image processing , the samples can be the values of pixels along a row or column of a raster image . The DFT is also used to efficiently solve partial differential equations , and to perform other operations such as convolutions or multiplying large integers . Since it deals with a finite amount of data , it can be implemented in computers by numerical algorithms or even dedicated hardware",
  "/wiki/Cofactor_matrix": "In linear algebra , a minor of a matrix A is the determinant of some smaller square matrix , cut down from A by removing one or more of its rows and columns . Minors obtained by removing just one row and one column from square matrices ( first minors ) are required for calculating matrix cofactors , which in turn are useful for computing both the determinant and inverse of square matrices",
  "/wiki/Cofactor_(linear_algebra)": "In linear algebra , a minor of a matrix A is the determinant of some smaller square matrix , cut down from A by removing one or more of its rows and columns . Minors obtained by removing just one row and one column from square matrices ( first minors ) are required for calculating matrix cofactors , which in turn are useful for computing both the determinant and inverse of square matrices",
  "/wiki/Commutation_matrix": "In mathematics , especially in linear algebra and matrix theory , the commutation matrix is used for transforming the vectorized form of a matrix into the vectorized form of its transpose . Specifically , the commutation matrix K ( m , n ) is the nm \u00d7 mn matrix which , for any m \u00d7 n matrix A , transforms vec ( A ) into vec ( AT ) :",
  "/wiki/Coxeter_matrix": "In mathematics , a Coxeter group , named after H. S. M. Coxeter , is an abstract group that admits a formal description in terms of reflections ( or kaleidoscopic mirrors ) . Indeed , the finite Coxeter groups are precisely the finite Euclidean reflection groups ; the symmetry groups of regular polyhedra are an example . However , not all Coxeter groups are finite , and not all can be described in terms of symmetries and Euclidean reflections . Coxeter groups were introduced ( Coxeter 1934 ) as abstractions of reflection groups , and finite Coxeter groups were classified in 1935 ( Coxeter 1935 ) . Coxeter groups find applications in many areas of mathematics . Examples of finite Coxeter groups include the symmetry groups of regular polytopes , and the Weyl groups of simple Lie algebras . Examples of infinite Coxeter groups include the triangle groups corresponding to regular tessellations of the Euclidean plane and the hyperbolic plane , and the Weyl groups of infinite-dimensional Kac-Moody algebras . Standard references include ( Humphreys 1992 ) and ( Davis 2007 )",
  "/wiki/Coxeter_groups": "In mathematics , a Coxeter group , named after H. S. M. Coxeter , is an abstract group that admits a formal description in terms of reflections ( or kaleidoscopic mirrors ) . Indeed , the finite Coxeter groups are precisely the finite Euclidean reflection groups ; the symmetry groups of regular polyhedra are an example . However , not all Coxeter groups are finite , and not all can be described in terms of symmetries and Euclidean reflections . Coxeter groups were introduced ( Coxeter 1934 ) as abstractions of reflection groups , and finite Coxeter groups were classified in 1935 ( Coxeter 1935 ) . Coxeter groups find applications in many areas of mathematics . Examples of finite Coxeter groups include the symmetry groups of regular polytopes , and the Weyl groups of simple Lie algebras . Examples of infinite Coxeter groups include the triangle groups corresponding to regular tessellations of the Euclidean plane and the hyperbolic plane , and the Weyl groups of infinite-dimensional Kac-Moody algebras . Standard references include ( Humphreys 1992 ) and ( Davis 2007 )",
  "/wiki/Symmetry": "Symmetry ( from Greek \u03c3\u03c5\u03bc\u03bc\u03b5\u03c4\u03c1\u03af\u03b1 symmetria agreement in dimensions , due proportion , arrangement ) in everyday language refers to a sense of harmonious and beautiful proportion and balance . [ a ] In mathematics , symmetry has a more precise definition , and is usually used to refer to an object that is invariant under some transformations ; including translation , reflection , rotation or scaling . Although these two meanings of symmetry can sometimes be told apart , they are intricately related , and hence are discussed together in this article . Mathematical symmetry may be observed with respect to the passage of time ; as a spatial relationship ; through geometric transformations ; through other kinds of functional transformations ; and as an aspect of abstract objects , including theoretic models , language , and music . [ b ] This article describes symmetry from three perspectives : in mathematics , including geometry , the most familiar type of symmetry for many people ; in science and nature ; and in the arts , covering architecture , art and music . The opposite of symmetry is asymmetry , which refers to the absence or a violation of symmetry",
  "/wiki/Minimal_polynomial_(linear_algebra)": "In linear algebra , the minimal polynomial \u03bcA of an n \u00d7 n matrix A over a field F is the monic polynomial P over F of least degree such that P ( A ) = 0 . Any other polynomial Q with Q ( A ) = 0 is a ( polynomial ) multiple of \u03bcA . The following three statements are equivalent :",
  "/wiki/Distance_matrix": "In mathematics , computer science and especially graph theory , a distance matrix is a square matrix ( two-dimensional array ) containing the distances , taken pairwise , between the elements of a set . Depending upon the application involved , the distance being used to define this matrix may or may not be a metric . If there are N elements , this matrix will have size N\u00d7N . In graph-theoretic applications the elements are more often referred to as points , nodes or vertices",
  "/wiki/Point_(geometry)": "In modern mathematics , a point refers usually to an element of some set called a space . More specifically , in Euclidean geometry , a point is a primitive notion upon which the geometry is built , meaning that a point can not be defined in terms of previously defined objects . That is , a point is defined only by some properties , called axioms , that it must satisfy . In particular , the geometric points do not have any length , area , volume or any other dimensional attribute . A common interpretation is that the concept of a point is meant to capture the notion of a unique location in Euclidean space",
  "/wiki/Computer_vision": "Computer vision is an interdisciplinary scientific field that deals with how computers can be made to gain high-level understanding from digital images or videos . From the perspective of engineering , it seeks to automate tasks that the human visual system can do . Computer vision tasks include methods for acquiring , processing , analyzing and understanding digital images , and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information , e.g . in the forms of decisions . Understanding in this context means the transformation of visual images ( the input of the retina ) into descriptions of the world that can interface with other thought processes and elicit appropriate action . This image understanding can be seen as the disentangling of symbolic information from image data using models constructed with the aid of geometry , physics , statistics , and learning theory . The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images . The image data can take many forms , such as video sequences , views from multiple cameras , or multi-dimensional data from a medical scanner . The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems . Sub-domains of computer vision include scene reconstruction , event detection , video tracking , object recognition , 3D pose estimation , learning , indexing , motion estimation , and image restoration",
  "/wiki/Network_analysis_(electronics)": "A network , in the context of electronics , is a collection of interconnected components . Network analysis is the process of finding the voltages across , and the currents through , all network components . There are many techniques for calculating these values . However , for the most part , the techniques assume linear components . Except where stated , the methods described in this article are applicable only to linear network analysis",
  "/wiki/Duplication_matrix": "In mathematics , especially in linear algebra and matrix theory , the duplication matrix and the elimination matrix are linear transformations used for transforming half-vectorizations of matrices into vectorizations or ( respectively ) vice versa",
  "/wiki/Vectorization_(mathematics)": "In mathematics , especially in linear algebra and matrix theory , the vectorization of a matrix is a linear transformation which converts the matrix into a column vector . Specifically , the vectorization of an m \u00d7 n matrix A , denoted vec ( A ) , is the mn \u00d7 1 column vector obtained by stacking the columns of the matrix A on top of one another :",
  "/wiki/Elimination_matrix": "In mathematics , especially in linear algebra and matrix theory , the duplication matrix and the elimination matrix are linear transformations used for transforming half-vectorizations of matrices into vectorizations or ( respectively ) vice versa",
  "/wiki/Euclidean_distance_matrix": "In mathematics , a Euclidean distance matrix is an n\u00d7n matrix representing the spacing of a set of n points in Euclidean space . If A is a Euclidean distance matrix and the points x 1 , x 2 , \u2026 , x n { \\displaystyle x_ { 1 } , x_ { 2 } , \\ldots , x_ { n } } are defined on m-dimensional space , then the elements of A are given by",
  "/wiki/Euclidean_space": "Euclidean space is the fundamental space of classical geometry . Originally it was the three-dimensional space of Euclidean geometry , but in modern mathematics there are Euclidean spaces of any nonnegative integer dimension , including the three-dimensional space and the Euclidean plane ( dimension two ) . It was introduced by the Ancient Greek mathematician Euclid of Alexandria , and the qualifier Euclidean is used to distinguish it from other spaces that were later discovered in physics and modern mathematics . Ancient Greek geometers introduced Euclidean space for modeling the physical universe . Their great innovation was to prove all properties of the space as theorems by starting from a few fundamental properties , called postulates , which either were considered as evident ( for example , there is exactly one straight line passing through two points ) , or seemed impossible to prove ( parallel postulate ) . After the introduction at the end of 19th century of non-Euclidean geometries , the old postulates were re-formalized to define Euclidean spaces through axiomatic theory . Another definition of Euclidean spaces by mean of vector spaces and linear algebra has been shown to be equivalent to the axiomatic definition . It is this definition that is more commonly used in modern mathematics , and detailed in this article . In all definitions , Euclidean spaces consist of points , which are defined only by the properties that they must have for forming a Euclidean space . There is essentially only one Euclidean space of each dimension ; that is , all Euclidean spaces of a given dimension are isomorphic . Therefore , in many cases , it is possible to work with a specific Euclidean space , which is generally the real n-space R n , { \\displaystyle \\mathbb { R } ^ { n } , } equipped with the dot product . An isomorphism from a Euclidean space to R n { \\displaystyle \\mathbb { R } ^ { n } } associates with each point an n-tuple of real numbers , which locate them in the Euclidean space and are called Cartesian coordinates",
  "/wiki/Fundamental_matrix_(linear_differential_equation)": "In mathematics , a fundamental matrix of a system of n homogeneous linear ordinary differential equations",
  "/wiki/Ordinary_differential_equation": "In mathematics , an ordinary differential equation ( ODE ) is a differential equation containing one or more functions of one independent variable and the derivatives of those functions . The term ordinary is used in contrast with the term partial differential equation which may be with respect to more than one independent variable",
  "/wiki/Generator_matrix": "In coding theory , a generator matrix is a matrix whose rows form a basis for a linear code . The codewords are all of the linear combinations of the rows of this matrix , that is , the linear code is the row space of its generator matrix",
  "/wiki/Linear_code": "In coding theory , a linear code is an error-correcting code for which any linear combination of codewords is also a codeword . Linear codes are traditionally partitioned into block codes and convolutional codes , although turbo codes can be seen as a hybrid of these two types . Linear codes allow for more efficient encoding and decoding algorithms than other codes ( cf . syndrome decoding ) . [ citation needed ] Linear codes are used in forward error correction and are applied in methods for transmitting symbols ( e.g. , bits ) on a communications channel so that , if errors occur in the communication , some errors can be corrected or detected by the recipient of a message block . The codewords in a linear block code are blocks of symbols that are encoded using more symbols than the original value to be sent . A linear code of length n transmits blocks containing n symbols . For example , the [ 7,4,3 ] Hamming code is a linear binary code which represents 4-bit messages using 7-bit codewords . Two distinct codewords differ in at least three bits . As a consequence , up to two errors per codeword can be detected while a single error can be corrected . This code contains 24=16 codewords",
  "/wiki/Coding_theory": "Coding theory is the study of the properties of codes and their respective fitness for specific applications . Codes are used for data compression , cryptography , error detection and correction , data transmission and data storage . Codes are studied by various scientific disciplines - such as information theory , electrical engineering , mathematics , linguistics , and computer science - for the purpose of designing efficient and reliable data transmission methods . This typically involves the removal of redundancy and the correction or detection of errors in the transmitted data . There are four types of coding :",
  "/wiki/Gramian_matrix": "In linear algebra , the Gram matrix ( a. k. a. Gramian matrix or Gramian ) of a set of vectors v 1 , \u2026 , v n { \\displaystyle v_ { 1 } , \\dots , v_ { n } } in an inner product space is the Hermitian matrix of inner products , whose entries are given by G i j = \u27e8 v i , v j \u27e9 { \\displaystyle G_ { ij } =\\langle v_ { i } , v_ { j } \\rangle } . An important application is to compute linear independence : a set of vectors are linearly independent if and only if the Gram determinant ( the determinant of the Gram matrix ) is non-zero . It is named after J\u00f8rgen Pedersen Gram",
  "/wiki/Inner_product_space": "In linear algebra , an inner product space is a vector space with an additional structure called an inner product . This additional structure associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors . Inner products allow the rigorous introduction of intuitive geometrical notions such as the length of a vector or the angle between two vectors . They also provide the means of defining orthogonality between vectors ( zero inner product ) . Inner product spaces generalize Euclidean spaces ( in which the inner product is the dot product , also known as the scalar product ) to vector spaces of any ( possibly infinite ) dimension , and are studied in functional analysis . The first usage of the concept of a vector space with an inner product is due to Giuseppe Peano , in 1898 . An inner product naturally induces an associated norm , ( x and y are the norm in the picture ) thus an inner product space is also a normed vector space . A complete space with an inner product is called a Hilbert space . An ( incomplete ) space with an inner product is called a pre-Hilbert space , since its completion with respect to the norm induced by the inner product is a Hilbert space . Inner product spaces over the field of complex numbers are sometimes referred to as unitary spaces",
  "/wiki/Linear_independence": "In the theory of vector spaces , a set of vectors is said to be linearly dependent if at least one of the vectors in the set can be defined as a linear combination of the others ; if no vector in the set can be written in this way , then the vectors are said to be linearly independent . These concepts are central to the definition of dimension . A vector space can be of finite-dimension or infinite-dimension depending on the number of linearly independent basis vectors . The definition of linear dependence and the ability to determine whether a subset of vectors in a vector space is linearly dependent are central to determining a basis for a vector space",
  "/wiki/Function_space": "In mathematics , a function space is a set of functions between two fixed sets . Often , the domain and/or codomain will have additional structure which is inherited by the function space . For example , the set of functions from any set X into a vector space has a natural vector space structure given by pointwise addition and scalar multiplication . In other scenarios , the function space might inherit a topological or metric structure , hence the name function space",
  "/wiki/Hessian_matrix": "In mathematics , the Hessian matrix or Hessian is a square matrix of second-order partial derivatives of a scalar-valued function , or scalar field . It describes the local curvature of a function of many variables . The Hessian matrix was developed in the 19th century by the German mathematician Ludwig Otto Hesse and later named after him . Hesse originally used the term functional determinants",
  "/wiki/Partial_derivative": "In mathematics , a partial derivative of a function of several variables is its derivative with respect to one of those variables , with the others held constant ( as opposed to the total derivative , in which all variables are allowed to vary ) . Partial derivatives are used in vector calculus and differential geometry . The partial derivative of a function f ( x , y , \u2026 ) { \\displaystyle f ( x , y , \\dots ) } with respect to the variable x { \\displaystyle x } is variously denoted by",
  "/wiki/Local_minimum": "In mathematical analysis , the maxima and minima ( the respective plurals of maximum and minimum ) of a function , known collectively as extrema ( the plural of extremum ) , are the largest and smallest value of the function , either within a given range ( the local or relative extrema ) or on the entire domain of a function ( the global or absolute extrema ) . Pierre de Fermat was one of the first mathematicians to propose a general technique , adequality , for finding the maxima and minima of functions . As defined in set theory , the maximum and minimum of a set are the greatest and least elements in the set , respectively . Unbounded infinite sets , such as the set of real numbers , have no minimum or maximum",
  "/wiki/Blob_detection": "In computer vision , blob detection methods are aimed at detecting regions in a digital image that differ in properties , such as brightness or color , compared to surrounding regions . Informally , a blob is a region of an image in which some properties are constant or approximately constant ; all the points in a blob can be considered in some sense to be similar to each other . The most common method for blob detection is convolution . Given some property of interest expressed as a function of position on the image , there are two main classes of blob detectors : ( i ) differential methods , which are based on derivatives of the function with respect to position , and ( ii ) methods based on local extrema , which are based on finding the local maxima and minima of the function . With the more recent terminology used in the field , these detectors can also be referred to as interest point operators , or alternatively interest region operators ( see also interest point detection and corner detection ) . There are several motivations for studying and developing blob detectors . One main reason is to provide complementary information about regions , which is not obtained from edge detectors or corner detectors . In early work in the area , blob detection was used to obtain regions of interest for further processing . These regions could signal the presence of objects or parts of objects in the image domain with application to object recognition and/or object tracking . In other domains , such as histogram analysis , blob descriptors can also be used for peak detection with application to segmentation . Another common use of blob descriptors is as main primitives for texture analysis and texture recognition . In more recent work , blob descriptors have found increasingly popular use as interest points for wide baseline stereo matching and to signal the presence of informative image features for appearance-based object recognition based on local image statistics",
  "/wiki/Householder_transformation": "In linear algebra , a Householder transformation ( also known as a Householder reflection or elementary reflector ) is a linear transformation that describes a reflection about a plane or hyperplane containing the origin . The Householder transformation was used in a 1958 paper by Alston Scott Householder . Its analogue over general inner product spaces is the Householder operator",
  "/wiki/QR_decomposition": "In linear algebra , a QR decomposition , also known as a QR factorization or QU factorization is a decomposition of a matrix A into a product A = QR of an orthogonal matrix Q and an upper triangular matrix R. QR decomposition is often used to solve the linear least squares problem and is the basis for a particular eigenvalue algorithm , the QR algorithm"
}