{
  "/wiki/Matrix_congruence": "In mathematics, two square matrices A and B over a field are called congruent if there exists an invertible matrix P  over the same field such that",
  "/wiki/EP_matrix": "In mathematics, an EP matrix (or range-Hermitian matrix or RPN matrix) is a matrix A whose range is equal to the range of its conjugate transpose A*. Another equivalent characterization of EP matrices is that the range of A is orthogonal to the nullspace of A. Thus, EP matrices are also known as RPN matrices, with RPN meaning Range Perpendicular to Nullspace.   \n EP matrices were introduced in 1950 by Hans Schwerdtfeger, and since then, many equivalent characterizations of EP matrices have been investigated through the literature. The meaning of the EP abbreviation stands originally for Equal Principal, but it is widely believed that it stands for Equal Projectors instead, since an equivalent characterization of EP matrices is based in terms of equality of the projectors AA+ and A+A.\n According to the fundamental theorem of linear algebra, the range of any matrix A is perpendicular to the null-space of A*, but is not necessarily perpendicular to the null-space of A. When A is an EP matrix, the range of A is precisely perpendicular to the null-space of A.",
  "/wiki/Moore\u2013Penrose_inverse": "In mathematics, and in particular linear algebra, the Moore\u2013Penrose inverse \n\n\n\n\nA\n\n+\n\n\n\n\n{\\displaystyle A^{+}}\n\n of a  matrix \n\n\n\nA\n\n\n{\\displaystyle A}\n\n is the most widely known generalization of the inverse matrix. It was independently described by E. H. Moore in 1920, Arne Bjerhammar in 1951, and Roger Penrose in 1955. Earlier, Erik Ivar Fredholm had introduced the concept of a pseudoinverse of integral operators in 1903. When referring to a matrix, the term pseudoinverse, without further specification, is often used to indicate the Moore\u2013Penrose inverse. The term generalized inverse is sometimes used as a synonym for pseudoinverse.\n A common use of the pseudoinverse is to compute a \"best fit\" (least squares) solution to a system of linear equations that lacks a unique solution (see below under \u00a7 Applications).\nAnother use is to find the minimum (Euclidean) norm solution to a system of linear equations with multiple solutions. The pseudoinverse facilitates the statement and proof of results in linear algebra.\n The pseudoinverse is defined and unique for all matrices whose entries are real or complex numbers. It can be computed using the singular value decomposition.",
  "/wiki/Idempotent_matrix": "In linear algebra, an idempotent matrix is a matrix which, when multiplied by itself, yields itself. That is, the matrix \n\n\n\nA\n\n\n{\\displaystyle A}\n\n is idempotent if and only if \n\n\n\n\nA\n\n2\n\n\n=\nA\n\n\n{\\displaystyle A^{2}=A}\n\n. For this product \n\n\n\n\nA\n\n2\n\n\n\n\n{\\displaystyle A^{2}}\n\n to be defined, \n\n\n\nA\n\n\n{\\displaystyle A}\n\n must necessarily be a square matrix. Viewed this way, idempotent matrices are idempotent elements of matrix rings.",
  "/wiki/Projection_(linear_algebra)": "In linear algebra and functional analysis, a projection is a linear transformation \n\n\n\nP\n\n\n{\\displaystyle P}\n\n from a vector space to itself such that \n\n\n\n\nP\n\n2\n\n\n=\nP\n\n\n{\\displaystyle P^{2}=P}\n\n. That is, whenever \n\n\n\nP\n\n\n{\\displaystyle P}\n\n is applied twice to any value, it gives the same result as if it were applied once (idempotent). It leaves its image unchanged. Though abstract, this definition of \"projection\" formalizes and generalizes the idea of graphical projection.  One can also consider the effect of a projection on a geometrical object by examining the effect of the projection on points in the object.",
  "/wiki/Invertible_matrix": "In linear algebra, an n-by-n square matrix A is called invertible (also nonsingular or nondegenerate) if there exists an n-by-n square matrix B such that",
  "/wiki/Inverse_matrix": "In linear algebra, an n-by-n square matrix A is called invertible (also nonsingular or nondegenerate) if there exists an n-by-n square matrix B such that",
  "/wiki/General_linear_group": "In mathematics, the general linear group of degree n is the set of n\u00d7n invertible matrices, together with the operation of ordinary matrix multiplication. This forms a group, because the product of two invertible matrices is again invertible, and the inverse of an invertible matrix is invertible, with identity matrix as the identity element of the group. The group is so named because the columns of an invertible matrix are linearly independent, hence the vectors/points they define are in general linear position, and matrices in the general linear group take points in general linear position to points in general linear position.\n To be more precise, it is necessary to specify what kind of objects may appear in the entries of the matrix. For example, the general linear group over R (the set of real numbers) is the group of n\u00d7n invertible matrices of real numbers, and is denoted by GLn(R) or GL(n, R).\n More generally, the general linear group of degree n over any field F (such as the complex numbers), or a ring R (such as the ring of integers), is the set of n\u00d7n invertible matrices with entries from F (or R), again with matrix multiplication as the group operation. Typical notation is GLn(F) or GL(n, F), or simply GL(n) if the field is understood.\n More generally still, the general linear group of a vector space GL(V) is the abstract automorphism group, not necessarily written as matrices.\n The special linear group, written SL(n, F) or SLn(F), is the subgroup of GL(n, F) consisting of matrices with a determinant of 1.\n The group GL(n, F) and its subgroups are often called linear groups or matrix groups (the abstract group GL(V) is a linear group but not a matrix group). These groups are important in the theory of group representations, and also arise in the study of spatial symmetries and symmetries of vector spaces in general, as well as the study of polynomials. The modular group may be realised as a quotient of the special linear group SL(2, Z).\n If n \u2265 2, then the group GL(n, F) is not abelian.",
  "/wiki/Involutory_matrix": "In mathematics, an involutory matrix is a matrix that is its own inverse. That is, multiplication by matrix A is an involution if and only if A2 = I. Involutory matrices are all square roots of the identity matrix. This is simply a consequence of the fact that any nonsingular matrix multiplied by its inverse is the identity.",
  "/wiki/Signature_matrix": "In mathematics, a signature matrix is  a diagonal matrix whose diagonal elements are plus or minus 1, that is, any matrix of the form:",
  "/wiki/Householder_transformation": "In linear algebra, a Householder transformation (also known as a Householder reflection or elementary reflector) is a linear transformation that describes a reflection about a plane or hyperplane containing the origin. The Householder transformation was used in a 1958 paper by Alston Scott Householder.\n Its analogue over general inner product spaces is the Householder operator.",
  "/wiki/Nilpotent_matrix": "In linear algebra, a nilpotent matrix is a square matrix N such that",
  "/wiki/Normal_matrix": "In mathematics, a complex square matrix A is normal if it commutes with its conjugate transpose A*:",
  "/wiki/Conjugate_transpose": "In mathematics, the conjugate transpose or Hermitian transpose of an m-by-n matrix \n\n\n\n\nA\n\n\n\n{\\displaystyle {\\boldsymbol {A}}}\n\n with complex entries is the n-by-m matrix \n\n\n\n\n\nA\n\n\n\nH\n\n\n\n\n\n{\\displaystyle {\\boldsymbol {A}}^{\\mathrm {H} }}\n\n obtained from \n\n\n\n\nA\n\n\n\n{\\displaystyle {\\boldsymbol {A}}}\n\n by taking the transpose and then taking the complex conjugate of each entry. (The complex conjugate of \n\n\n\na\n+\ni\nb\n\n\n{\\displaystyle a+ib}\n\n, where \n\n\n\na\n\n\n{\\displaystyle a}\n\n and \n\n\n\nb\n\n\n{\\displaystyle b}\n\n are real numbers, is \n\n\n\na\n\u2212\ni\nb\n\n\n{\\displaystyle a-ib}\n\n.)",
  "/wiki/Spectral_theorem": "In mathematics, particularly linear algebra and functional analysis, a spectral theorem is a result about when a linear operator or matrix can be diagonalized (that is, represented as a diagonal matrix in some basis). This is extremely useful because computations involving a diagonalizable matrix can often be reduced to much simpler computations involving the corresponding diagonal matrix. The concept of diagonalization is relatively straightforward for operators on finite-dimensional vector spaces but requires some modification for operators on infinite-dimensional spaces. In general, the spectral theorem identifies a class of linear operators that can be modeled by multiplication operators, which are as simple as one can hope to find. In more abstract language, the spectral theorem is a statement about commutative C*-algebras. See also spectral theory for a historical perspective.\n Examples of operators to which the spectral theorem applies are self-adjoint operators or more generally normal operators on Hilbert spaces.\n The spectral theorem also provides a canonical decomposition, called the spectral decomposition, eigenvalue decomposition, or eigendecomposition, of the underlying vector space on which the operator acts.\n Augustin-Louis Cauchy proved the spectral theorem for self-adjoint matrices, i.e., that every real, symmetric matrix is diagonalizable. In addition, Cauchy was the first to be systematic about determinants. The spectral theorem as generalized by John von Neumann is today perhaps the most important result of operator theory.\n This article mainly focuses on the simplest kind of spectral theorem, that for a self-adjoint operator on a Hilbert space. However, as noted above, the spectral theorem also holds for normal operators on a Hilbert space.",
  "/wiki/Orthogonal_matrix": "In linear algebra, an orthogonal matrix is a square matrix whose columns and rows are orthogonal unit vectors (orthonormal vectors). One way to express this is",
  "/wiki/Transpose": "In linear algebra, the transpose of a matrix is an operator which flips a matrix over its diagonal, that is it switches the row and column indices of the matrix by producing another matrix denoted as AT (also written A\u2032, Atr, tA or At). It is achieved by any one of the following equivalent actions:",
  "/wiki/Orthogonal_group": "In mathematics, the orthogonal group in dimension n (sometimes called general orthogonal group, by analogy with the general linear group), denoted O(n), is the group of distance-preserving transformations of a Euclidean space of dimension n that preserve a fixed point, where the group operation is given by composing transformations. Equivalently, it is the group of n\u00d7n orthogonal matrices, where the group operation is given by matrix multiplication; an orthogonal matrix is a real matrix whose inverse equals its transpose. The orthogonal group is an algebraic group and a Lie group. It is compact.\n The orthogonal group in dimension n has two connected components. The one that contains the identity element is a subgroup, called the special orthogonal group, and denoted SO(n). It consists of all orthogonal matrices of determinant 1. This group is also called the rotation group, generalizing the fact that in dimensions 2 and 3, its elements are the usual rotations around a point (in dimension 2) or a line (in dimension 3). In low dimension, these groups have been widely studied, see SO(2), SO(3) and SO(4). In the other connected component all orthogonal matrices have \u20131 as a determinant.\n By extension, for any field F, a n\u00d7n matrix with entries in F such that its inverse equals its transpose is called an orthogonal matrix over F. The n\u00d7n orthogonal\nmatrices form a subgroup, denoted O(n, F), of the general linear group GL(n, F); that is",
  "/wiki/Orthonormal_matrix": "In linear algebra, an orthogonal matrix is a square matrix whose columns and rows are orthogonal unit vectors (orthonormal vectors). One way to express this is",
  "/wiki/Orthonormal": "In linear algebra, two vectors in an inner product space are orthonormal if they are orthogonal and unit vectors. A set of vectors form an orthonormal set if all vectors in the set are mutually orthogonal and all of unit length. An orthonormal set which forms a basis is called an orthonormal basis.",
  "/wiki/Singular_matrix": "A singular matrix is a square matrix which is not invertible. Alternatively, a matrix is singular if and only if it has a determinant of 0. When an \n\n\n\nn\n\u00d7\nn\n\n\n{\\displaystyle n\\times n}\n\n matrix is taken to represent a linear transformation in n-dimensional Euclidean space, it is singular if and only if it maps any n-dimensional hypervolume to a n-dimensional hypervolume of zero volume.",
  "/wiki/Unimodular_matrix": "In mathematics, a unimodular matrix M is a square integer matrix having determinant +1 or \u22121. Equivalently, it is an integer matrix that is invertible over the integers: there is an integer matrix N which is its inverse (these are equivalent under Cramer's rule). Thus every equation Mx = b, where M and b are both integer, and M is unimodular, has an integer solution. The unimodular matrices of order n  form a group, which is denoted \n\n\n\nG\n\nL\n\nn\n\n\n(\n\nZ\n\n)\n\n\n{\\displaystyle GL_{n}(\\mathbb {Z} )}\n\n.",
  "/wiki/Integer_matrix": "In mathematics, an integer matrix is a matrix whose entries are all integers. Examples include  binary matrices, the zero matrix, the matrix of ones, the identity matrix, and the adjacency matrices used in graph theory, amongst many others. Integer matrices find frequent application in combinatorics.",
  "/wiki/Unipotent_matrix": "In mathematics, a unipotent element r of a ring R is one such that r \u2212 1 is a nilpotent element; in other words, (r \u2212 1)n is zero for some n.\n In particular, a square matrix, M, is a unipotent matrix, if and only if its characteristic polynomial, P(t), is a power of t \u2212 1. Thus all the eigenvalues of a unipotent matrix are 1.\n The term quasi-unipotent means that some power is unipotent, for example for a diagonalizable matrix with eigenvalues that are all roots of unity.\n In a unipotent affine algebraic group, all elements are unipotent (see below for the definition of an element being unipotent in such a group).",
  "/wiki/Unipotent_group": "In mathematics, a unipotent element r of a ring R is one such that r \u2212 1 is a nilpotent element; in other words, (r \u2212 1)n is zero for some n.\n In particular, a square matrix, M, is a unipotent matrix, if and only if its characteristic polynomial, P(t), is a power of t \u2212 1. Thus all the eigenvalues of a unipotent matrix are 1.\n The term quasi-unipotent means that some power is unipotent, for example for a diagonalizable matrix with eigenvalues that are all roots of unity.\n In a unipotent affine algebraic group, all elements are unipotent (see below for the definition of an element being unipotent in such a group).",
  "/wiki/Unitary_matrix": "In linear algebra, a complex square matrix U is unitary if its conjugate transpose U\u2217 is also its inverse, that is, if",
  "/wiki/Totally_unimodular_matrix": "In mathematics, a unimodular matrix M is a square integer matrix having determinant +1 or \u22121. Equivalently, it is an integer matrix that is invertible over the integers: there is an integer matrix N which is its inverse (these are equivalent under Cramer's rule). Thus every equation Mx = b, where M and b are both integer, and M is unimodular, has an integer solution. The unimodular matrices of order n  form a group, which is denoted \n\n\n\nG\n\nL\n\nn\n\n\n(\n\nZ\n\n)\n\n\n{\\displaystyle GL_{n}(\\mathbb {Z} )}\n\n.",
  "/wiki/Linear_programming": "Linear programming (LP, also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (also known as mathematical optimization).\n More formally, linear programming is a technique for the optimization of a linear objective function, subject to linear equality and linear inequality constraints. Its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, each of which is defined by a linear inequality. Its objective function is a real-valued affine (linear) function defined on this polyhedron. A linear programming algorithm finds a point in the polytope where this function has the smallest (or largest) value if such a point exists.\n Linear programs are problems that can be expressed in canonical form as",
  "/wiki/Linear_programming_relaxation": "In mathematics, the relaxation of a (mixed) integer linear program is the problem that arises by removing the integrality constraint of each variable.\n For example, in a 0-1 integer program, all constraints are of the form",
  "/wiki/Integer_program": "An integer programming problem is a mathematical optimization or feasibility program in which some or all of the variables are restricted to be integers. In many settings the term refers to integer linear programming (ILP), in which the objective function and the constraints (other than the integer constraints) are linear.\n Integer programming is NP-complete. In particular, the special case of 0-1 integer linear programming, in which unknowns are binary, and only the restrictions must be satisfied, is one of Karp's 21 NP-complete problems.\n If some decision variables are not discrete the problem is known as a mixed-integer programming problem.",
  "/wiki/Weighing_matrix": "In mathematics, a weighing matrix W of order n and weight w is an n \u00d7 n (0,1,-1)-matrix such that \n\n\n\nW\n\nW\n\nT\n\n\n=\nw\n\nI\n\nn\n\n\n\n\n{\\displaystyle WW^{T}=wI_{n}}\n\n, where \n\n\n\n\nW\n\nT\n\n\n\n\n{\\displaystyle W^{T}}\n\n is the transpose of \n\n\n\nW\n\n\n{\\displaystyle W}\n\n and \n\n\n\n\nI\n\nn\n\n\n\n\n{\\displaystyle I_{n}}\n\n is the identity matrix of order \n\n\n\nn\n\n\n{\\displaystyle n}\n\n.\n For convenience, a weighing matrix of order n and weight w is often denoted by W(n,w). A W(n,n) is a Hadamard matrix and a W(n,n-1) is equivalent to a conference matrix.",
  "/wiki/List_of_matrices": "This page lists some important classes of matrices used in mathematics, science and engineering. A matrix (plural matrices, or less commonly matrixes) is a rectangular array of numbers called entries. Matrices have a long history of both study and application, leading to diverse ways of classifying matrices. A first group is matrices satisfying concrete conditions of the entries, including constant matrices. An important example is the identity matrix given by"
}