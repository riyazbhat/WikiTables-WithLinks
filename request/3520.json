{
  "/wiki/Matrix_congruence": "In mathematics, two square matrices A and B over a field are called congruent if there exists an invertible matrix P  over the same field such that",
  "/wiki/EP_matrix": "In mathematics, an EP matrix (or range-Hermitian matrix or RPN matrix) is a matrix A whose range is equal to the range of its conjugate transpose A*. Another equivalent characterization of EP matrices is that the range of A is orthogonal to the nullspace of A. Thus, EP matrices are also known as RPN matrices, with RPN meaning Range Perpendicular to Nullspace.",
  "/wiki/Moore%E2%80%93Penrose_inverse": "In mathematics, and in particular linear algebra, the Moore\u2013Penrose inverse \n\n\n\n\nA\n\n+\n\n\n\n\n{\\displaystyle A^{+}}\n\n of a  matrix \n\n\n\nA\n\n\n{\\displaystyle A}\n\n is the most widely known generalization of the inverse matrix. It was independently described by E. H. Moore in 1920, Arne Bjerhammar in 1951, and Roger Penrose in 1955. Earlier, Erik Ivar Fredholm had introduced the concept of a pseudoinverse of integral operators in 1903. When referring to a matrix, the term pseudoinverse, without further specification, is often used to indicate the Moore\u2013Penrose inverse. The term generalized inverse is sometimes used as a synonym for pseudoinverse.",
  "/wiki/Idempotent_matrix": "In linear algebra, an idempotent matrix is a matrix which, when multiplied by itself, yields itself. That is, the matrix \n\n\n\nA\n\n\n{\\displaystyle A}\n\n is idempotent if and only if \n\n\n\n\nA\n\n2\n\n\n=\nA\n\n\n{\\displaystyle A^{2}=A}\n\n. For this product \n\n\n\n\nA\n\n2\n\n\n\n\n{\\displaystyle A^{2}}\n\n to be defined, \n\n\n\nA\n\n\n{\\displaystyle A}\n\n must necessarily be a square matrix. Viewed this way, idempotent matrices are idempotent elements of matrix rings.",
  "/wiki/Projection_(linear_algebra)": "In linear algebra and functional analysis, a projection is a linear transformation \n\n\n\nP\n\n\n{\\displaystyle P}\n\n from a vector space to itself such that \n\n\n\n\nP\n\n2\n\n\n=\nP\n\n\n{\\displaystyle P^{2}=P}\n\n. That is, whenever \n\n\n\nP\n\n\n{\\displaystyle P}\n\n is applied twice to any value, it gives the same result as if it were applied once (idempotent). It leaves its image unchanged. Though abstract, this definition of \"projection\" formalizes and generalizes the idea of graphical projection.  One can also consider the effect of a projection on a geometrical object by examining the effect of the projection on points in the object.",
  "/wiki/Invertible_matrix": "In linear algebra, an n-by-n square matrix A is called invertible (also nonsingular or nondegenerate) if there exists an n-by-n square matrix B such that",
  "/wiki/Inverse_matrix": "In linear algebra, an n-by-n square matrix A is called invertible (also nonsingular or nondegenerate) if there exists an n-by-n square matrix B such that",
  "/wiki/General_linear_group": "In mathematics, the general linear group of degree n is the set of n\u00d7n invertible matrices, together with the operation of ordinary matrix multiplication. This forms a group, because the product of two invertible matrices is again invertible, and the inverse of an invertible matrix is invertible, with identity matrix as the identity element of the group. The group is so named because the columns of an invertible matrix are linearly independent, hence the vectors/points they define are in general linear position, and matrices in the general linear group take points in general linear position to points in general linear position.",
  "/wiki/Involutory_matrix": "In mathematics, an involutory matrix is a matrix that is its own inverse. That is, multiplication by matrix A is an involution if and only if A2\u00a0=\u00a0I. Involutory matrices are all square roots of the identity matrix. This is simply a consequence of the fact that any nonsingular matrix multiplied by its inverse is the identity.",
  "/wiki/Signature_matrix": "In mathematics, a signature matrix is  a diagonal matrix whose diagonal elements are plus or minus 1, that is, any matrix of the form:",
  "/wiki/Householder_transformation": "In linear algebra, a Householder transformation (also known as a Householder reflection or elementary reflector) is a linear transformation that describes a reflection about a plane or hyperplane containing the origin. The Householder transformation was used in a 1958 paper by Alston Scott Householder.",
  "/wiki/Nilpotent_matrix": "In linear algebra, a nilpotent matrix is a square matrix N such that",
  "/wiki/Normal_matrix": "In mathematics, a complex square matrix A is normal if it commutes with its conjugate transpose A*:",
  "/wiki/Conjugate_transpose": "In mathematics, the conjugate transpose or Hermitian transpose of an m-by-n matrix \n\n\n\n\nA\n\n\n\n{\\displaystyle {\\boldsymbol {A}}}\n\n with complex entries is the n-by-m matrix \n\n\n\n\n\nA\n\n\n\nH\n\n\n\n\n\n{\\displaystyle {\\boldsymbol {A}}^{\\mathrm {H} }}\n\n obtained from \n\n\n\n\nA\n\n\n\n{\\displaystyle {\\boldsymbol {A}}}\n\n by taking the transpose and then taking the complex conjugate of each entry. (The complex conjugate of \n\n\n\na\n+\ni\nb\n\n\n{\\displaystyle a+ib}\n\n, where \n\n\n\na\n\n\n{\\displaystyle a}\n\n and \n\n\n\nb\n\n\n{\\displaystyle b}\n\n are real numbers, is \n\n\n\na\n\u2212\ni\nb\n\n\n{\\displaystyle a-ib}\n\n.)",
  "/wiki/Spectral_theorem": "In mathematics, particularly linear algebra and functional analysis, a spectral theorem is a result about when a linear operator or matrix can be diagonalized (that is, represented as a diagonal matrix in some basis). This is extremely useful because computations involving a diagonalizable matrix can often be reduced to much simpler computations involving the corresponding diagonal matrix. The concept of diagonalization is relatively straightforward for operators on finite-dimensional vector spaces but requires some modification for operators on infinite-dimensional spaces. In general, the spectral theorem identifies a class of linear operators that can be modeled by multiplication operators, which are as simple as one can hope to find. In more abstract language, the spectral theorem is a statement about commutative C*-algebras. See also spectral theory for a historical perspective.",
  "/wiki/Orthogonal_matrix": "An orthogonal matrix is a square matrix whose columns and rows are orthogonal unit vectors (i.e., orthonormal vectors), i.e.",
  "/wiki/Transpose": "In linear algebra, the transpose of a matrix is an operator which flips a matrix over its diagonal, that is it switches the row and column indices of the matrix by producing another matrix denoted as AT (also written A\u2032, Atr, tA or At). It is achieved by any one of the following equivalent actions:",
  "/wiki/Orthogonal_group": "In mathematics, the orthogonal group in dimension n (sometimes called general orthogonal group, by analogy with the general linear group), denoted O(n), is the group of distance-preserving transformations of a Euclidean space of dimension n that preserve a fixed point, where the group operation is given by composing transformations. Equivalently, it is the group of n\u00d7n orthogonal matrices, where the group operation is given by matrix multiplication; an orthogonal matrix is a real matrix whose inverse equals its transpose. The orthogonal group is an algebraic group and a Lie group. It is compact.",
  "/wiki/Orthonormal_matrix": "An orthogonal matrix is a square matrix whose columns and rows are orthogonal unit vectors (i.e., orthonormal vectors), i.e.",
  "/wiki/Orthonormal": "In linear algebra, two vectors in an inner product space are orthonormal if they are orthogonal and unit vectors. A set of vectors form an orthonormal set if all vectors in the set are mutually orthogonal and all of unit length. An orthonormal set which forms a basis is called an orthonormal basis.",
  "/wiki/Singular_matrix": "A singular matrix is a square matrix which is not invertible. Alternatively, a matrix is singular if and only if it has a determinant of 0. When an \n\n\n\nn\n\u00d7\nn\n\n\n{\\displaystyle n\\times n}\n\n matrix is taken to represent a linear transformation in n-dimensional Euclidean space, it is singular if and only if it maps any n-dimensional hypervolume to a n-dimensional hypervolume of zero volume.",
  "/wiki/Unimodular_matrix": "In mathematics, a unimodular matrix M is a square integer matrix having determinant +1 or \u22121. Equivalently, it is an integer matrix that is invertible over the integers: there is an integer matrix N which is its inverse (these are equivalent under Cramer's rule). Thus every equation Mx = b, where M and b are both integer, and M is unimodular, has an integer solution. The unimodular matrices of order n  form a group, which is denoted \n\n\n\nG\n\nL\n\nn\n\n\n(\n\nZ\n\n)\n\n\n{\\displaystyle GL_{n}(\\mathbb {Z} )}\n\n.",
  "/wiki/Integer_matrix": "In mathematics, an integer matrix is a matrix whose entries are all integers. Examples include  binary matrices, the zero matrix, the matrix of ones, the identity matrix, and the adjacency matrices used in graph theory, amongst many others. Integer matrices find frequent application in combinatorics.",
  "/wiki/Unipotent_matrix": "In mathematics, a unipotent element r of a ring R is one such that r\u00a0\u2212\u00a01 is a nilpotent element; in other words, (r\u00a0\u2212\u00a01)n is zero for some n.",
  "/wiki/Unipotent_group": "In mathematics, a unipotent element r of a ring R is one such that r\u00a0\u2212\u00a01 is a nilpotent element; in other words, (r\u00a0\u2212\u00a01)n is zero for some n.",
  "/wiki/Unitary_matrix": "In mathematics, a complex square matrix U is unitary if its conjugate transpose U\u2217 is also its inverse\u2014that is, if",
  "/wiki/Totally_unimodular_matrix": "In mathematics, a unimodular matrix M is a square integer matrix having determinant +1 or \u22121. Equivalently, it is an integer matrix that is invertible over the integers: there is an integer matrix N which is its inverse (these are equivalent under Cramer's rule). Thus every equation Mx = b, where M and b are both integer, and M is unimodular, has an integer solution. The unimodular matrices of order n  form a group, which is denoted \n\n\n\nG\n\nL\n\nn\n\n\n(\n\nZ\n\n)\n\n\n{\\displaystyle GL_{n}(\\mathbb {Z} )}\n\n.",
  "/wiki/Linear_programming": "Linear programming (LP, also called linear optimization) is a method to achieve the best outcome (such as maximum profit or lowest cost) in a mathematical model whose requirements are represented by linear relationships. Linear programming is a special case of mathematical programming (also known as mathematical optimization).",
  "/wiki/Linear_programming_relaxation": "In mathematics, the relaxation of a (mixed) integer linear program is the problem that arises by removing the integrality constraint of each variable.",
  "/wiki/Integer_program": "An integer programming problem is a mathematical optimization or feasibility program in which some or all of the variables are restricted to be integers. In many settings the term refers to integer linear programming (ILP), in which the objective function and the constraints (other than the integer constraints) are linear.",
  "/wiki/Weighing_matrix": "In mathematics, a weighing matrix W of order n and weight w is an n \u00d7 n (0,1,-1)-matrix such that \n\n\n\nW\n\nW\n\nT\n\n\n=\nw\n\nI\n\nn\n\n\n\n\n{\\displaystyle WW^{T}=wI_{n}}\n\n, where \n\n\n\n\nW\n\nT\n\n\n\n\n{\\displaystyle W^{T}}\n\n is the transpose of \n\n\n\nW\n\n\n{\\displaystyle W}\n\n and \n\n\n\n\nI\n\nn\n\n\n\n\n{\\displaystyle I_{n}}\n\n is the identity matrix of order \n\n\n\nn\n\n\n{\\displaystyle n}\n\n.",
  "/wiki/List_of_matrices": "This page lists some important classes of matrices used in mathematics, science and engineering. A matrix (plural matrices, or less commonly matrixes) is a rectangular array of numbers called entries. Matrices have a long history of both study and application, leading to diverse ways of classifying matrices. A first group is matrices satisfying concrete conditions of the entries, including constant matrices. An important example is the identity matrix given by"
}