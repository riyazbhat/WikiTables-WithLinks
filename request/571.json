{
  "/wiki/Atom_probe": "The atom probe was introduced at the 14th Field Emission Symposium in 1967 by Erwin Wilhelm M\u00fcller and J. A. Panitz. It combined a field ion microscope with a mass spectrometer having a single particle detection capability and, for the first time, an instrument could \u201c... determine the nature of one single atom seen on a metal surface and selected from neighboring atoms at the discretion of the observer\u201d.\n Atom probes are unlike conventional optical or electron microscopes, in that the magnification effect comes from the magnification provided by a highly curved electric field, rather than by the manipulation of radiation paths. The method is destructive in nature removing ions from a sample surface in order to image and identify them, generating magnifications sufficient to observe individual atoms as they are removed from the sample surface. Through coupling of this magnification method with time of flight mass spectrometry, ions evaporated by application of electric pulses can have their mass-to-charge ratio computed.\n Through successive evaporation of material, layers of atoms are removed from a specimen, allowing for probing not only of the surface, but also through the material itself. Computer methods are used to rebuild a three-dimensional view of the sample, prior to it being evaporated, providing atomic scale information on the structure of a sample, as well as providing the type atomic species information. The instrument allows the three-dimensional reconstruction of up to billions of atoms from a sharp tip (corresponding to specimen volumes of 10,000-10,000,000 nm3).",
  "/wiki/Visible_spectrum": "The visible spectrum is the portion of the electromagnetic spectrum that is visible to the human eye. Electromagnetic radiation in this range of wavelengths is called visible light or simply light. A typical human eye will respond to wavelengths from about 380 to 740 nanometers. In terms of frequency, this corresponds to a band in the vicinity of 430\u2013770 THz.\n The spectrum does not contain all the colors that the human eyes and brain can distinguish. Unsaturated colors such as pink, or purple variations like magenta, for example, are absent because they can only be made from a mix of multiple wavelengths. Colors containing only one wavelength are also called pure colors or spectral colors.\n Visible wavelengths pass largely unattenuated through the Earth's atmosphere via the \"optical window\" region of the electromagnetic spectrum. An example of this phenomenon is when clean air scatters blue light more than red light, and so the midday sky appears blue (apart from the area around the sun which appears white because the light is not scattered as much). The optical window is also referred to as the \"visible window\" because it overlaps the human visible response spectrum. The near infrared (NIR) window lies just out of the human vision, as well as the medium wavelength infrared (MWIR) window, and the long wavelength or far infrared (LWIR or FIR) window, although other animals may experience them.",
  "/wiki/Spectral_imaging": "Spectral imaging is imaging that uses multiple bands across the electromagnetic spectrum.  While an ordinary camera captures light across three wavelength bands in the visible spectrum, red, green, and blue (RGB), spectral imaging encompasses a wide variety of techniques that go beyond RGB.  Spectral imaging may use the infrared, the visible spectrum, the ultraviolet, x-rays, or some combination of the above.  It may include the acquisition of image data in visible and non-visible bands simultaneously, illumination from outside the visible range, or the use of optical filters to capture a specific spectral range.  It is also possible to capture hundreds of wavelength bands for each pixel in an image. \n Multispectral imaging captures a small number of spectral bands, typically three to fifteen, through the use of varying filters and illumination.  Many off-the-shelf RGB cameras will detect a small amount of Near-Infrared (NIR) light.  A scene may be illuminated with NIR light, and, simultaneously, an infrared-passing filter may be used on the camera to ensure that visible light is blocked and only NIR is captured in the image.  Industrial, military, and scientific work, however, uses sensors built for the purpose.\n Hyperspectral imaging is another subcategory of spectral imaging, which combines spectroscopy and digital photography. In hyperspectral imaging, a complete spectrum or some spectral information (such as the Doppler shift or Zeeman splitting of a spectral line) is collected at every pixel in an image plane. A hyperspectral camera uses special hardware to capture hundreds of wavelength bands for each pixel, which can be interpreted as a complete spectrum. In other words, the camera has a high spectral resolution. The phrase \"spectral imaging\" is sometimes used as a shorthand way of referring to this technique, but it is preferable to use the term \"hyperspectral imaging\" in places when ambiguity may arise. Hyperspectral images are often represented as an image cube, which is type of data cube.\n Applications of spectral imaging include art conservation, astronomy, solar physics, planetology, and Earth remote sensing.",
  "/wiki/Chemiluminescence": "Chemiluminescence (also chemoluminescence) is the emission of light (luminescence), as the result of a chemical reaction. There may also be limited emission of heat. Given reactants A and B, with an excited intermediate \u25ca,",
  "/wiki/Flame": "A flame (from Latin flamma) is the visible, gaseous part of a fire. It is caused by a highly exothermic reaction taking place in a thin zone. Very hot flames are hot enough to have ionized gaseous components of sufficient density to be considered plasma.[vague][citation needed]",
  "/wiki/Laser_scanning_confocal_microscopy": "Confocal microscopy, most frequently confocal laser scanning microscopy (CLSM) or laser confocal scanning microscopy (LCSM), is an optical imaging technique for increasing optical resolution and contrast of a micrograph by means of using a spatial pinhole to block out-of-focus light in image formation. Capturing multiple two-dimensional images at different depths in a sample enables the reconstruction of three-dimensional structures (a process known as optical sectioning) within an object. This technique is used extensively in the scientific and industrial communities and typical applications are in life sciences, semiconductor inspection and materials science.\n Light travels through the sample under a conventional microscope as far into the specimen as it can penetrate, while a confocal microscope only focuses a smaller beam of light at one narrow depth level at a time. The CLSM achieves a controlled and highly limited depth of focus.",
  "/wiki/Cryogenic_electron_tomography": "Electron cryotomography (CryoET) is an imaging technique used to produce high-resolution (~1-4 nm) three-dimensional views of samples, typically biological macromolecules and cells. CryoET is a specialized application of transmission electron cryomicroscopy (CryoTEM) in which samples are imaged as they are tilted, resulting in a series of 2D images that can be combined to produce a 3D reconstruction, similar to a CT scan of the human body. In contrast to other electron tomography techniques, samples are immobilized in non-crystalline (\"vitreous\") ice and imaged under cryogenic conditions (< \u2212150 \u00b0C), allowing them to be imaged without dehydration or chemical fixation, which could otherwise disrupt or distort biological structures.",
  "/wiki/Cryogenic_transmission_electron_microscopy": "Transmission electron cryomicroscopy (CryoTEM), commonly known as cryo-EM, is a form of cryogenic electron microscopy, more specifically a type of transmission electron microscopy (TEM) where the sample is studied at cryogenic temperatures (generally liquid-nitrogen temperatures). Cryo-EM is gaining popularity in structural biology.\n The utility of transmission electron cryomicroscopy stems from the fact that it allows the observation of specimens that have not been stained or fixed in any way, showing them in their native environment. This is in contrast to X-ray crystallography, which requires crystallizing the specimen, which can be difficult, and placing them in non-physiological environments, which can occasionally lead to functionally irrelevant conformational changes.\n Recent advances in detector technology and software algorithms have allowed for the determination of macromolecular structures at near-atomic resolution by cryo-EM. These include viruses, ribosomes, mitochondria, ion channels, and enzyme complexes. As of 2018, cryo-EM can be applied to structures as small as hemoglobin (64 kDa) and with resolutions up to 1.8 \u00c5. Cryo-EM structures currently represent just over 2.5% of structures deposited in the Protein Data Bank, although this number is rapidly increasing as more and more cryo-EM structures are published each year. An application of cryo-EM is cryo-electron tomography (cryo-ET), where a 3D reconstruction of the sample is created from tilted 2D images.",
  "/wiki/Electrical_capacitance_tomography": "Electrical capacitance tomography (ECT) is a method for determination of the dielectric permittivity distribution in the interior of an object from external capacitance measurements. It is a close relative of electrical impedance tomography and is proposed as a method for industrial process monitoring, although it has yet to see widespread use. Potential applications include the measurement of flow of fluids in pipes and measurement of the concentration of one fluid in another, or the distribution of a solid in a fluid. \n Although capacitance sensing methods were in widespread use the idea of using capacitance measurement to form images is attributed to Maurice Beck and co-workers at UMIST in the 1980s.\n Although usually called tomography, the technique differs from conventional tomographic methods, in which high resolution images are formed of slices of a material. The measurement electrodes, which are metallic plates, must be sufficiently large to give a measureable change in capacitance. This means that very few electrodes are used and eight or twelve electrodes is common. An N-electrode system can only provide N(N\u22121)/2 independent measurements. This means that the technique is limited to producing very low resolution images of approximate slices. However, ECT is fast, and relatively inexpensive.",
  "/wiki/Electrical_capacitance": "Capacitance is the ratio of the change in electric charge of a system, to the corresponding change in its electric potential. There are two closely related notions of capacitance: self capacitance and mutual capacitance. Any object that can be electrically charged exhibits self capacitance. A material with a large self capacitance holds more electric charge at a given voltage than one with low capacitance. The notion of mutual capacitance is particularly important for understanding the operations of the capacitor, one of the three elementary linear electronic components (along with resistors and inductors).\n The capacitance is a function only of the geometry of the design (e.g. area of the plates and the distance between them) and the permittivity of the dielectric material between the plates of the capacitor. For many dielectric materials, the permittivity and thus the capacitance, is independent of the potential difference between the conductors and the total charge on them.\n The SI unit of capacitance is the farad (symbol: F), named after the English physicist Michael Faraday. A 1 farad capacitor, when charged with 1 coulomb of electrical charge, has a potential difference of 1 volt between its plates. The reciprocal of capacitance is called elastance.",
  "/wiki/Electrical_capacitance_volume_tomography": "Electrical capacitance volume tomography (ECVT) is a non-invasive 3D imaging technology applied primarily to multiphase flows. It was first introduced by W. Warsito, Q. Marashdeh, and L.-S. Fan as an extension of the conventional electrical capacitance tomography (ECT). In conventional ECT, sensor plates are distributed around a surface of interest. Measured capacitance between plate combinations is used to reconstruct 2D images (tomograms) of material distribution. In ECT, the fringing field from the edges of the plates is viewed as a source of distortion to the final reconstructed image and is thus mitigated by guard electrodes. ECVT exploits this fringing field and expands it through 3D sensor designs that deliberately establish an electric field variation in all three dimensions. The image reconstruction algorithms are similar in nature to ECT; nevertheless, the reconstruction problem in ECVT is more complicated. The sensitivity matrix of an ECVT sensor is more ill-conditioned and the overall reconstruction problem is more ill-posed compared to ECT. The ECVT approach to sensor design allows direct 3D imaging of the outrounded geometry. This is different than 3D-ECT that relies on stacking images from individual ECT sensors. 3D-ECT can also be accomplished by stacking frames from a sequence of time intervals of ECT measurements. Because the ECT sensor plates are required to have lengths on the order of the domain cross-section, 3D-ECT does not provide the required resolution in the axial dimension. ECVT solves this problem by going directly to the image reconstruction and avoiding the stacking approach. This is accomplished by using a sensor that is inherently three-dimensional.",
  "/wiki/Electrical_resistivity_tomography": "Electrical resistivity tomography (ERT) or electrical resistivity imaging (ERI) is a geophysical technique for imaging sub-surface structures from electrical resistivity measurements made at the surface, or by electrodes in one or more boreholes. If the electrodes are suspended in the boreholes, deeper sections can be investigated. It is closely related to the medical imaging technique electrical impedance tomography (EIT), and mathematically is the same inverse problem. In contrast to medical EIT, however, ERT is essentially a direct current method. A related geophysical method, induced polarization (or spectral induced polarization), measures the transient response and aims to determine the subsurface chargeability properties.",
  "/wiki/Electrical_resistivity": "Electrical resistivity (also called specific electrical resistance or volume resistivity) and its inverse, electrical conductivity, is a fundamental property of  a material that quantifies how strongly it resists or conducts electric current. A low resistivity indicates a material that readily allows electric current. Resistivity is commonly represented by the Greek letter \u03c1 (rho). The SI unit of electrical resistivity is the ohm-meter (\u03a9\u22c5m).  For example, if a 1 m \u00d7 1 m \u00d7 1 m solid cube of material has sheet contacts on two opposite faces, and the resistance between these contacts is 1 \u03a9, then the resistivity of the material is 1 \u03a9\u22c5m.\n Electrical conductivity or specific conductance is the reciprocal of electrical resistivity. It represents a material's ability to conduct  electric current. It is commonly signified by the Greek letter \u03c3 (sigma), but \u03ba (kappa) (especially in electrical engineering) and \u03b3 (gamma) are sometimes used. The SI unit of electrical conductivity is siemens per metre (S/m).",
  "/wiki/Electrical_impedance_tomography": "Electrical impedance tomography (EIT) is a noninvasive type of medical imaging in which the electrical conductivity, permittivity, and impedance of a part of the body is inferred from surface electrode measurements and used to form a tomographic image of that part. Electrical conductivity varies considerably among various biological tissues (absolute EIT) or the movement of fluids and gases within tissues (difference EIT). The majority of EIT systems apply small alternating currents at a single frequency, however, some EIT systems use multiple frequencies to better differentiate between normal and suspected abnormal tissue within the same organ (multifrequency-EIT or electrical impedance spectroscopy).\n Typically, conducting surface electrodes are attached to the skin around the body part being examined. Small alternating currents will be applied to some or all of the electrodes, the resulting equi-potentials being recorded from the other electrodes (figures 1 and 2). This process will then be repeated for numerous different electrode configurations and finally result in a two-dimensional tomogram according to the image reconstruction algorithms incorporated.\n Since free ion content determines tissue and fluid conductivity, muscle and blood will conduct the applied currents better than fat, bone or lung tissue. This property can be used to reconstruct static images by morphological or absolute EIT (a-EIT). However, in contrast to linear x-rays used in Computed Tomography, electric currents travel three dimensionally along the path of least resistivity. This means, that a part of the electric current leaves the transverse plane and results in an impedance transfer. This and other factors are the reason why image reconstruction in absolute EIT is so hard, since there is usually more than just one solution for image reconstruction of a three-dimensional area projected onto a two-dimensional plane.\n Mathematically, the problem of recovering conductivity from surface measurements of current and potential is a non-linear inverse problem and is severely ill-posed. The mathematical formulation of the problem is due to Alberto Calder\u00f3n, and in the mathematical literature of inverse problems it is often referred to as \"Calder\u00f3n's inverse problem\" or the \"Calder\u00f3n problem\". There is extensive mathematical research on the problem of uniqueness of solution and numerical algorithms for this problem.\n Compared to the tissue conductivities of most other soft tissues within the human thorax, lung tissue conductivity is approximately five-fold lower, resulting in high absolute contrast. This characteristic may partially explain the amount of research conducted in EIT lung imaging. Furthermore, lung conductivity fluctuates intensely during the breath cycle which accounts for the immense interest of the research community to use EIT as a bedside method to visualize inhomogeneity of lung ventilation in mechanically ventilated patients. EIT measurements between two or more physiological states, e.g. between inspiration and expiration, are therefore referred to as time difference EIT (td-EIT).\n Time difference EIT (td-EIT) has one major advantage over absolute EIT (a-EIT): inaccuracies resulting from interindividual anatomy, insufficient skin contact of surface electrodes or impedance transfer can be dismissed because most artifacts will eliminate themselves due to simple image subtraction in f-EIT. This is most likely the reason why, as of today, the greatest progress of EIT research has been achieved with difference EIT.\n Further EIT applications proposed include detection/location of cancer in skin, breast, or cervix, localization of epileptic foci, imaging of brain activity. as well as a diagnostic tool for impaired gastric emptying. Attempts to detect or localize tissue pathology within normal tissue usually rely on multifrequency EIT (MF-EIT), also termed Electrical Impedance Spectroscopy (EIS) and are based on differences in conductance patterns at varying frequencies.\n The invention of EIT as a medical imaging technique is usually attributed to John G. Webster and a publication in 1978, although the first practical realization of a medical EIT system was detailed in 1984 due to the work of David C. Barber and Brian H. Brown. Together, Brown and Barber published the first Electrical Impedance Tomogram in 1983, visualizing the cross section of a human forearm by absolute EIT. Even though there has been substantial progress in the meantime, most a-EIT applications are still considered experimental. However, two commercial f-EIT devices for monitoring lung function in intensive care patients have been introduced just recently.\n A technique similar to EIT is used in geophysics and industrial process monitoring \u2013 electrical resistivity tomography. In analogy to EIT, surface electrodes are being placed on the earth, within bore holes, or within a vessel or pipe in order to locate resistivity anomalies or monitor mixtures of conductive fluids. Setup and reconstruction techniques are comparable to EIT. In geophysics, the idea dates from the 1930s.\n Electrical resistivity tomography has also been proposed for mapping the electrical properties of substrates and thin films for electronic applications.",
  "/wiki/Electrical_impedance": "Electrical impedance is the measure of the opposition that a circuit presents to a current when a voltage is applied. The term complex impedance may be used interchangeably.\n Quantitatively, the impedance of a two-terminal circuit element is the ratio of the complex representation of a sinusoidal voltage between its terminals to the complex representation of the current flowing through it. In general, it depends upon the frequency of the sinusoidal voltage. \n Impedance extends the concept of resistance to AC circuits, and possesses both magnitude and phase, unlike resistance, which has only magnitude. When a circuit is driven with direct current (DC), there is no distinction between impedance and resistance; the latter can be thought of as impedance with zero phase angle.\n The notion of impedance is useful for performing AC analysis of electrical networks, because it allows relating sinusoidal voltages and currents by a simple linear law.  \nIn multiple port networks, the two-terminal definition of impedance is inadequate, but the complex voltages at the ports and the currents flowing through them are still linearly related by the impedance matrix. \n Impedance is a complex number, with the same units as resistance, for which the SI unit is the ohm (\u03a9).\nIts symbol is usually Z, and it may be represented by writing its magnitude and phase in the form |Z|\u2220\u03b8. However, cartesian complex number representation is often more powerful for circuit analysis purposes. \n The reciprocal of impedance is admittance, whose SI unit is the siemens, formerly called mho.\n Instruments used to measure the electrical impedance are called impedance analyzers.",
  "/wiki/Electron_tomography": "Electron tomography (ET) is a tomography technique for obtaining detailed 3D structures of sub-cellular macro-molecular objects. Electron tomography is an extension of traditional transmission electron microscopy and uses a transmission electron microscope to collect the data. In the process, a beam of electrons is passed through the sample at incremental degrees of rotation around the center of the target sample. This information is collected and used to assemble a three-dimensional image of the target. For biological applications, the typical resolution of ET systems are in the 5\u201320 nm range, suitable for examining supra-molecular multi-protein structures, although not the secondary and tertiary structure of an individual protein or polypeptide.",
  "/wiki/Transmission_electron_microscopy": "Transmission electron microscopy (TEM, an abbreviation which can also stand for the instrument, a transmission electron microscope) is a microscopy technique in which a beam of electrons is transmitted through a specimen to form an image. The specimen is most often an ultrathin section less than 100 nm thick or a suspension on a grid. An image is formed from the interaction of the electrons with the sample as the beam is transmitted through the specimen. The image is then magnified and focused onto an imaging device, such as a fluorescent screen, a layer of photographic film, or a sensor such as a scintillator attached to a charge-coupled device.\n Transmission electron microscopes are capable of imaging at a significantly higher resolution than light microscopes, owing to the smaller de Broglie wavelength of electrons. This enables the instrument to capture fine detail\u2014even as small as a single column of atoms, which is thousands of times smaller than a resolvable object seen in a light microscope. Transmission electron microscopy is a major analytical method in the physical, chemical and biological sciences. TEMs find application in cancer research, virology, and materials science as well as pollution, nanotechnology and semiconductor research, but also in other fields such as paleontology and palynology.\n TEM instruments boast an enormous array of operating modes including conventional imaging, scanning TEM imaging (STEM), diffraction, spectroscopy, and combinations of these. Even within conventional imaging, there are many fundamentally different ways that contrast is produced, called \"image contrast mechanisms.\" Contrast can arise from position-to-position differences in the thickness or density (\"mass-thickness contrast\"), atomic number (\"Z contrast,\" referring to the common abbreviation Z for atomic number), crystal structure or orientation (\"crystallographic contrast\" or \"diffraction contrast\"), the slight quantum-mechanical phase shifts that individual atoms produce in electrons that pass through them (\"phase contrast\"), the energy lost by electrons on passing through the sample (\"spectrum imaging\") and more. Each mechanism tells the user a different kind of information, depending not only on the contrast mechanism but on how the microscope is used\u2014the settings of lenses, apertures, and detectors. What this means is that a TEM is capable of returning an extraordinary variety of nanometer- and atomic-resolution information, in ideal cases revealing not only where all the atoms are but what kinds of atoms they are and how they are bonded to each other. For this reason TEM is regarded as an essential tool for nanoscience in both biological and materials fields.\n The first TEM was demonstrated by Max Knoll and Ernst Ruska in 1931, with this group developing the first TEM with resolution greater than that of light in 1933 and the first commercial TEM in 1939. In 1986, Ruska was awarded the Nobel Prize in physics for the development of transmission electron microscopy.",
  "/wiki/Focal_plane_tomography": "In radiography, focal plane tomography is tomography (imaging a single plane, or slice, of an object) by simultaneously moving the X-ray generator and X-ray detector so as to keep a consistent exposure of only the plane of interest during image acquisition. This was the main method of obtaining tomographs in medical imaging until the late-1970s. It has since been largely replaced by more advanced imaging techniques such as CT and MRI. It remains in use today in a few specialized applications, such as for acquiring orthopantomographs of the jaw in dental radiography.\n Focal plane tomography\u2019s development began in the 1930s by the Italian radiologist Alessandro Vallebona as a means of reducing the problem of superimposition of structures which is inherent to projectional radiography.",
  "/wiki/X-ray": "X-rays make up X-radiation, a form of high-energy electromagnetic radiation. Most X-rays have a wavelength ranging from 0.03 to 3 nanometres, corresponding to frequencies in the range 30 petahertz to 30 exahertz (3\u00d71016 Hz to 3\u00d71019 Hz) and energies in the range 100 eV to 200 keV. X-ray wavelengths are shorter than those of UV rays and typically longer than those of gamma rays. In many languages, X-radiation is referred to as R\u00f6ntgen radiation, after the German scientist Wilhelm R\u00f6ntgen, who discovered it on November 8, 1895. He named it X-radiation to signify an unknown type of radiation. Spellings of X-ray(s) in English include the variants x-ray(s), xray(s), and X ray(s).",
  "/wiki/Functional_magnetic_resonance_imaging": "Functional magnetic resonance imaging or functional MRI (fMRI) measures brain activity by detecting changes associated with blood flow. This technique relies on the fact that cerebral blood flow and neuronal activation are coupled. When an area of the brain is in use, blood flow to that region also increases.\n The primary form of fMRI uses the blood-oxygen-level dependent (BOLD) contrast, discovered by Seiji Ogawa in 1990. This is a type of specialized brain and body scan used to map neural activity in the brain or spinal cord of humans or other animals by imaging the change in blood flow (hemodynamic response) related to energy use by brain cells. Since the early 1990s, fMRI has come to dominate brain mapping research because it does not require people to undergo injections or surgery, to ingest substances, or to be exposed to ionizing radiation.  This measure is frequently corrupted by noise from various sources; hence, statistical procedures are used to extract the underlying signal.  The resulting brain activation can be graphically represented by color-coding the strength of activation across the brain or the specific region studied.  The technique can localize activity to within millimeters but, using standard techniques, no better than within a window of a few seconds. Other methods of obtaining contrast are arterial spin labeling and diffusion MRI. The latter procedure is similar to BOLD fMRI but provides contrast based on the magnitude of diffusion of water molecules in the brain.\n In addition to detecting BOLD responses from activity due to tasks/stimuli, fMRI can measure resting state fMRI, or taskless fMRI, which shows the subjects' baseline BOLD variance.  Since about 1998 studies have shown the existence and properties of the default mode network (DMN), aka 'Resting State Network' (RSN), a functionally connected neural network of apparent 'brain states'.\n fMRI is used in research, and to a lesser extent, in clinical work.  It can complement other measures of brain physiology such as EEG and NIRS.  Newer methods which improve both spatial and time resolution are being researched, and these largely use biomarkers other than the BOLD signal.  Some companies have developed commercial products such as lie detectors based on fMRI techniques, but the research is not believed to be developed enough for widespread commercialization.",
  "/wiki/Nuclear_magnetic_resonance": "Nuclear magnetic resonance (NMR) is a method of physical observation in which nuclei in a strong constant magnetic field are perturbed by a weak oscillating magnetic field (in the near field and therefore not involving electromagnetic waves) and respond by producing an electromagnetic signal with a frequency characteristic of the magnetic field at the nucleus. This process occurs near resonance, when the oscillation frequency matches the intrinsic frequency of the nuclei, which depends on the strength of the static magnetic field, the chemical environment, and the magnetic properties of the isotope involved; in practical applications with static magnetic fields up to ca. 20  tesla, the frequency is similar to VHF and UHF television broadcasts (60\u20131000 MHz). NMR results from specific magnetic properties of certain atomic nuclei. Nuclear magnetic resonance spectroscopy is widely used to determine the structure of organic molecules in solution and study molecular physics, crystals as well as non-crystalline materials. NMR is also routinely used in advanced medical imaging techniques, such as in magnetic resonance imaging (MRI).\n All isotopes that contain an odd number of protons and/or neutrons (see Isotope) have an intrinsic nuclear magnetic moment and angular momentum, in other words a nonzero nuclear spin, while all nuclides with even numbers of both have a total spin of zero. The most commonly used nuclei are 1H and 13C, although isotopes of many other elements can be studied by high-field NMR spectroscopy as well.\n A key feature of NMR is that the resonance frequency of a particular simple substance is usually directly proportional to the strength of the applied magnetic field. It is this feature that is exploited in imaging techniques; if a sample is placed in a non-uniform magnetic field then the resonance frequencies of the sample's nuclei depend on where in the field they are located. Since the resolution of the imaging technique depends on the magnitude of the magnetic field gradient, many efforts are made to develop increased gradient field strength. \n The principle of NMR usually involves three sequential steps:",
  "/wiki/Hydraulic_tomography": "Hydraulic tomography (HT) is a sequential cross-hole hydraulic test followed by inversion of all the data to map the spatial distribution of aquifer hydraulic properties. Specifically, HT involves installation of multiple wells in an aquifer, which are partitioned into several intervals along the depth using packers. A sequential aquifer test at selected intervals is then conducted. During the test, water is injected or withdrawn (i.e. a pressure excitation) at a selected interval in a given well. Pressure responses of the subsurface are then monitored at other intervals at this well and also in other wells. This test produces a set of pressure excitation/response data of the subsurface. \n Once a given test has been completed, the pump is moved to another interval and the test is repeated to collect another set of data. The same procedure is then applied to the intervals at other wells. Afterward, the data sets from all tests are processed by a mathematical model to estimate the spatial distribution of hydraulic properties of the aquifer. These pairs of pumping and drawdown data sets at different locations make an inverse problem better posed, because each pair cross-validates the others such that the estimates become less non-unique. In other words, predictions of ground water flow based on the HT estimates will be more accurate and less uncertain than those based on estimates from traditional site-characterization approaches and model calibrations.",
  "/wiki/Fluid_flow": "In physics and engineering, fluid dynamics  is a subdiscipline of fluid mechanics that describes the flow of fluids\u2014liquids and gases.  It has several subdisciplines, including aerodynamics (the study of air and other gases in motion) and hydrodynamics (the study of liquids in motion).  Fluid dynamics has a wide range of applications, including calculating forces and moments on aircraft, determining the mass flow rate of petroleum through pipelines, predicting weather patterns, understanding nebulae in interstellar space and modelling fission weapon detonation.\n Fluid dynamics offers a systematic structure\u2014which underlies these practical disciplines\u2014that embraces empirical and semi-empirical laws derived from flow measurement and used to solve practical problems. The solution to a fluid dynamics problem typically involves the calculation of various properties of the fluid, such as flow velocity, pressure, density, and temperature, as functions of space and time.\n Before the twentieth century, hydrodynamics was synonymous with fluid dynamics.  This is still reflected in names of some fluid dynamics topics, like magnetohydrodynamics and hydrodynamic stability, both of which can also be applied to gases.",
  "/wiki/Mid-infrared": "Infrared radiation (IR), sometimes called infrared light, is electromagnetic radiation (EMR) with wavelengths longer than those of visible light. It is therefore generally invisible to the human eye, although IR at wavelengths up to 1050 nanometers (nm)s from specially pulsed lasers can be seen by humans under certain conditions. IR wavelengths extend from the nominal red edge of the visible spectrum at 700 nanometers (frequency 430 THz), to 1 millimeter (300 GHz). Most of the thermal radiation emitted by objects near room temperature is infrared. As with all EMR, IR carries radiant energy and behaves both like a wave and like its quantum particle, the photon.\n Infrared radiation was discovered in 1800 by astronomer Sir William Herschel, who discovered a type of invisible radiation in the spectrum lower in energy than red light, by means of its effect on a thermometer. Slightly more than half of the total energy from the Sun was eventually found to arrive on Earth in the form of infrared. The balance between absorbed and emitted infrared radiation has a critical effect on Earth's climate.\n Infrared radiation is emitted or absorbed by molecules when they change their rotational-vibrational movements. It excites vibrational modes in a molecule through a change in the dipole moment, making it a useful frequency range for study of these energy states for molecules of the proper symmetry. Infrared spectroscopy examines absorption and transmission of photons in the infrared range.\n Infrared radiation is used in industrial, scientific, military, \nlaw enforcement, and medical applications. Night-vision devices using active near-infrared illumination allow people or animals to be observed without the observer being detected. Infrared astronomy uses sensor-equipped telescopes to penetrate dusty regions of space such as molecular clouds, detect objects such as planets, and to view highly red-shifted objects from the early days of the universe. Infrared thermal-imaging cameras are used to detect heat loss in insulated systems, to observe changing blood flow in the skin, and to detect overheating of electrical apparatus.\n Extensive uses for military and civilian applications include target acquisition, surveillance, night vision, homing, and tracking. Humans at normal body temperature radiate chiefly at wavelengths around 10 \u03bcm (micrometers). Non-military uses include thermal efficiency analysis, environmental monitoring, industrial facility inspections, detection of grow-ops, remote temperature sensing, short-range wireless communication, spectroscopy, and weather forecasting.",
  "/wiki/Laser_ablation": "Laser ablation or photoablation is the process of removing material from a solid (or occasionally liquid) surface by irradiating it with a laser beam.  At low laser flux, the material is heated by the absorbed laser energy and evaporates or sublimates.  At high laser flux, the material is typically converted to a plasma.  Usually, laser ablation refers to removing material with a pulsed laser, but it is possible to ablate material with a continuous wave laser beam if the laser intensity is high enough. Excimer lasers of deep ultra-violet light are mainly used in photoablation; the wavelength of laser used in photoablation is approximately 200 nm.",
  "/wiki/Fluorescence_microscopy": "A fluorescence microscope is an optical microscope that uses fluorescence and phosphorescence instead of, or in addition to, scattering, reflection, and attenuation or absorption, to study the properties of organic or inorganic substances.  \"Fluorescence microscope\" refers to any microscope that uses fluorescence to generate an image, whether it is a more simple set up like an epifluorescence microscope or a more complicated design such as a confocal microscope, which uses optical sectioning to get better resolution of the fluorescence image.",
  "/wiki/Magnetic_induction_tomography": "Magnetic induction tomography is an imaging technique used to image electromagnetic properties of an object by using the eddy current effect. It is also called electromagnetic induction tomography, electromagnetic tomography (EMT), eddy current tomography, and eddy current testing.",
  "/wiki/Electromagnetic_induction": "Electromagnetic or magnetic induction is the production of an electromotive force (i.e., voltage) across an electrical conductor in a changing magnetic field.\n Michael Faraday is generally credited with the discovery of induction in 1831, and James Clerk Maxwell mathematically described it as Faraday's law of induction. Lenz's law describes the direction of the induced field. Faraday's law was later generalized to become the Maxwell\u2013Faraday equation, one of the four Maxwell equations in his theory of electromagnetism.\n Electromagnetic induction has found many applications, including electrical components such as inductors and transformers, and devices such as electric motors and generators.",
  "/wiki/Magnetic_resonance_imaging": "Magnetic resonance imaging (MRI) is a medical imaging technique used in radiology to form pictures of the anatomy and the physiological processes of the body. MRI scanners use strong magnetic fields, magnetic field gradients, and radio waves to generate images of the organs in the body. MRI does not involve X-rays or the use of ionizing radiation, which distinguishes it from CT and PET scans. MRI is a medical application of nuclear magnetic resonance (NMR). NMR can also be used for imaging in other NMR applications, such as NMR spectroscopy.\n While the hazards of ionizing radiation are now well controlled in most medical contexts, an MRI may still be seen as a better choice than a CT scan. MRI is widely used in hospitals and clinics for medical diagnosis and staging and follow-up of disease without exposing the body to radiation. An MRI may yield different information compared with CT. Risks and discomfort may be associated with MRI scans. Compared with CT scans, MRI scans typically take longer and are louder, and they usually need the subject to enter a narrow, confining tube. In addition, people with some medical implants or other non-removable metal inside the body may be unable to undergo an MRI examination safely.\n MRI was originally called NMRI (nuclear magnetic resonance imaging), but \"nuclear\" was dropped to avoid negative associations. Certain atomic nuclei are able to absorb and emit radio frequency energy when placed in an external magnetic field. In clinical and research MRI, hydrogen atoms are most often used to generate a detectable radio-frequency signal that is received by antennas close to the subject being examined. Hydrogen atoms are naturally abundant in humans and other biological organisms, particularly in water and fat. For this reason, most MRI scans essentially map the location of water and fat in the body. Pulses of radio waves excite the nuclear spin energy transition, and magnetic field gradients localize the signal in space. By varying the parameters of the pulse sequence, different contrasts may be generated between tissues based on the relaxation properties of the hydrogen atoms therein.\n Since its development in the 1970s and 1980s, MRI has proven to be a versatile imaging technique. While MRI is most prominently used in diagnostic medicine and biomedical research, it also may be used to form images of non-living objects. MRI scans are capable of producing a variety of chemical and physical data, in addition to detailed spatial images. The sustained increase in demand for MRI within health systems has led to concerns about cost effectiveness and overdiagnosis.",
  "/wiki/Nuclear_magnetic_moment": "The nuclear magnetic moment is the magnetic moment of an atomic nucleus and arises from the spin of the protons and neutrons. It is mainly a magnetic dipole moment; the quadrupole moment does cause some small shifts in the hyperfine structure as well. All nuclei that have nonzero spin also possess a nonzero magnetic moment and vice versa, although the connection between the two quantities is not straightforward or easy to calculate.\n The nuclear magnetic moment varies from isotope to isotope of an element. For a nucleus of which the numbers of protons and of neutrons are both even in its ground state (i.e. lowest energy state), the nuclear spin and magnetic moment are both always zero. In cases with odd numbers of either or both protons and neutrons, the nucleus often has nonzero spin and magnetic moment. The nuclear magnetic moment is not sum of nucleon magnetic moments, this property being assigned to the tensorial character of the nuclear force, such as in the case of the most simple nucleus where both proton and neutron appear, namely deuterium nucleus, deuteron.",
  "/wiki/Muon_tomography": "Muon tomography is a technique that uses cosmic ray muons to generate three-dimensional images of volumes using information contained in the Coulomb scattering of the muons. Since muons are much more deeply penetrating than X-rays, muon tomography can be used to image through much thicker material than x-ray based tomography such as CT scanning. The muon flux at the Earth's surface is such that a single muon passes through an area the size of a human hand per second.\nSince its development in the 1950s, muon tomography has taken many forms, the most important of which are muon transmission radiography and muon scattering tomography. Muon tomography imagers are under development for the purposes of detecting nuclear material in road transport vehicles and cargo containers for the purposes of non-proliferation.\nAnother application is the usage of muon tomography to monitor potential underground sites used for carbon sequestration.",
  "/wiki/Muon": "The muon (/\u02c8mju\u02d0\u0252n/; from the Greek letter mu (\u03bc) used to represent it) is an elementary particle similar to the electron, with an electric charge of \u22121 e and a spin of 1/2, but with a much greater mass. It is classified as a lepton. As is the case with other leptons, the muon is not known to have any sub-structure\u2014that is, it is not thought to be composed of any simpler particles.\n The muon is an unstable subatomic particle with a mean lifetime of 2.2 \u03bcs, much longer than many other subatomic particles. As with the decay of the non-elementary neutron (with a lifetime around 15 minutes), muon decay is slow (by subatomic standards) because the decay is mediated by the weak interaction exclusively (rather than the more powerful strong interaction or electromagnetic interaction), and because the mass difference between the muon and the set of its decay products is small, providing few kinetic degrees of freedom for decay. Muon decay almost always produces at least three particles, which must include an electron of the same charge as the muon and two neutrinos of different types.\n Like all elementary particles, the muon has a corresponding antiparticle of opposite charge (+1 e) but equal mass and spin: the antimuon (also called a positive muon). Muons are denoted by \u03bc\u2212 and antimuons by \u03bc+. Muons were previously called mu mesons, but are not classified as mesons by modern particle physicists (see \u00a7 History), and that name is no longer used by the physics community.\n Muons have a mass of 105.66 MeV/c2, which is about 207 times that of the electron. More precisely, it is:",
  "/wiki/Neutron_tomography": "Neutron tomography is a form of computed tomography involving the production of three-dimensional images by the detection of the absorbance of neutrons produced by a neutron source.\nIt created a three-dimensional image of an object by combining multiple planar images with a known separation.  It has a resolution of down to 25 \u03bcm.\nWhilst its resolution is lower than that of X-ray tomography, it can be useful for specimens containing low contrast between the matrix and object of interest; for instance, fossils with a high carbon content, such as plants or vertebrate remains.\n Neutron tomography can have the unfortunate side-effect of leaving imaged samples radioactive if they contain appreciable levels of certain elements.",
  "/wiki/Neutron": "The neutron is a subatomic particle, symbol n or n0, with no net electric charge and a mass slightly greater than that of a proton. Protons and neutrons constitute the nuclei of atoms. Since protons and neutrons behave similarly within the nucleus, and each has a mass of approximately one atomic mass unit, they are both referred to as nucleons. Their properties and interactions are described by nuclear physics.\n The chemical and nuclear properties of the nucleus are determined by the number of protons, called the atomic number, and the number of neutrons, called the neutron number. The atomic mass number is the total number of nucleons. For example, carbon has atomic number 6, and its abundant carbon-12 isotope has 6 neutrons, whereas its rare carbon-13 isotope has 7 neutrons. Some elements occur in nature with only one stable isotope, such as fluorine. Other elements occur with many stable isotopes, such as tin with ten stable isotopes.\n Within the nucleus, protons and neutrons are bound together through the nuclear force. Neutrons are required for the stability of nuclei, with the exception of the single-proton hydrogen atom. Neutrons are produced copiously in nuclear fission and fusion. They are a primary contributor to the nucleosynthesis of chemical elements within stars through fission, fusion, and neutron capture processes.\n The neutron is essential to the production of nuclear power. In the decade after the neutron was discovered by James Chadwick in 1932, neutrons were used to induce many different types of nuclear transmutations. With the discovery of nuclear fission in 1938, it was quickly realized that, if a fission event produced neutrons, each of these neutrons might cause further fission events, in a cascade known as a nuclear chain reaction. These events and findings led to the first self-sustaining nuclear reactor (Chicago Pile-1, 1942) and the first nuclear weapon (Trinity, 1945).\n Free neutrons, while not directly ionizing atoms, cause ionizing radiation. As such they can be a biological hazard, depending upon dose. A small natural \"neutron background\" flux of free neutrons exists on Earth, caused by cosmic ray showers, and by the natural radioactivity of spontaneously fissionable elements in the Earth's crust. Dedicated neutron sources like neutron generators, research reactors and spallation sources produce free neutrons for use in irradiation and in neutron scattering experiments.",
  "/wiki/Tomography": "Tomography is imaging by sections or sectioning, through the use of any kind of penetrating wave. The method is used in radiology, archaeology, biology, atmospheric science, geophysics, oceanography, plasma physics, materials science, astrophysics, quantum information, and other areas of science. The word tomography is derived from Ancient Greek \u03c4\u03cc\u03bc\u03bf\u03c2 tomos, \"slice, section\" and \u03b3\u03c1\u03ac\u03c6\u03c9 graph\u014d, \"to write\". A device used in tomography is called a tomograph, while the image produced is a tomogram.\n In many cases, the production of these images is based on the mathematical procedure tomographic reconstruction, such as X-ray computed tomography technically being produced from multiple projectional radiographs. Many different reconstruction algorithms exist. Most algorithms fall into one of two categories: filtered back projection (FBP) and iterative reconstruction (IR). These procedures give inexact results: they represent a compromise between accuracy and computation time required. FBP demands fewer computational resources, while IR generally produces fewer artifacts (errors in the reconstruction) at a higher computing cost.\n Although MRI and ultrasound are transmission methods, they typically do not require movement of the transmitter to acquire data from different directions. In MRI, both projections and higher spatial harmonics are sampled by applying spatially-varying magnetic fields; no moving parts are necessary to generate an image. On the other hand, since ultrasound uses time-of-flight to spatially encode the received signal, it is not strictly a tomographic method and does not require multiple acquisitions at all."
}