{
  "/wiki/Adjugate_matrix": "In linear algebra, the adjugate, classical adjoint, or adjunct of a square matrix is the transpose of its cofactor matrix.",
  "/wiki/Minor_(linear_algebra)": "In linear algebra, a minor of a matrix A is the determinant of some smaller square matrix, cut down from A by removing one or more of its rows and columns. Minors obtained by removing just one row and one column from square matrices (first minors) are required for calculating matrix cofactors, which in turn are useful for computing both the determinant and inverse of square matrices.",
  "/wiki/Inverse_matrix": "In linear algebra, an n-by-n square matrix A is called invertible (also nonsingular or nondegenerate) if there exists an n-by-n square matrix B such that",
  "/wiki/Laplace_expansion": "In linear algebra, the Laplace expansion, named after Pierre-Simon Laplace, also called cofactor expansion, is an expression for the determinant |B| of an n \u00d7 n matrix B that is a weighted sum of the determinants of n sub-matrices (or minors) of B, each of size (n\u00a0\u2212\u00a01)\u00a0\u00d7\u00a0(n\u00a0\u2212\u00a01). The Laplace expansion is of didactic interest for its simplicity and as one of several ways to view and compute the determinant. For large matrices, it quickly becomes inefficient to compute when compared to methods using matrix decomposition.",
  "/wiki/Alternating_sign_matrix": "In mathematics, an alternating sign matrix is a square matrix of 0s, 1s, and \u22121s such that the sum of each row and column is 1 and the nonzero entries in each row and column alternate in sign. These matrices generalize permutation matrices and arise naturally when using Dodgson condensation to compute a determinant. They are also closely related to the six-vertex model with domain wall boundary conditions from statistical mechanics. They were first defined by William Mills, David Robbins, and Howard Rumsey in the former context.",
  "/wiki/Dodgson_condensation": "In mathematics, Dodgson condensation is a method of computing the determinants of square matrices. It is named for its inventor, Charles Lutwidge Dodgson (better known as Lewis Carroll). The method in the case of an n\u00a0\u00d7\u00a0n matrix is to construct an (n\u00a0\u2212\u00a01)\u00a0\u00d7\u00a0(n\u00a0\u2212\u00a01) matrix, an (n\u00a0\u2212\u00a02)\u00a0\u00d7\u00a0(n\u00a0\u2212\u00a02), and so on, finishing with a 1\u00a0\u00d7\u00a01 matrix, which has one entry, the determinant of the original matrix.",
  "/wiki/Augmented_matrix": "In linear algebra, an augmented matrix is a matrix obtained by appending the columns of two given matrices, usually for the purpose of performing the same elementary row operations on each of the given matrices.",
  "/wiki/B\u00e9zout_matrix": "In mathematics, a B\u00e9zout matrix (or B\u00e9zoutian or Bezoutiant) is a special square matrix associated with two polynomials, introduced by James Joseph Sylvester\u00a0(1853) and Arthur Cayley\u00a0(1857) and named after \u00c9tienne B\u00e9zout.  B\u00e9zoutian may also refer to the determinant of this matrix, which is equal to the resultant of the two polynomials. B\u00e9zout matrices are sometimes used to test the stability of a given polynomial.",
  "/wiki/Control_theory": "Control theory in control systems engineering is a subfield of mathematics that deals with the control of continuously operating dynamical systems  in engineered processes and machines. The objective is to develop a control model for controlling such systems using a control action in an optimum manner without delay or overshoot and ensuring control stability.",
  "/wiki/Stable_polynomial": "In the context of the characteristic polynomial of a differential equation or difference equation,  a polynomial is said to be stable if either:",
  "/wiki/Carleman_matrix": "In mathematics, a Carleman matrix is a matrix  used to convert function composition into matrix multiplication. It is often used in iteration theory to find the continuous iteration of functions which cannot be iterated by pattern recognition alone. Other uses of Carleman matrices occur in the theory of probability generating functions, and Markov chains.",
  "/wiki/Cartan_matrix": "In mathematics, the term Cartan matrix has three meanings.  All of these are named after the French mathematician \u00c9lie Cartan. Amusingly, the Cartan matrices in the context of Lie algebras were first investigated by Wilhelm Killing, whereas the Killing form is due to Cartan.[citation needed]",
  "/wiki/Associative_algebra": "In mathematics, an associative algebra is an algebraic structure with compatible operations of addition, multiplication (assumed to be associative), and a scalar multiplication by elements in some field. The addition and multiplication operations together give A the structure of a ring; the addition and scalar multiplication operations together give A the structure of a vector space over K. In this article we will also use the term K-algebra to mean an associative algebra over the field K. A standard first example of a K-algebra is a ring of square matrices over a field K, with the usual matrix multiplication.",
  "/wiki/Semisimple_Lie_algebra": "In mathematics, a Lie algebra is semisimple if it is a direct sum of simple Lie algebras, i.e., non-abelian Lie algebras \n\n\n\n\n\ng\n\n\n\n\n{\\displaystyle {\\mathfrak {g}}}\n\n whose only ideals are {0} and \n\n\n\n\n\ng\n\n\n\n\n{\\displaystyle {\\mathfrak {g}}}\n\n itself.",
  "/wiki/Circulant_matrix": "In linear algebra, a circulant matrix is a special kind of Toeplitz matrix where each row vector is rotated one element to the right relative to the preceding row vector. In numerical analysis, circulant matrices are important because they are diagonalized by a discrete Fourier transform, and hence linear equations that contain them may be quickly solved using a fast Fourier transform.  They can be interpreted analytically as the integral kernel of a convolution operator on the cyclic group \n\n\n\n\nC\n\nn\n\n\n\n\n{\\displaystyle C_{n}}\n\n and hence frequently appear in formal descriptions of spatially invariant linear operations.",
  "/wiki/System_of_linear_equations": "In mathematics, a system of linear equations (or linear system) is a collection of one or more linear equations involving the same set of variables. For example,",
  "/wiki/Discrete_Fourier_transform": "In mathematics, the discrete Fourier transform (DFT) converts a finite sequence of equally-spaced samples of a function into a same-length sequence of equally-spaced samples of the discrete-time Fourier transform (DTFT), which is a complex-valued function of frequency. The interval at which the DTFT is sampled is the reciprocal of the duration of the input sequence.  An inverse DFT is a Fourier series, using the DTFT samples as coefficients of complex sinusoids at the corresponding DTFT frequencies.  It has the same sample-values as the original input sequence.  The DFT is therefore said to be a frequency domain representation of the original input sequence.  If the original sequence spans all the non-zero values of a function, its DTFT is continuous (and periodic), and the DFT provides discrete samples of one cycle.  If the original sequence is one cycle of a periodic function, the DFT provides all the non-zero values of one DTFT cycle.",
  "/wiki/Cofactor_matrix": "In linear algebra, a minor of a matrix A is the determinant of some smaller square matrix, cut down from A by removing one or more of its rows and columns. Minors obtained by removing just one row and one column from square matrices (first minors) are required for calculating matrix cofactors, which in turn are useful for computing both the determinant and inverse of square matrices.",
  "/wiki/Cofactor_(linear_algebra)": "In linear algebra, a minor of a matrix A is the determinant of some smaller square matrix, cut down from A by removing one or more of its rows and columns. Minors obtained by removing just one row and one column from square matrices (first minors) are required for calculating matrix cofactors, which in turn are useful for computing both the determinant and inverse of square matrices.",
  "/wiki/Commutation_matrix": "In mathematics, especially in linear algebra and matrix theory, the commutation matrix is used for transforming the vectorized form of a matrix into the vectorized form of its transpose. Specifically, the commutation matrix K(m,n) is the nm \u00d7 mn matrix which, for any m \u00d7 n matrix A, transforms vec(A) into vec(AT):",
  "/wiki/Coxeter_matrix": "In mathematics, a Coxeter group, named after H. S. M. Coxeter, is an abstract group that admits a  formal description in terms of reflections (or kaleidoscopic mirrors). Indeed, the finite Coxeter groups are precisely the finite Euclidean reflection groups; the symmetry groups of regular polyhedra are an example.  However, not all Coxeter groups are finite, and not all can be described in terms of symmetries and Euclidean reflections. Coxeter groups were introduced (Coxeter 1934) as abstractions of reflection groups, and finite Coxeter groups were classified in 1935 (Coxeter 1935).",
  "/wiki/Coxeter_groups": "In mathematics, a Coxeter group, named after H. S. M. Coxeter, is an abstract group that admits a  formal description in terms of reflections (or kaleidoscopic mirrors). Indeed, the finite Coxeter groups are precisely the finite Euclidean reflection groups; the symmetry groups of regular polyhedra are an example.  However, not all Coxeter groups are finite, and not all can be described in terms of symmetries and Euclidean reflections. Coxeter groups were introduced (Coxeter 1934) as abstractions of reflection groups, and finite Coxeter groups were classified in 1935 (Coxeter 1935).",
  "/wiki/Symmetry": "Symmetry (from Greek \u03c3\u03c5\u03bc\u03bc\u03b5\u03c4\u03c1\u03af\u03b1 symmetria \"agreement in dimensions, due proportion, arrangement\") in everyday language refers to a sense of harmonious and beautiful proportion and balance.[a] In mathematics, \"symmetry\" has a more precise definition, and is usually used to refer to an object that is invariant under some transformations; including translation, reflection, rotation or scaling. Although these two meanings of \"symmetry\" can sometimes be told apart, they are intricately related, and hence are discussed together in this article.",
  "/wiki/Minimal_polynomial_(linear_algebra)": "In linear algebra, the minimal polynomial \u03bcA of an n \u00d7 n matrix A over a field F is the monic polynomial P over F of least degree such that P(A) = 0.  Any other polynomial Q with Q(A) = 0 is a (polynomial) multiple of \u03bcA.",
  "/wiki/Distance_matrix": "In mathematics, computer science and especially graph theory, a distance matrix is a square matrix (two-dimensional array) containing the distances, taken pairwise, between the elements of a set. Depending upon the application involved, the distance being used to define this matrix may or may not be a metric. If there are N elements, this matrix will have size N\u00d7N. In graph-theoretic applications the elements are more often referred to as points, nodes or vertices.",
  "/wiki/Point_(geometry)": "In modern mathematics, a point refers usually to an element of some set called a space.",
  "/wiki/Computer_vision": "Computer vision is an interdisciplinary scientific field that deals with how computers can be made to gain high-level understanding from digital images or videos. From the perspective of engineering, it seeks to automate tasks that the human visual system can do.",
  "/wiki/Network_analysis_(electronics)": "A network, in the context of electronics, is a collection of interconnected components.  Network analysis is the process of finding the voltages across, and the currents through, all network components.  There are many techniques for calculating these values.  However, for the most part, the techniques assume linear components.\nExcept where stated, the methods described in this article are applicable only to linear network analysis.",
  "/wiki/Duplication_matrix": "In mathematics, especially in linear algebra and matrix theory, the duplication matrix and the elimination matrix are linear transformations used for transforming half-vectorizations of matrices into vectorizations or (respectively) vice versa.",
  "/wiki/Vectorization_(mathematics)": "In mathematics, especially in linear algebra and matrix theory, the vectorization of a matrix is a linear transformation which converts the matrix into a column vector. Specifically, the vectorization of an m \u00d7 n matrix A, denoted vec(A), is the mn \u00d7 1 column vector obtained by stacking the columns of the matrix A on top of one another:",
  "/wiki/Elimination_matrix": "In mathematics, especially in linear algebra and matrix theory, the duplication matrix and the elimination matrix are linear transformations used for transforming half-vectorizations of matrices into vectorizations or (respectively) vice versa.",
  "/wiki/Euclidean_distance_matrix": "In mathematics, a Euclidean distance matrix is an n\u00d7n matrix representing the spacing of a set of n points in Euclidean space. If A is a Euclidean distance matrix and the points \n\n\n\n\nx\n\n1\n\n\n,\n\nx\n\n2\n\n\n,\n\u2026\n,\n\nx\n\nn\n\n\n\n\n{\\displaystyle x_{1},x_{2},\\ldots ,x_{n}}\n\n are defined on m-dimensional space, then the elements of A are given by",
  "/wiki/Euclidean_space": "Euclidean space is the fundamental space of classical geometry. Originally it was the three-dimensional space of Euclidean geometry, but in modern mathematics there are Euclidean spaces of any nonnegative integer dimension, including the three-dimensional space and the Euclidean plane (dimension two). It was introduced by the Ancient Greek mathematician Euclid of Alexandria, and the qualifier Euclidean is used to distinguish it from other spaces that were later discovered in physics and modern mathematics.",
  "/wiki/Fundamental_matrix_(linear_differential_equation)": "In mathematics, a fundamental matrix of a system of n homogeneous linear ordinary differential equations",
  "/wiki/Ordinary_differential_equation": "In mathematics, an ordinary differential equation (ODE) is a differential equation containing one or more functions of one independent variable and the derivatives of those functions. The term ordinary is used in contrast with the term partial differential equation which may be with respect to more than one independent variable.",
  "/wiki/Generator_matrix": "In coding theory, a generator matrix is a matrix whose rows form a basis for a linear code. The codewords are all of the linear combinations of the rows of this matrix, that is, the linear code is the row space of its generator matrix.",
  "/wiki/Linear_code": "In coding theory, a linear code is an error-correcting code for which any linear combination of codewords is also a codeword. Linear codes are traditionally partitioned into block codes and convolutional codes, although turbo codes can be seen as a hybrid of these two types. Linear codes allow for more efficient encoding and decoding algorithms than other codes (cf. syndrome decoding).[citation needed]",
  "/wiki/Coding_theory": "Coding theory is the study of the properties of codes and their respective fitness for specific applications. Codes are used for data compression, cryptography, error detection and correction, data transmission and data storage. Codes are studied by various scientific disciplines\u2014such as information theory, electrical engineering,  mathematics, linguistics, and computer science\u2014for the purpose of designing efficient and reliable data transmission methods. This typically involves the removal of redundancy and the correction or detection of errors in the transmitted data.",
  "/wiki/Gramian_matrix": "In linear algebra, the Gram matrix (a. k. a. Gramian matrix or Gramian) of a set of vectors \n\n\n\n\nv\n\n1\n\n\n,\n\u2026\n,\n\nv\n\nn\n\n\n\n\n{\\displaystyle v_{1},\\dots ,v_{n}}\n\n in an inner product space is the Hermitian matrix of inner products, whose entries are given by \n\n\n\n\nG\n\ni\nj\n\n\n=\n\u27e8\n\nv\n\ni\n\n\n,\n\nv\n\nj\n\n\n\u27e9\n\n\n{\\displaystyle G_{ij}=\\langle v_{i},v_{j}\\rangle }\n\n.",
  "/wiki/Inner_product_space": "In linear algebra, an inner product space is a vector space with an additional structure called an inner product. This additional structure associates each pair of vectors in the space with a scalar quantity known as the inner product of the vectors. Inner products allow the rigorous introduction of intuitive geometrical notions such as the length of a vector or the angle between two vectors. They also provide the means of defining orthogonality between vectors (zero inner product). Inner product spaces generalize Euclidean spaces (in which the inner product is the dot product, also known as the scalar product) to vector spaces of any (possibly infinite) dimension, and are studied in functional analysis. The first usage of the concept of a vector space with an inner product is due to Giuseppe Peano, in 1898.",
  "/wiki/Linear_independence": "In the theory of vector spaces,  a set of vectors is said to be linearly dependent if at least one of the vectors in the set can be defined as a linear combination of the others;  if no vector in the set can be written in this way, then the vectors are said to be linearly independent. These concepts are central to the definition of dimension.",
  "/wiki/Function_space": "In mathematics, a function space is a set of functions between two fixed sets. Often, the domain and/or codomain will have additional structure which is inherited by the function space. For example, the set of functions from any set X into a vector space has a natural vector space structure given by pointwise addition and scalar multiplication. In other scenarios, the function space might inherit a topological or metric structure, hence the name function space.",
  "/wiki/Hessian_matrix": "In mathematics, the Hessian matrix or Hessian is a square matrix of second-order partial derivatives of a scalar-valued function, or scalar field. It describes the local curvature of a function of many variables. The Hessian matrix was developed in the 19th century by the German mathematician Ludwig Otto Hesse and later named after him. Hesse originally used the term \"functional determinants\".",
  "/wiki/Partial_derivative": "In mathematics, a partial derivative of a function of several variables is its derivative with respect to one of those variables, with the others held constant (as opposed to the total derivative, in which all variables are allowed to vary). Partial derivatives are used in vector calculus and differential geometry.",
  "/wiki/Local_minimum": "In mathematical analysis, the maxima and minima (the respective plurals of maximum and minimum) of a function, known collectively as extrema (the plural of extremum), are the largest and smallest value of the function, either within a given range (the local or relative extrema) or on the entire domain of a function (the global or absolute extrema). Pierre de Fermat was one of the first mathematicians to propose a general technique, adequality, for finding the maxima and minima of functions.",
  "/wiki/Blob_detection": "In computer vision, blob detection methods are aimed at detecting regions in a digital image that differ in properties, such as brightness or color, compared to surrounding regions. Informally, a blob is a region of an image in which some properties are constant or approximately constant; all the points in a blob can be considered in some sense to be similar to each other. The most common method for blob detection is convolution.",
  "/wiki/Householder_transformation": "In linear algebra, a Householder transformation (also known as a Householder reflection or elementary reflector) is a linear transformation that describes a reflection about a plane or hyperplane containing the origin. The Householder transformation was used in a 1958 paper by Alston Scott Householder.",
  "/wiki/QR_decomposition": "In linear algebra, a QR decomposition, also known as a QR factorization or QU factorization is a decomposition of a matrix A into a product A\u00a0=\u00a0QR of an orthogonal matrix Q and an upper triangular matrix R. QR decomposition is often used to solve the linear least squares problem and is the basis for a particular eigenvalue algorithm, the QR algorithm.",
  "/wiki/List_of_matrices": "This page lists some important classes of matrices used in mathematics, science and engineering. A matrix (plural matrices, or less commonly matrixes) is a rectangular array of numbers called entries. Matrices have a long history of both study and application, leading to diverse ways of classifying matrices. A first group is matrices satisfying concrete conditions of the entries, including constant matrices. An important example is the identity matrix given by"
}