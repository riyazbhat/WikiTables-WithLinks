{
  "/wiki/5G": "5G is the fifth generation wireless technology for digital cellular networks that began wide deployment in 2019. As with previous standards, the covered areas are divided into regions called \"cells\", serviced by individual antennas. Virtually every major telecommunication service provider in the developed world is deploying antennas or intends to deploy them soon. The frequency spectrum of 5G is divided into millimeter waves, mid-band and low-band. Low-band uses a similar frequency range as the predecessor, 4G. 5G millimeter wave is the fastest, with actual speeds often being 1\u20132 Gbit/s down. Frequencies are above 24 GHz reaching up to 72 GHz which is above the extremely high frequency band's lower boundary. The reach is short, so more cells are required.",
  "/wiki/Smartphones": "Smartphones are a class of mobile phones and of multi-purpose mobile computing devices. They are distinguished from feature phones by their stronger hardware capabilities and extensive mobile operating systems, which facilitate wider software, internet (including web browsing over mobile broadband), and multimedia functionality (including music, video, cameras, and gaming), alongside core phone functions such as voice calls and text messaging. Smartphones typically contain a number of metal\u2013oxide\u2013semiconductor (MOS) integrated circuit (IC) chips, include various sensors that can be leveraged by their software (such as a magnetometer, proximity sensors, barometer, gyroscope, or accelerometer), and support wireless communications protocols (such as Bluetooth, Wi-Fi, or satellite navigation). Early smartphones were marketed primarily towards the enterprise market, attempting to bridge the functionality of standalone personal digital assistant (PDA) devices with support for cellular telephony, but were limited by their bulky form, short battery life, slow analog cellular networks, and the immaturity of wireless data services. These issues were eventually resolved with the exponential scaling and miniaturization of MOS transistors down to sub-micron levels (Moore's law), the improved lithium-ion battery, faster digital mobile data networks (Edholm's law), and more mature software platforms that allowed mobile device ecosystems to develop independently of data providers. In the 2000s, NTT DoCoMo's i-mode platform, BlackBerry, Nokia's Symbian platform, and Windows Mobile began to gain market traction, with models often featuring QWERTY keyboards or resistive touchscreen input, and emphasizing access to push email and wireless internet. Since the unveiling of the iPhone in 2007, the majority of smartphones have featured thin, slate-like form factors, with large, capacitive screens with support for multi-touch gestures rather than physical keyboards, and offer the ability for users to download or purchase additional applications from a centralized store, and use cloud storage and synchronization, virtual assistants, as well as mobile payment services. Improved hardware and faster wireless communication (due to standards such as LTE) have bolstered the growth of the smartphone industry.",
  "/wiki/Tablet_computer": "A tablet computer, commonly shortened to tablet, is a mobile device, typically with a mobile operating system and touchscreen display processing circuitry, and a rechargeable battery in a single, thin and flat package. Tablets, being computers, do what other personal computers do, but lack some input/output (I/O) abilities that others have. Modern tablets largely resemble modern smartphones, the only differences being that tablets are relatively larger than smartphones, with screens 7 inches (18 cm) or larger, measured diagonally, and may not support access to a cellular network. The touchscreen display is operated by gestures executed by finger or digital pen (stylus), instead of the mouse, trackpad, and keyboard of larger computers. Portable computers can be classified according to the presence and appearance of physical keyboards. Two species of tablet, the slate and booklet, do not have physical keyboards and usually accept text and other input by use of a virtual keyboard shown on their touchscreen displays. To compensate for their lack of a physical keyboard, most tablets can connect to independent physical keyboards by Bluetooth or USB; 2-in-1 PCs have keyboards, distinct from tablets. The form of the tablet was conceptualized in the middle of the 20th century (Stanley Kubrick depicted fictional tablets in the 1968 science fiction film  A Space Odyssey) and prototyped and developed in the last two decades of that century.",
  "/wiki/Smartphone": "Smartphones are a class of mobile phones and of multi-purpose mobile computing devices. They are distinguished from feature phones by their stronger hardware capabilities and extensive mobile operating systems, which facilitate wider software, internet (including web browsing over mobile broadband), and multimedia functionality (including music, video, cameras, and gaming), alongside core phone functions such as voice calls and text messaging. Smartphones typically contain a number of metal\u2013oxide\u2013semiconductor (MOS) integrated circuit (IC) chips, include various sensors that can be leveraged by their software (such as a magnetometer, proximity sensors, barometer, gyroscope, or accelerometer), and support wireless communications protocols (such as Bluetooth, Wi-Fi, or satellite navigation). Early smartphones were marketed primarily towards the enterprise market, attempting to bridge the functionality of standalone personal digital assistant (PDA) devices with support for cellular telephony, but were limited by their bulky form, short battery life, slow analog cellular networks, and the immaturity of wireless data services. These issues were eventually resolved with the exponential scaling and miniaturization of MOS transistors down to sub-micron levels (Moore's law), the improved lithium-ion battery, faster digital mobile data networks (Edholm's law), and more mature software platforms that allowed mobile device ecosystems to develop independently of data providers. In the 2000s, NTT DoCoMo's i-mode platform, BlackBerry, Nokia's Symbian platform, and Windows Mobile began to gain market traction, with models often featuring QWERTY keyboards or resistive touchscreen input, and emphasizing access to push email and wireless internet. Since the unveiling of the iPhone in 2007, the majority of smartphones have featured thin, slate-like form factors, with large, capacitive screens with support for multi-touch gestures rather than physical keyboards, and offer the ability for users to download or purchase additional applications from a centralized store, and use cloud storage and synchronization, virtual assistants, as well as mobile payment services. Improved hardware and faster wireless communication (due to standards such as LTE) have bolstered the growth of the smartphone industry.",
  "/wiki/Ambient_intelligence": "In computing, ambient intelligence (AmI) refers to electronic environments that are sensitive and responsive to the presence of people. Ambient intelligence was a projection on the future of consumer electronics, telecommunications and computing that was originally developed in the late 1990s by Eli Zelkha and his team at Palo Alto Ventures for the time frame 2010\u20132020. Ambient intelligence would allow devices to work in concert to support people in carrying out their everyday life activities, tasks and rituals in an intuitive way using information and intelligence that is hidden in the network connecting these devices (for example: The Internet of Things). As these devices grew smaller, more connected and more integrated into our environment, the technological framework behind them would disappear into our surroundings until only the user interface remains perceivable by users.\n The ambient intelligence paradigm builds upon pervasive computing, ubiquitous computing, profiling, context awareness, and human-centric computer interaction design, of which, is characterized by systems and technologies that are:",
  "/wiki/Artificial_brain": "An artificial brain (or artificial mind) is software and hardware with cognitive abilities similar to those of the animal or human brain.\n Research investigating \"artificial brains\" and brain emulation plays three important roles in science:",
  "/wiki/Artificial_intelligence": "In computer science, artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, in contrast to the natural intelligence displayed by humans. Leading AI textbooks define the field as the study of \"intelligent agents\": any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals. Colloquially, the term \"artificial intelligence\" is often used to describe machines (or computers) that mimic \"cognitive\" functions that humans associate with the human mind, such as \"learning\" and \"problem solving\". As machines become increasingly capable, tasks considered to require \"intelligence\" are often removed from the definition of AI, a phenomenon known as the AI effect. A quip in Tesler's Theorem says \"AI is whatever hasn't been done yet.\" For instance, optical character recognition is frequently excluded from things considered to be AI , having become a routine technology. Modern machine capabilities generally classified as AI include successfully understanding human speech, competing at the highest level in strategic game systems (such as chess and Go), autonomously operating cars, intelligent routing in content delivery networks, and military simulations. Artificial intelligence was founded as an academic discipline in 1955, and in the years since has experienced several waves of optimism, followed by disappointment and the loss of funding (known as an \"AI winter\"), followed by new approaches, success and renewed funding.",
  "/wiki/Artificial_general_intelligence": "Artificial general intelligence (AGI) is the intelligence of a machine that has the capacity to understand or learn any intellectual task that a human being can. It is a primary goal of some artificial intelligence research and a common topic in science fiction and futures studies. AGI can also be referred to as strong AI, full AI, or general intelligent action. (Some academic sources reserve the term \"strong AI\" for machines that can experience consciousness.)\n Some authorities emphasize a distinction between strong AI and applied AI (also called narrow AI or weak AI): the use of software to study or accomplish specific problem solving or reasoning tasks. Weak AI, in contrast to strong AI, does not attempt to perform the full range of human cognitive abilities.\n As of 2017, over forty organizations were doing research on AGI.",
  "/wiki/Robot": "A robot is a machine\u2014especially one programmable by a computer\u2014 capable of carrying out a complex series of actions automatically. Robots can be guided by an external control device or the control may be embedded within. Robots may be constructed on the lines of human form, but most robots are machines designed to perform a task with no regard to their aesthetics. Robots can be autonomous or semi-autonomous and range from humanoids such as Honda's Advanced Step in Innovative Mobility (ASIMO) and TOSY's TOSY Ping Pong Playing Robot (TOPIO) to industrial robots, medical operating robots, patient assist robots, dog therapy robots, collectively programmed swarm robots, UAV drones such as General Atomics MQ-1 Predator, and even microscopic nano robots. By mimicking a lifelike appearance or automating movements, a robot may convey a sense of intelligence or thought of its own. Autonomous things are expected to proliferate in the coming decade, with home robotics and the autonomous car as some of the main drivers. The branch of technology that deals with the design, construction, operation, and application of robots, as well as computer systems for their control, sensory feedback, and information processing is robotics. These technologies deal with automated machines that can take the place of humans in dangerous environments or manufacturing processes, or resemble humans in appearance, behavior, or cognition.",
  "/wiki/Atomtronics": "Atomtronics is an emerging sub-field of ultracold atomic physics which encompasses a broad range of topics featuring guided atomic matter waves. The systems typically include components analogous to those found in electronic or optical systems, such as beam splitters and transistors. Applications range from studies of fundamental physics to the development of practical devices.",
  "/wiki/Augmented_reality": "Augmented reality (AR) is an interactive experience of a real-world environment where the objects that reside in the real world are enhanced by computer-generated perceptual information, sometimes across multiple sensory modalities, including visual, auditory, haptic, somatosensory and olfactory. An augogram is a computer generated image that is used to create AR. Augography is the science and practice of making augograms for AR. AR can be defined as a system that fulfills three basic features: a combination of real and virtual worlds, real-time interaction, and accurate 3D registration of virtual and real objects. The overlaid sensory information can be constructive (i.e. additive to the natural environment), or destructive (i.e. masking of the natural environment). This experience is seamlessly interwoven with the physical world such that it is perceived as an immersive aspect of the real environment.",
  "/wiki/Blockchain_(database)": "A blockchain, originally block chain, is a growing list of records, called blocks, that are linked using cryptography. Each block contains a cryptographic hash of the previous block, a timestamp, and transaction data (generally represented as a Merkle tree). By design, a blockchain is resistant to modification of the data. It is \"an open, distributed ledger that can record transactions between two parties efficiently and in a verifiable and permanent way\". For use as a distributed ledger, a blockchain is typically managed by a peer-to-peer network collectively adhering to a protocol for inter-node communication and validating new blocks. Once recorded, the data in any given block cannot be altered retroactively without alteration of all subsequent blocks, which requires consensus of the network majority. Although blockchain records are not unalterable, blockchains may be considered secure by design and exemplify a distributed computing system with high Byzantine fault tolerance. Decentralized consensus has therefore been claimed with a blockchain.",
  "/wiki/Carbon_nanotube_field-effect_transistor": "A carbon nanotube field-effect transistor (CNTFET) refers to a field-effect transistor that utilizes a single carbon nanotube or an array of carbon nanotubes as the channel material instead of bulk silicon in the traditional MOSFET structure. First demonstrated in 1998, there have been major developments in CNTFETs since.",
  "/wiki/Moore's_law": "Moore's law is the observation that the number of transistors in a dense integrated circuit doubles about every two years. The observation is named after Gordon Moore, the co-founder of Fairchild Semiconductor and was the CEO of Intel, whose 1965 paper described a doubling every year in the number of components per integrated circuit, and projected this rate of growth would continue for at least another decade. In 1975, looking forward to the next decade, he revised the forecast to doubling every two years, a compound annual growth rate (CAGR) of 40%. The doubling period is often misquoted as 18 months because of a prediction by Moore's colleague, Intel executive David House. In 1975, House noted that Moore's revised law of doubling transistor count every 2 years in turn implied that computer chip performance would roughly double every 18 months (with no increase in power consumption). Moore's law is closely related to MOSFET scaling, as the rapid scaling and miniaturization of metal\u2013oxide\u2013silicon field-effect transistors (MOSFETs, or MOS transistors) is the key driving force behind Moore's law. Moore's prediction proved accurate for several decades and has been used in the semiconductor industry to guide long-term planning and to set targets for research and development (R&D). Advancements in digital electronics are strongly linked to Moore's law: quality-adjusted microprocessor prices, memory capacity (RAM and flash), sensors, and even the number and size of pixels in digital cameras.",
  "/wiki/Civic_technology": "Civic technology, or civic tech, enhances the relationship between the people and government with software for communications, decision-making, service delivery, and political process. It includes information and communications technology supporting government with software built by community-led teams of volunteers, nonprofits, consultants, and private companies.",
  "/wiki/Smart_cities": "A Smart city is an urban area that uses different types of electronic Internet of Things (IoT) sensors to collect data and then use insights gained from that data to manage assets, resources and services efficiently. This includes data collected from citizens, devices, and assets that is processed and analyzed to monitor and manage traffic and transportation systems, power plants, utilities, water supply networks, waste management, crime detection, information systems, schools, libraries, hospitals, and other community services. The Smart city concept integrates information and communication technology (ICT), and various physical devices connected to the IoT network to optimize the efficiency of city operations and services and connect to citizens. Smart city technology allows city officials to interact directly with both community and city infrastructure and to monitor what is happening in the city and how the city is evolving. ICT is used to enhance quality, performance and interactivity of urban services, to reduce costs and resource consumption and to increase contact between citizens and government. Smart city applications are developed to manage urban flows and allow for real-time responses. A Smart city may therefore be more prepared to respond to challenges than one with a simple \"transactional\" relationship with its citizens. Yet, the term itself remains unclear to its specifics and therefore, open to many interpretations.",
  "/wiki/Cryptocurrency": "A cryptocurrency (or crypto currency) is a digital asset designed to work as a medium of exchange that uses strong cryptography to secure financial transactions, control the creation of additional units, and verify the transfer of assets. Cryptocurrencies use decentralized control as opposed to centralized digital currency and central banking systems.\n The decentralized control of each cryptocurrency works through distributed ledger technology, typically a blockchain, that serves as a public financial transaction database.\nBitcoin, first released as open-source software in 2009, is generally considered the first decentralized cryptocurrency. Since the release of bitcoin, over 6,000 altcoins (alternative variants of bitcoin, or other cryptocurrencies) have been created.",
  "/wiki/Money_supply": "Commercial banks play a role in the process of money creation, especially under the fractional-reserve banking system used throughout the world. In this system, money is created whenever a bank gives out a new loan. This is because the loan, when drawn on and spent, mostly finishes up as a deposit in the banking system, which is counted as part of money supply. After putting aside a part of these deposits as mandated bank reserves, the balance is available for the making of further loans by the bank. This process continues multiple times, and is called the multiplier effect.\n This new money makes up the non-M0 components in the M1-M3 statistics. In short, there are two types of money in a fractional-reserve banking system:",
  "/wiki/Reserve_currency": "A reserve currency (or anchor currency) is a foreign currency that is held in significant quantities by central banks or other monetary authorities as part of their foreign exchange reserves. The reserve currency can be used in international transactions, international investments and all aspects of the global economy. It is often considered a hard currency or safe-haven currency. \n By the end of the 20th century, the United States dollar was considered the world's dominant reserve currency. The world's need for dollars has allowed the United States government as well as Americans to borrow at lower costs, giving the United States an advantage in excess of $100 billion per year.",
  "/wiki/DNA_digital_data_storage": "DNA digital data storage is the process of encoding and decoding binary data to and from synthesized strands of DNA.\n While DNA as a storage medium has enormous potential because of its high storage density, its practical use is currently severely limited because of its high cost and very slow read and write times.\n In June 2019, scientists reported that all 16 GB of text from Wikipedia's English-language version have been encoded into synthetic DNA.",
  "/wiki/Exascale_computing": "Exascale computing refers to computing systems capable of at least one exaFLOPS, or a billion billion (i.e. a quintillion) calculations per second. Such capacity represents a thousandfold increase over the first  petascale computer that came into operation in 2008. (One exaflop is a thousand petaflops or a quintillion, 1018, double precision floating point operations per second.) At a supercomputing conference in 2009, Computerworld projected exascale implementation by 2018. Although the exascale wall for FLOPS was not broken in 2019, the Oak Ridge National Laboratory performed a 1.8\u00d71018 operation calculation per second (which is not the same as 1.8\u00d71018 flops) on the Summit OLCF-4 Supercomputer while analyzing genomic information in 2018. They were Gordon Bell Award winners at Supercomputing 2018. Exascale computing would be a significant achievement in computer engineering, as an exascale computer would have processing power on the order of the estimated processing power of the human brain at the neural level (although the functional power required to simulate a human brain might be lower).",
  "/wiki/Gesture_recognition": "Gesture recognition is a topic in computer science and language technology with the goal of interpreting human gestures via mathematical algorithms. Gestures can originate from any bodily motion or state but commonly originate from the face or hand. Current[when?] focuses in the field include emotion recognition from face and hand gesture recognition. Users can use simple gestures to control or interact with devices without physically touching them. Many approaches have been made using cameras and computer vision algorithms to interpret sign language. However, the identification and recognition of posture, gait, proxemics, and human behaviors is also the subject of gesture recognition techniques. Gesture recognition can be seen as a way for computers to begin to understand human body language, thus building a richer bridge between machines and humans than primitive text user interfaces or even GUIs (graphical user interfaces), which still limit the majority of input to keyboard and mouse and interact naturally without any mechanical devices.",
  "/wiki/Internet_of_Things": "The Internet of things (IoT) is a system of interrelated computing devices, mechanical and digital machines, objects, animals or people that are provided with unique identifiers (UIDs) and the ability to transfer data over a network without requiring human-to-human or human-to-computer interaction.\n The definition of the Internet of things has evolved due to the convergence of multiple technologies, real-time analytics, machine learning, commodity sensors, and embedded systems. Traditional fields of embedded systems, wireless sensor networks, control systems, automation (including home and building automation), and others all contribute to enabling the Internet of things. In the consumer market, IoT technology is most synonymous with products pertaining to the concept of the \"smart home\", covering devices and appliances (such as lighting fixtures, thermostats, home security systems and cameras, and other home appliances) that support one or more common ecosystems, and can be controlled via devices associated with that ecosystem, such as smartphones and smart speakers.\n There are a number of serious concerns about dangers in the growth of IoT, especially in the areas of privacy and security, and consequently industry and governmental moves to begin to address these.",
  "/wiki/Heat-Assisted_Magnetic_Recording": "Heat-assisted magnetic recording (HAMR) is a magnetic storage technology for greatly increasing the amount of data that can be stored on a magnetic device such as a hard disk drive by temporarily heating the disk material during writing, which makes it much more receptive to magnetic effects and allows writing to much smaller regions (and much higher levels of data on a disk). The technology was initially seen as extremely difficult to achieve, with doubts expressed about its feasibility in 2013. The regions being written must be heated in a tiny area - small enough that diffraction prevents the use of normal laser focused heating - and requires a heating, writing and cooling cycle of less than 1 nanosecond, while also controlling the effects of repeated spot-heating on the drive platters, the drive-to-head contact, and the adjacent magnetic data which must not be affected. These challenges required the development of nano-scale surface plasmons (surface guided laser) instead of direct laser-based heating, new types of glass platters and heat-control coatings that tolerate rapid spot-heating without affecting the contact with the recording head or nearby data, new methods to mount the heating laser onto the drive head, and a wide range of other technical, development and control issues that needed to be overcome. In February 2019, Seagate Technology announced that HAMR would be launched commercially in 2019, having been extensively tested at partners during 2017 and 2018. The first drives will be 16 TB, with 20 TB expected in 2020, 24 TB drives in advanced development, and 40 TB drives by around 2023. Its planned successor, known as heated-dot magnetic recording (HDMR), or bit-pattern recording, is also under development, although not expected to be available until at least 2025 or later. HAMR drives have the same form factor (size and layout) as existing traditional hard drives, and do not require any change to the computer or other device in which they are installed; they can be used identically to existing hard drives.",
  "/wiki/Patterned_media": "Patterned media (also known as bit-patterned media or BPM) is a potential future hard disk drive technology to record data in magnetic islands (one bit per island), as opposed to current hard disk drive technology where each bit is stored in 20-30 magnetic grains within a continuous magnetic film. The islands would be patterned from a precursor magnetic film using nanolithography. It is one of the proposed technologies to succeed perpendicular recording due to the greater storage densities it would enable. BPM was introduced by Toshiba in 2010.",
  "/wiki/Shingled_Magnetic_Recording": "Shingled magnetic recording (SMR) is a magnetic storage data recording technology used in hard disk drives (HDDs) to increase storage density and overall per-drive storage capacity. Conventional hard disk drives record data by writing non-overlapping magnetic tracks parallel to each other (perpendicular recording), while shingled recording writes new tracks that overlap part of the previously written magnetic track, leaving the previous track narrower and allowing for higher track density. Thus, the tracks partially overlap similar to roof shingles. This approach was selected because physical limitations prevent recording magnetic heads from having the same width as reading heads, leaving recording heads wider. :7\u20139\n The overlapping-tracks architecture may slow down the writing process since writing to one track overwrites adjacent tracks, and requires them to be rewritten as well. Device-managed SMR devices hide this complexity by managing it in the firmware, presenting an interface like any other hard disk, while other SMR devices are host-managed and depend on the operating system to know how to handle the drive, and only write sequentially to certain regions of the drive. :11 ff. Seagate has been shipping device-managed SMR hard drives since September 2013, while referring to an increase in overall hard disk drive capacity of about 25%, compared to non-shingled storage.",
  "/wiki/Optical_disc": "In computing and optical disc recording technologies, an optical disc (OD) is a flat, usually circular  disc which encodes binary data (bits) in the form of pits (binary value of 0 or off, due to lack of reflection when read) and lands (binary value of 1 or on, due to a reflection when read) on a special material (often aluminium ) on one of its flat surfaces. The encoding material sits atop a thicker substrate (usually polycarbonate) which makes up the bulk of the disc and forms a dust defocusing layer. The encoding pattern follows a continuous, spiral path covering the entire disc surface and extending from the innermost track to the outermost track. The data is stored on the disc with a laser or stamping machine, and can be accessed when the data path is illuminated with a laser diode in an optical disc drive which spins the disc at speeds of about 200 to 4,000 RPM or more, depending on the drive type, disc format, and the distance of the read head from the center of the disc (inner tracks are read at a higher disc speed). Most optical discs exhibit a characteristic iridescence as a result of the diffraction grating formed by its grooves. This side of the disc contains the actual data and is typically coated with a transparent material, usually lacquer. The reverse side of an optical disc usually has a printed label, sometimes made of paper but often printed or stamped onto the disc itself. Unlike the 3\u200b1\u20442-inch floppy disk, most optical discs do not have an integrated protective casing and are therefore susceptible to data transfer problems due to scratches, fingerprints, and other environmental problems.",
  "/wiki/3D_optical_data_storage": "3D optical data storage is any form of optical data storage in which information can be recorded or read with three-dimensional resolution (as opposed to the two-dimensional resolution afforded, for example, by CD).\n This innovation has the potential to provide petabyte-level mass storage on DVD-sized discs (120 mm). Data recording and readback are achieved by focusing lasers within the medium. However, because of the volumetric nature of the data structure, the laser light must travel through other data points before it reaches the point where reading or recording is desired. Therefore, some kind of nonlinearity is required to ensure that these other data points do not interfere with the addressing of the desired point.\n No commercial product based on 3D optical data storage has yet arrived on the mass market, although several companies are actively developing the technology and claim that it may become available 'soon'.",
  "/wiki/Holographic_data_storage": "Holographic data storage is a potential  technology in the area of  high-capacity data storage. While magnetic and optical data storage devices rely on individual bits being stored as distinct magnetic or optical changes on the surface of the recording medium, holographic data storage records information throughout the volume of the medium and is capable of recording multiple images in the same area utilizing light at different angles.\n Additionally, whereas magnetic and optical data storage records information a bit at a time in a linear fashion, holographic storage is capable of recording and reading millions of bits in parallel, enabling data transfer rates greater than those attained by traditional optical storage.",
  "/wiki/GPGPU": "General-purpose computing on graphics processing units (GPGPU, rarely GPGP) is the use of a graphics processing unit (GPU), which typically handles computation only for computer graphics, to perform computation in applications traditionally handled by the central processing unit (CPU). The use of multiple video cards in one computer, or large numbers of graphics chips, further parallelizes the already parallel nature of graphics processing. In addition, even a single GPU-CPU framework provides advantages that multiple CPUs on their own do not offer due to the specialization in each chip. Essentially, a GPGPU pipeline is a kind of parallel processing between one or more GPUs and CPUs that analyzes data as if it were in image or other graphic form. While GPUs operate at lower frequencies, they typically have many times the number of cores. Thus, GPUs can process far more pictures and graphical data per second than a traditional CPU. Migrating data into graphical form and then using the GPU to scan and analyze it can create a large speedup. GPGPU pipelines were developed at the beginning of the 21st century for graphics processing (e.g., for better shaders).",
  "/wiki/Parallel_computing": "Parallel computing is a type of computation in which many calculations or the execution of processes are carried out simultaneously. Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has long been employed in high-performance computing, but it's gaining broader interest due to the physical constraints preventing frequency scaling. As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors. Parallel computing is closely related to concurrent computing\u2014they are frequently used together, and often conflated, though the two are distinct: it is possible to have parallelism without concurrency (such as bit-level parallelism), and concurrency without parallelism (such as multitasking by time-sharing on a single-core CPU). In parallel computing, a computational task is typically broken down into several, often many, very similar sub-tasks that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent computing, the various processes often do not address related tasks; when they do, as is typical in distributed computing, the separate tasks may have a varied nature and often require some inter-process communication during execution.",
  "/wiki/Exocortex": "A brain-computer interface (BCI), sometimes called a neural-control interface (NCI), mind-machine interface (MMI), direct neural interface (DNI), or brain-machine interface (BMI), is a direct communication pathway between an enhanced or wired brain and an external device. BCI differs from neuromodulation in that it allows for bidirectional information flow. BCIs are often directed at researching, mapping, assisting, augmenting, or repairing human cognitive or sensory-motor functions.\n Research on BCIs began in the 1970s at the University of California, Los Angeles (UCLA) under a grant from the National Science Foundation, followed by a contract from DARPA. The papers published after this research also mark the first appearance of the expression brain\u2013computer interface in scientific literature.\n Due to the cortical plasticity of the brain, signals from implanted prostheses can, after adaptation, be handled by the brain like natural sensor or effector channels. Following years of animal experimentation, the first neuroprosthetic devices implanted in humans appeared in the mid-1990s.",
  "/wiki/List_of_emerging_technologies": "Emerging technologies are those technical innovations that represent progressive innovations within a field for competitive advantage."
}