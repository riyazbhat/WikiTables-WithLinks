{
  "/wiki/BeeGFS": "BeeGFS (formerly FhGFS) is a parallel file system, developed and optimized for high-performance computing. BeeGFS includes a distributed metadata architecture for scalability and flexibility reasons. Its most important aspect is data throughput. BeeGFS was originally developed at the Fraunhofer Center for High Performance Computing in Germany by a team around Sven Breuner, who later became the CEO of ThinkParQ, the spin-off company that was founded in 2014 to maintain BeeGFS and offer professional services. The software can be downloaded and used free of charge from the project's website.",
  "/wiki/Posix": "The Portable Operating System Interface (POSIX) is a family of standards specified by the IEEE Computer Society for maintaining compatibility between operating systems. POSIX defines the application programming interface (API), along with command line shells and utility interfaces, for software compatibility with variants of Unix and other operating systems.",
  "/wiki/ObjectiveFS": "ObjectiveFS is a distributed file system developed by Objective Security Corp. It is a POSIX-compliant file system built with an object store backend. It was initially released with AWS S3 backend, and has later implemented support for Google Cloud Storage and object store devices. It was released for beta in early 2013, and the first version was officially released on August 11, 2013.",
  "/wiki/Proprietary_software": "Proprietary software, also known as closed-source software, is non-free computer software for which the software's publisher or another person retains intellectual property rights\u2014usually copyright of the source code, but sometimes patent rights.",
  "/wiki/Filesystem_in_Userspace": "Filesystem in Userspace (FUSE) is a software interface for Unix and Unix-like computer operating systems that lets non-privileged users create their own file systems without editing kernel code. This is achieved by running file system code in user space while the FUSE module provides only a bridge to the actual kernel interfaces. FUSE is available for Linux, FreeBSD, OpenBSD, NetBSD (as puffs), OpenSolaris, Minix 3, Android and macOS. FUSE is free software originally released under the terms of the GNU General Public License and the GNU Lesser General Public License.",
  "/wiki/GPFS": "IBM Spectrum Scale (also known as General Parallel File System or GPFS)[citation needed] is high-performance clustered file system software developed by IBM. It can be deployed in shared-disk or shared-nothing distributed parallel modes. It is used by many of the world's largest commercial companies, as well as some of the supercomputers on the Top 500 List. For example, it was the filesystem of the ASC Purple Supercomputer which was composed of more than 12,000 processors and had 2 petabytes of total disk storage spanning more than 11,000 disks. Like typical cluster filesystems, Spectrum Scale provides concurrent high-speed file access to applications executing on multiple nodes of clusters. It can be used with AIX 5L clusters, Linux clusters, on Microsoft Windows Server, or a heterogeneous cluster of AIX, Linux and Windows nodes. In addition to providing filesystem storage capabilities, Spectrum Scale provides tools for management and administration of the Spectrum Scale cluster and allows for shared access to file systems from remote Spectrum Scale clusters. Spectrum Scale has been available on IBM's AIX since 1998, on Linux since 2001, and on Windows Server since 2008.",
  "/wiki/POSIX": "The Portable Operating System Interface (POSIX) is a family of standards specified by the IEEE Computer Society for maintaining compatibility between operating systems. POSIX defines the application programming interface (API), along with command line shells and utility interfaces, for software compatibility with variants of Unix and other operating systems.",
  "/wiki/Server_Message_Block": "In computer networking, Server Message Block (SMB), one version of which was also known as Common Internet File System (CIFS /s\u026afs/), is a network communication protocol for providing shared access to files, printers, and serial ports between nodes on a network. It also provides an authenticated inter-process communication mechanism. Most usage of SMB involves computers running Microsoft Windows, where it was known as Microsoft Windows Network before the introduction of Active Directory. Corresponding Windows services are LAN Manager Server for the server component, and LAN Manager Workstation for the client component.",
  "/wiki/MapR_FS": "The MapR File System (MapR FS) is a clustered file system that supports both very\nlarge-scale and high-performance uses. MapR FS supports a variety of interfaces including\nconventional read/write file access via NFS and a FUSE interface, as well as via the HDFS interface used by\nmany systems such as Apache Hadoop and Apache Spark. In addition to file-oriented access,\nMapR FS supports access to tables and message streams using the Apache HBase and Apache Kafka APIs as well as via a document database interface. First released in 2010, MapR FS is now typically described as the MapR Converged Data Platform due\nto the addition of tabular and messaging interfaces. The same core technology is, however, used to\nimplement all of these forms of persistent data storage and all of the interfaces are ultimately\nsupported by the same server processes. To distinguish the different capabilities of the overall\ndata platform, the term MapR FS is used more specifically to refer to the file-oriented interfaces,\nMapR DB or MapR JSON DB is used to refer to the tabular interfaces and MapR Streams is used to\ndescribe the message streaming capabilities. MapR FS is a cluster filesystem in that it provides uniform access from/to files and other objects\nsuch as tables using a universal namespace accessible from any client of the system. Access control\nis also provided for files, tables and streams using access control expressions, which are an\nextension of the more common (and limited) access control list to allow permissions to be\ncomposed not just of lists of allowed users or groups, but instead to allow boolean combinations of\nuser id and groups.",
  "/wiki/Network_File_System": "Network File System (NFS) is a distributed file system protocol originally developed by Sun Microsystems (Sun) in 1984,  allowing a user on a client computer to access files over a computer network much like local storage is accessed. NFS, like many other protocols, builds on the Open Network Computing Remote Procedure Call (ONC RPC) system. The NFS is an open standard defined in a Request for Comments (RFC), allowing anyone to implement the protocol.",
  "/wiki/Amazon_S3": "Amazon S3 or Amazon Simple Storage Service is a service offered by Amazon Web Services (AWS) that provides object storage through a web service interface. Amazon S3 uses the same scalable storage infrastructure that Amazon.com uses to run its global e-commerce network. Amazon S3 can be employed to store any type of object which allows for uses like storage for Internet applications, backup and recovery, disaster recovery, data archives, data lakes for analytics, and hybrid cloud storage. In its service-level agreement, Amazon S3 guarantees 99.9% uptime, which works out to less than 43 minutes of downtime per month. AWS launched Amazon S3 in the United States on March 14, 2006, then in Europe in November 2007.",
  "/wiki/Panasas": "Panasas is a data storage company that creates network-attached storage for technical computing environments.",
  "/wiki/Hypertext_Transfer_Protocol": "The Hypertext Transfer Protocol (HTTP) is an application protocol for distributed, collaborative, hypermedia information systems. HTTP is the foundation of data communication for the World Wide Web, where hypertext documents include hyperlinks to other resources that the user can easily access, for example by a mouse click or by tapping the screen in a web browser. Development of HTTP was initiated by Tim Berners-Lee at CERN in 1989. Development of early HTTP Requests for Comments (RFCs) was a coordinated effort by the Internet Engineering Task Force (IETF) and the World Wide Web Consortium (W3C), with work later moving to the IETF. HTTP/1.1 was first documented in RFC 2068 in 1997. That specification was obsoleted by RFC 2616 in 1999, which was likewise replaced by the RFC 7230 family of RFCs in 2014. HTTP/2 is a more efficient expression of HTTP's semantics on the wire, and was published in 2015; it is now supported by major web servers and browsers over Transport Layer Security (TLS) using an Application-Layer Protocol Negotiation (ALPN) extension where TLS 1.2 or newer is required. HTTP/3 is the proposed successor to HTTP/2, which is already in use on the web, using UDP instead of TCP for the underlying transport protocol.",
  "/wiki/Command-line_interface": "A command-line interface (CLI) processes commands to a computer program in the form of lines of text. The program which handles the interface is called a command-line interpreter or command-line processor. Operating systems implement a command-line interface in a shell for interactive access to operating system functions or services. Such access was primarily provided to users by computer terminals starting in the mid-1960s, and continued to be used throughout the 1970s and 1980s on VAX/VMS, Unix systems and personal computer systems including DOS, CP/M and Apple DOS. Today, users rely upon graphical user interfaces and menu-driven interactions. However some programming and maintenance tasks may not have a graphical user interface and may still use a command line. Alternatives to the command line interface include text user interface menus (for example, IBM AIX SMIT), keyboard shortcuts, and various desktop metaphors centered on the pointer (usually controlled with a mouse). Examples of this include the Microsoft Windows, DosShell, and Mouse Systems PowerPanel.",
  "/wiki/Installable_File_System": "The Installable File System (IFS) is a filesystem API in MS-DOS/PC DOS 4.x, IBM OS/2 and Microsoft Windows that enables the operating system to recognize and load drivers for file systems.",
  "/wiki/Software_Development_Kit": "A software development kit (SDK) is a collection of software development tools in one installable package. They ease creation of applications by having compiler, debugger and perhaps a software framework. They are normally specific to a hardware platform and operating system combination. To create applications with advanced functionalities such as advertisements, push notifications, etc; most application software developers use specific software development kits. Some SDKs are required for developing a platform-specific app. For example, the development of an Android app on Java platform requires a Java Development Kit. For iOS applications (apps) the iOS SDK is required. For Universal Windows Platform the .NET Framework SDK might be used.",
  "/wiki/OneFS_distributed_file_system": "The OneFS File System is a parallel distributed networked file system designed by Isilon Systems and is the basis for the Isilon Scale-out Storage Platform. The OneFS file system is controlled and managed by the OneFS Operating System, a FreeBSD variant.",
  "/wiki/SMB/CIFS": "In computer networking, Server Message Block (SMB), one version of which was also known as Common Internet File System (CIFS /s\u026afs/), is a network communication protocol for providing shared access to files, printers, and serial ports between nodes on a network. It also provides an authenticated inter-process communication mechanism. Most usage of SMB involves computers running Microsoft Windows, where it was known as Microsoft Windows Network before the introduction of Active Directory. Corresponding Windows services are LAN Manager Server for the server component, and LAN Manager Workstation for the client component.",
  "/wiki/HDFS": "Apache Hadoop ( /h\u0259\u02c8du\u02d0p/) is a collection of open-source software utilities that facilitate using a network of many computers to solve problems involving massive amounts of data and computation. It provides a software framework for distributed storage and processing of big data using the MapReduce programming model. Originally designed for computer clusters built from commodity hardware\u2014still the common use\u2014it has also found use on clusters of higher-end hardware. All the modules in Hadoop are designed with a fundamental assumption that hardware failures are common occurrences and should be automatically handled by the framework. The core of Apache Hadoop consists of a storage part, known as Hadoop Distributed File System (HDFS), and a processing part which is a MapReduce programming model. Hadoop splits files into large blocks and distributes them across nodes in a cluster. It then transfers packaged code into nodes to process the data in parallel. This approach takes advantage of data locality, where nodes manipulate the data they have access to.",
  "/wiki/HTTP": "The Hypertext Transfer Protocol (HTTP) is an application protocol for distributed, collaborative, hypermedia information systems. HTTP is the foundation of data communication for the World Wide Web, where hypertext documents include hyperlinks to other resources that the user can easily access, for example by a mouse click or by tapping the screen in a web browser. Development of HTTP was initiated by Tim Berners-Lee at CERN in 1989. Development of early HTTP Requests for Comments (RFCs) was a coordinated effort by the Internet Engineering Task Force (IETF) and the World Wide Web Consortium (W3C), with work later moving to the IETF. HTTP/1.1 was first documented in RFC 2068 in 1997. That specification was obsoleted by RFC 2616 in 1999, which was likewise replaced by the RFC 7230 family of RFCs in 2014. HTTP/2 is a more efficient expression of HTTP's semantics on the wire, and was published in 2015; it is now supported by major web servers and browsers over Transport Layer Security (TLS) using an Application-Layer Protocol Negotiation (ALPN) extension where TLS 1.2 or newer is required. HTTP/3 is the proposed successor to HTTP/2, which is already in use on the web, using UDP instead of TCP for the underlying transport protocol.",
  "/wiki/FTP": "The File Transfer Protocol (FTP) is a standard network protocol used for the transfer of computer files between a client and server on a computer network. FTP is built on a client-server model architecture using separate control and data connections between the client and the server. FTP users may authenticate themselves with a clear-text sign-in protocol, normally in the form of a username and password, but can connect anonymously if the server is configured to allow it. For secure transmission that protects the username and password, and encrypts the content, FTP is often secured with SSL/TLS (FTPS) or replaced with SSH File Transfer Protocol (SFTP). The first FTP client applications were command-line programs developed before operating systems had graphical user interfaces, and are still shipped with most Windows, Unix, and Linux operating systems. Many FTP clients and automation utilities have since been developed for desktops, servers, mobile devices, and hardware, and FTP has been incorporated into productivity applications, such as HTML editors.",
  "/wiki/Scality": "Scality is a global company based in San Francisco, California that develops software-defined object storage. The Scality scale-out object storage software platform called RING is the company's commercial product. Scality RING software deploys on industry-standard x86 servers to store objects and files. Scality also offers a number of open source tools called Zenko, including Zenko CloudServer, compatible with the Amazon S3 API.",
  "/wiki/NFS_file_system": "Network File System (NFS) is a distributed file system protocol originally developed by Sun Microsystems (Sun) in 1984,  allowing a user on a client computer to access files over a computer network much like local storage is accessed. NFS, like many other protocols, builds on the Open Network Computing Remote Procedure Call (ONC RPC) system. The NFS is an open standard defined in a Request for Comments (RFC), allowing anyone to implement the protocol.",
  "/wiki/Representational_state_transfer": "Representational state transfer (REST) is a software architectural style that defines a set of constraints to be used for creating Web services. Web services that conform to the REST architectural style, called RESTful Web services, provide interoperability between computer systems on the Internet. RESTful Web services allow the requesting systems to access and manipulate textual representations of Web resources by using a uniform and predefined set of stateless operations. Other kinds of Web services, such as SOAP Web services, expose their own arbitrary sets of operations. Web resources were first defined on the World Wide Web as documents or files identified by their URLs. However, today they have a much more generic and abstract definition that encompasses every thing or entity that can be identified, named, addressed, or handled, in any way whatsoever, on the Web. In a RESTful Web service, requests made to a resource's URI will elicit a response with a payload formatted in HTML, XML, JSON, or some other format. The response can confirm that some alteration has been made to the stored resource, and the response can provide hypertext links to other related resources or collections of resources.",
  "/wiki/AWS_S3": "Amazon S3 or Amazon Simple Storage Service is a service offered by Amazon Web Services (AWS) that provides object storage through a web service interface. Amazon S3 uses the same scalable storage infrastructure that Amazon.com uses to run its global e-commerce network. Amazon S3 can be employed to store any type of object which allows for uses like storage for Internet applications, backup and recovery, disaster recovery, data archives, data lakes for analytics, and hybrid cloud storage. In its service-level agreement, Amazon S3 guarantees 99.9% uptime, which works out to less than 43 minutes of downtime per month. AWS launched Amazon S3 in the United States on March 14, 2006, then in Europe in November 2007.",
  "/wiki/Tensorflow": "TensorFlow is a free and open-source software library for dataflow and differentiable programming across a range of tasks. It is a symbolic math library, and is also used for machine learning applications such as neural networks. It is used for both research and production at Google.\u200d:min 0:15/2:17\u2009:p.2\u2009:0:26/2:17\n TensorFlow was developed by the Google Brain team for internal Google use. It was released under the Apache License 2.0 on November 9, 2015."
}